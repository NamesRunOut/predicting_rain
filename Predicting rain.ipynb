{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7afc3ca",
   "metadata": {},
   "source": [
    "# Climate Weather Surface of Brazil - Hourly\n",
    "## Hourly Climate data from 122 weathes stations between 2000 and 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff57f8",
   "metadata": {},
   "source": [
    "### About Dataset\n",
    "\n",
    "#### Context\n",
    "It's covers hourly weather data from 623 inmet weathers stations of Brazil.\n",
    "\n",
    "Dataset Source: INMET (National Meteorological Institute - Brazil).\n",
    "\n",
    "Equipament: Vaisala Automatic Weather Station AWS310\n",
    "\n",
    "Category: Weather\n",
    "\n",
    "#### Content\n",
    "Data:\n",
    "\n",
    "Date (YYYY-MM-DD) \\\n",
    "Time (HH:00) \\\n",
    "Amount of precipitation in millimetres (last hour) \\\n",
    "Atmospheric pressure at station level (mb) \\\n",
    "Maximum air pressure for the last hour (mb) \\\n",
    "Minimum air pressure for the last hour (mb) \\\n",
    "Solar radiation (KJ/m2) \\\n",
    "Air temperature (instant) (°c) \\\n",
    "Dew point temperature (instant) (°c) \\\n",
    "Maximum temperature for the last hour (°c) \\\n",
    "Minimum temperature for the last hour (°c) \\\n",
    "Maximum dew point temperature for the last hour (°c) \\\n",
    "Minimum dew point temperature for the last hour (°c) \\\n",
    "Maximum relative humid temperature for the last hour (%) \\\n",
    "Minimum relative humid temperature for the last hour (%) \\\n",
    "Relative humid (% instant) \\\n",
    "Wind direction (radius degrees (0-360)) \\\n",
    "Wind gust in metres per second \\\n",
    "Wind speed in metres per second \\\n",
    "Brazilian geopolitical regions \\\n",
    "State (Province) \\\n",
    "Station Name (usually city location or nickname) \\\n",
    "Station code (INMET number) \\\n",
    "Latitude \\\n",
    "Longitude \\\n",
    "Elevation \\\n",
    "\n",
    "NOTE: Not all weather stations started operating since 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a390fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef5be4f",
   "metadata": {},
   "source": [
    "Dataset jest podzielony na kilka plików (każdy odpowiada innemu regionowi Brazylii), ze względu na ogromną ilość danych i inne warunki pogodowe, bierzemy pod uwagę jeden region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68b5fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hr</th>\n",
       "      <th>prcp</th>\n",
       "      <th>stp</th>\n",
       "      <th>smax</th>\n",
       "      <th>smin</th>\n",
       "      <th>gbrd</th>\n",
       "      <th>temp</th>\n",
       "      <th>dewp</th>\n",
       "      <th>tmax</th>\n",
       "      <th>...</th>\n",
       "      <th>wdct</th>\n",
       "      <th>gust</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>region</th>\n",
       "      <th>state</th>\n",
       "      <th>station</th>\n",
       "      <th>station_code</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>268802</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>21:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>MA</td>\n",
       "      <td>PREGUICAS</td>\n",
       "      <td>A218</td>\n",
       "      <td>-2.592341</td>\n",
       "      <td>-42.707553</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268803</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>22:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>MA</td>\n",
       "      <td>PREGUICAS</td>\n",
       "      <td>A218</td>\n",
       "      <td>-2.592341</td>\n",
       "      <td>-42.707553</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268804</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>23:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>MA</td>\n",
       "      <td>PREGUICAS</td>\n",
       "      <td>A218</td>\n",
       "      <td>-2.592341</td>\n",
       "      <td>-42.707553</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268805</th>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>MA</td>\n",
       "      <td>PREGUICAS</td>\n",
       "      <td>A218</td>\n",
       "      <td>-2.592341</td>\n",
       "      <td>-42.707553</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268806</th>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>MA</td>\n",
       "      <td>PREGUICAS</td>\n",
       "      <td>A218</td>\n",
       "      <td>-2.592341</td>\n",
       "      <td>-42.707553</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date     hr  prcp  stp  smax  smin  gbrd  temp  dewp  tmax  ...  \\\n",
       "268802  2020-09-01  21:00   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "268803  2020-09-01  22:00   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "268804  2020-09-01  23:00   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "268805  2020-09-02  00:00   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "268806  2020-09-02  01:00   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "\n",
       "        wdct  gust  wdsp  region  state    station  station_code       lat  \\\n",
       "268802   NaN   NaN   NaN      NE     MA  PREGUICAS          A218 -2.592341   \n",
       "268803   NaN   NaN   NaN      NE     MA  PREGUICAS          A218 -2.592341   \n",
       "268804   NaN   NaN   NaN      NE     MA  PREGUICAS          A218 -2.592341   \n",
       "268805   NaN   NaN   NaN      NE     MA  PREGUICAS          A218 -2.592341   \n",
       "268806   NaN   NaN   NaN      NE     MA  PREGUICAS          A218 -2.592341   \n",
       "\n",
       "             long height  \n",
       "268802 -42.707553   4.62  \n",
       "268803 -42.707553   4.62  \n",
       "268804 -42.707553   4.62  \n",
       "268805 -42.707553   4.62  \n",
       "268806 -42.707553   4.62  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plik ma nazwy kolumn po Brazylijsku, więc je nadpisujemy\n",
    "# dataset jest duży, więc ignorujemy jego znaczną część\n",
    "\n",
    "data1 = pd.read_csv(\n",
    "    \"northeast.csv\", na_values=[\"-9999.0\"], \n",
    "#     header=0, index_col=0, skiprows=16000000,\n",
    "    header=0, index_col=0, skiprows=15500000,\n",
    "#     header=0, index_col=0,\n",
    "    names=[\n",
    "        'date', \n",
    "        'hr', \n",
    "        'prcp', \n",
    "        'stp', \n",
    "        'smax',\n",
    "        'smin', \n",
    "        'gbrd', \n",
    "        'temp', \n",
    "        'dewp', \n",
    "        'tmax',\n",
    "        'tmin', \n",
    "        'dmax',\n",
    "        'dmin', \n",
    "        'hmax',\n",
    "        'hmin',\n",
    "        'hmdy',\n",
    "        'wdct',\n",
    "        'gust',\n",
    "        'wdsp',\n",
    "        'region',\n",
    "        'state',\n",
    "        'station',\n",
    "        'station_code',\n",
    "        'lat',\n",
    "        'long',\n",
    "        'height'\n",
    "    ])\n",
    "display(data1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e156734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([data1, data2, data3, data4, data5], ignore_index=True)\n",
    "data = data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8da0dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilość rekordów: 760936\n"
     ]
    }
   ],
   "source": [
    "print(\"Ilość rekordów:\",data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15fb2c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hr</th>\n",
       "      <th>prcp</th>\n",
       "      <th>stp</th>\n",
       "      <th>smax</th>\n",
       "      <th>smin</th>\n",
       "      <th>gbrd</th>\n",
       "      <th>temp</th>\n",
       "      <th>dewp</th>\n",
       "      <th>tmax</th>\n",
       "      <th>...</th>\n",
       "      <th>wdct</th>\n",
       "      <th>gust</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>region</th>\n",
       "      <th>state</th>\n",
       "      <th>station</th>\n",
       "      <th>station_code</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>268802</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>21:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>MA</td>\n",
       "      <td>PREGUICAS</td>\n",
       "      <td>A218</td>\n",
       "      <td>-2.592341</td>\n",
       "      <td>-42.707553</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268803</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>22:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>MA</td>\n",
       "      <td>PREGUICAS</td>\n",
       "      <td>A218</td>\n",
       "      <td>-2.592341</td>\n",
       "      <td>-42.707553</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268804</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>23:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>MA</td>\n",
       "      <td>PREGUICAS</td>\n",
       "      <td>A218</td>\n",
       "      <td>-2.592341</td>\n",
       "      <td>-42.707553</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268805</th>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>MA</td>\n",
       "      <td>PREGUICAS</td>\n",
       "      <td>A218</td>\n",
       "      <td>-2.592341</td>\n",
       "      <td>-42.707553</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268806</th>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>MA</td>\n",
       "      <td>PREGUICAS</td>\n",
       "      <td>A218</td>\n",
       "      <td>-2.592341</td>\n",
       "      <td>-42.707553</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date     hr  prcp  stp  smax  smin  gbrd  temp  dewp  tmax  ...  \\\n",
       "268802  2020-09-01  21:00   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "268803  2020-09-01  22:00   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "268804  2020-09-01  23:00   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "268805  2020-09-02  00:00   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "268806  2020-09-02  01:00   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "\n",
       "        wdct  gust  wdsp  region  state    station  station_code       lat  \\\n",
       "268802   NaN   NaN   NaN      NE     MA  PREGUICAS          A218 -2.592341   \n",
       "268803   NaN   NaN   NaN      NE     MA  PREGUICAS          A218 -2.592341   \n",
       "268804   NaN   NaN   NaN      NE     MA  PREGUICAS          A218 -2.592341   \n",
       "268805   NaN   NaN   NaN      NE     MA  PREGUICAS          A218 -2.592341   \n",
       "268806   NaN   NaN   NaN      NE     MA  PREGUICAS          A218 -2.592341   \n",
       "\n",
       "             long height  \n",
       "268802 -42.707553   4.62  \n",
       "268803 -42.707553   4.62  \n",
       "268804 -42.707553   4.62  \n",
       "268805 -42.707553   4.62  \n",
       "268806 -42.707553   4.62  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b90b57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prcp</th>\n",
       "      <th>stp</th>\n",
       "      <th>smax</th>\n",
       "      <th>smin</th>\n",
       "      <th>gbrd</th>\n",
       "      <th>temp</th>\n",
       "      <th>dewp</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>dmax</th>\n",
       "      <th>dmin</th>\n",
       "      <th>hmax</th>\n",
       "      <th>hmin</th>\n",
       "      <th>hmdy</th>\n",
       "      <th>wdct</th>\n",
       "      <th>gust</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>354866.000000</td>\n",
       "      <td>445969.000000</td>\n",
       "      <td>445583.000000</td>\n",
       "      <td>445583.000000</td>\n",
       "      <td>258062.000000</td>\n",
       "      <td>445018.000000</td>\n",
       "      <td>415681.00000</td>\n",
       "      <td>442150.000000</td>\n",
       "      <td>444617.000000</td>\n",
       "      <td>414679.000000</td>\n",
       "      <td>414230.000000</td>\n",
       "      <td>415446.000000</td>\n",
       "      <td>414912.000000</td>\n",
       "      <td>416052.000000</td>\n",
       "      <td>420788.000000</td>\n",
       "      <td>420096.000000</td>\n",
       "      <td>420802.000000</td>\n",
       "      <td>760936.000000</td>\n",
       "      <td>760936.000000</td>\n",
       "      <td>760936.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.137186</td>\n",
       "      <td>976.541076</td>\n",
       "      <td>976.769706</td>\n",
       "      <td>976.229359</td>\n",
       "      <td>1453.214379</td>\n",
       "      <td>25.886785</td>\n",
       "      <td>19.41777</td>\n",
       "      <td>26.499634</td>\n",
       "      <td>25.262349</td>\n",
       "      <td>19.939587</td>\n",
       "      <td>18.905038</td>\n",
       "      <td>73.375579</td>\n",
       "      <td>67.843379</td>\n",
       "      <td>70.618418</td>\n",
       "      <td>143.230368</td>\n",
       "      <td>5.169690</td>\n",
       "      <td>2.116383</td>\n",
       "      <td>-8.556301</td>\n",
       "      <td>-40.128008</td>\n",
       "      <td>280.115381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.239988</td>\n",
       "      <td>29.016462</td>\n",
       "      <td>29.023245</td>\n",
       "      <td>29.018714</td>\n",
       "      <td>1165.035508</td>\n",
       "      <td>4.030422</td>\n",
       "      <td>3.85804</td>\n",
       "      <td>4.211006</td>\n",
       "      <td>3.829627</td>\n",
       "      <td>3.847888</td>\n",
       "      <td>3.908563</td>\n",
       "      <td>19.376327</td>\n",
       "      <td>20.573204</td>\n",
       "      <td>20.066765</td>\n",
       "      <td>85.118991</td>\n",
       "      <td>3.001998</td>\n",
       "      <td>1.637023</td>\n",
       "      <td>3.787549</td>\n",
       "      <td>3.191300</td>\n",
       "      <td>242.237969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>865.000000</td>\n",
       "      <td>865.900000</td>\n",
       "      <td>865.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>-10.00000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-17.963016</td>\n",
       "      <td>-47.459839</td>\n",
       "      <td>3.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>962.000000</td>\n",
       "      <td>962.000000</td>\n",
       "      <td>961.400000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>17.70000</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>22.700000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-10.984722</td>\n",
       "      <td>-42.252500</td>\n",
       "      <td>84.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>981.000000</td>\n",
       "      <td>980.800000</td>\n",
       "      <td>980.200000</td>\n",
       "      <td>1347.000000</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>20.20000</td>\n",
       "      <td>26.100000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>20.700000</td>\n",
       "      <td>19.700000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>-8.059280</td>\n",
       "      <td>-39.558056</td>\n",
       "      <td>232.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1002.000000</td>\n",
       "      <td>1001.900000</td>\n",
       "      <td>1001.400000</td>\n",
       "      <td>2403.000000</td>\n",
       "      <td>28.700000</td>\n",
       "      <td>22.10000</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>27.800000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>-5.555723</td>\n",
       "      <td>-37.766667</td>\n",
       "      <td>421.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>89.200000</td>\n",
       "      <td>1024.000000</td>\n",
       "      <td>1024.500000</td>\n",
       "      <td>1024.200000</td>\n",
       "      <td>6776.000000</td>\n",
       "      <td>40.800000</td>\n",
       "      <td>34.70000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>39.200000</td>\n",
       "      <td>35.100000</td>\n",
       "      <td>32.100000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>40.100000</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>0.916877</td>\n",
       "      <td>-29.345904</td>\n",
       "      <td>1283.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                prcp            stp           smax           smin  \\\n",
       "count  354866.000000  445969.000000  445583.000000  445583.000000   \n",
       "mean        0.137186     976.541076     976.769706     976.229359   \n",
       "std         1.239988      29.016462      29.023245      29.018714   \n",
       "min         0.000000     865.000000     865.900000     865.400000   \n",
       "25%         0.000000     962.000000     962.000000     961.400000   \n",
       "50%         0.000000     981.000000     980.800000     980.200000   \n",
       "75%         0.000000    1002.000000    1001.900000    1001.400000   \n",
       "max        89.200000    1024.000000    1024.500000    1024.200000   \n",
       "\n",
       "                gbrd           temp          dewp           tmax  \\\n",
       "count  258062.000000  445018.000000  415681.00000  442150.000000   \n",
       "mean     1453.214379      25.886785      19.41777      26.499634   \n",
       "std      1165.035508       4.030422       3.85804       4.211006   \n",
       "min         0.000000      10.400000     -10.00000      10.700000   \n",
       "25%       312.000000      23.100000      17.70000      23.500000   \n",
       "50%      1347.000000      25.500000      20.20000      26.100000   \n",
       "75%      2403.000000      28.700000      22.10000      29.600000   \n",
       "max      6776.000000      40.800000      34.70000      41.000000   \n",
       "\n",
       "                tmin           dmax           dmin           hmax  \\\n",
       "count  444617.000000  414679.000000  414230.000000  415446.000000   \n",
       "mean       25.262349      19.939587      18.905038      73.375579   \n",
       "std         3.829627       3.847888       3.908563      19.376327   \n",
       "min        10.300000     -10.000000     -10.000000       7.000000   \n",
       "25%        22.700000      18.200000      17.100000      61.000000   \n",
       "50%        25.000000      20.700000      19.700000      77.000000   \n",
       "75%        27.800000      22.500000      21.600000      89.000000   \n",
       "max        39.200000      35.100000      32.100000     100.000000   \n",
       "\n",
       "                hmin           hmdy           wdct           gust  \\\n",
       "count  414912.000000  416052.000000  420788.000000  420096.000000   \n",
       "mean       67.843379      70.618418     143.230368       5.169690   \n",
       "std        20.573204      20.066765      85.118991       3.001998   \n",
       "min         7.000000       7.000000       1.000000       0.000000   \n",
       "25%        53.000000      57.000000      86.000000       2.800000   \n",
       "50%        70.000000      74.000000     124.000000       5.100000   \n",
       "75%        85.000000      88.000000     185.000000       7.300000   \n",
       "max       100.000000     100.000000     360.000000      40.100000   \n",
       "\n",
       "                wdsp            lat           long         height  \n",
       "count  420802.000000  760936.000000  760936.000000  760936.000000  \n",
       "mean        2.116383      -8.556301     -40.128008     280.115381  \n",
       "std         1.637023       3.787549       3.191300     242.237969  \n",
       "min         0.000000     -17.963016     -47.459839       3.720000  \n",
       "25%         0.800000     -10.984722     -42.252500      84.850000  \n",
       "50%         1.900000      -8.059280     -39.558056     232.290000  \n",
       "75%         3.100000      -5.555723     -37.766667     421.440000  \n",
       "max        18.900000       0.916877     -29.345904    1283.950000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "493d2989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzamy ilość null-i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c9b1839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                 0\n",
       "hr                   0\n",
       "prcp            406070\n",
       "stp             314967\n",
       "smax            315353\n",
       "smin            315353\n",
       "gbrd            502874\n",
       "temp            315918\n",
       "dewp            345255\n",
       "tmax            318786\n",
       "tmin            316319\n",
       "dmax            346257\n",
       "dmin            346706\n",
       "hmax            345490\n",
       "hmin            346024\n",
       "hmdy            344884\n",
       "wdct            340148\n",
       "gust            340840\n",
       "wdsp            340134\n",
       "region               0\n",
       "state                0\n",
       "station              0\n",
       "station_code         0\n",
       "lat                  0\n",
       "long                 0\n",
       "height               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed5a4555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 760936 entries, 268802 to 352795\n",
      "Data columns (total 26 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   date          760936 non-null  object \n",
      " 1   hr            760936 non-null  object \n",
      " 2   prcp          354866 non-null  float64\n",
      " 3   stp           445969 non-null  float64\n",
      " 4   smax          445583 non-null  float64\n",
      " 5   smin          445583 non-null  float64\n",
      " 6   gbrd          258062 non-null  float64\n",
      " 7   temp          445018 non-null  float64\n",
      " 8   dewp          415681 non-null  float64\n",
      " 9   tmax          442150 non-null  float64\n",
      " 10  tmin          444617 non-null  float64\n",
      " 11  dmax          414679 non-null  float64\n",
      " 12  dmin          414230 non-null  float64\n",
      " 13  hmax          415446 non-null  float64\n",
      " 14  hmin          414912 non-null  float64\n",
      " 15  hmdy          416052 non-null  float64\n",
      " 16  wdct          420788 non-null  float64\n",
      " 17  gust          420096 non-null  float64\n",
      " 18  wdsp          420802 non-null  float64\n",
      " 19  region        760936 non-null  object \n",
      " 20  state         760936 non-null  object \n",
      " 21  station       760936 non-null  object \n",
      " 22  station_code  760936 non-null  object \n",
      " 23  lat           760936 non-null  float64\n",
      " 24  long          760936 non-null  float64\n",
      " 25  height        760936 non-null  float64\n",
      "dtypes: float64(20), object(6)\n",
      "memory usage: 156.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aebf8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patrzymy na dane kategoryczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b52039a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BA    240768\n",
       "MA     95741\n",
       "PI     95373\n",
       "CE     90000\n",
       "PE     72899\n",
       "RN     61056\n",
       "AL     41451\n",
       "PB     34704\n",
       "SE     28944\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"state\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "010456a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NE    760936\n",
       "Name: region, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"region\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b14e66f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VALENCA DO PIAUI        11664\n",
       "BARREIRAS               11664\n",
       "SANTA RITA DE CASSIA    11664\n",
       "SANTA CRUZ              11664\n",
       "PIATA                   11664\n",
       "                        ...  \n",
       "ITAPETINGA               2880\n",
       "CAROLINA                 2880\n",
       "CARAVELAS                2880\n",
       "PIRANHAS                 2880\n",
       "FLORIANO                 2880\n",
       "Name: station, Length: 146, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"station\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f93a35c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A363    11664\n",
       "A402    11664\n",
       "A415    11664\n",
       "A367    11664\n",
       "A430    11664\n",
       "        ...  \n",
       "A446     2880\n",
       "A205     2880\n",
       "A405     2880\n",
       "A371     2880\n",
       "A311     2880\n",
       "Name: station_code, Length: 146, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"station_code\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32529d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Większość z nim albo nie ma dla nas znaczenia, albo opisuje to samo\n",
    "# lat, long, height i station może być wyrażone tylko za pomocą station_code (opisuje to samo miejsce)\n",
    "# dane demogrficzne, typu nazwa stanu lub regionu nie mają dla nas znaczenia\n",
    "data = data.drop(['region', 'state', 'station', 'lat', 'long', 'height'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb5c5bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hr</th>\n",
       "      <th>prcp</th>\n",
       "      <th>stp</th>\n",
       "      <th>smax</th>\n",
       "      <th>smin</th>\n",
       "      <th>gbrd</th>\n",
       "      <th>temp</th>\n",
       "      <th>dewp</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>dmax</th>\n",
       "      <th>dmin</th>\n",
       "      <th>hmax</th>\n",
       "      <th>hmin</th>\n",
       "      <th>hmdy</th>\n",
       "      <th>wdct</th>\n",
       "      <th>gust</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>station_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>268802</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>21:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268803</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>22:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268804</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>23:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268805</th>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268806</th>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date     hr  prcp  stp  smax  smin  gbrd  temp  dewp  tmax  \\\n",
       "268802  2020-09-01  21:00   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "268803  2020-09-01  22:00   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "268804  2020-09-01  23:00   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "268805  2020-09-02  00:00   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "268806  2020-09-02  01:00   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "        tmin  dmax  dmin  hmax  hmin  hmdy  wdct  gust  wdsp station_code  \n",
       "268802   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN         A218  \n",
       "268803   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN         A218  \n",
       "268804   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN         A218  \n",
       "268805   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN         A218  \n",
       "268806   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN         A218  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ae90797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usuwamy te rekordy, gdzie nie mamy informacji o opadach\n",
    "data = data.dropna(subset=['prcp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96c7d038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                 0\n",
       "hr                   0\n",
       "prcp                 0\n",
       "stp                 12\n",
       "smax               246\n",
       "smin               246\n",
       "gbrd            149656\n",
       "temp               791\n",
       "dewp             23826\n",
       "tmax              3504\n",
       "tmin              1037\n",
       "dmax             24581\n",
       "dmin             24952\n",
       "hmax             23858\n",
       "hmin             24283\n",
       "hmdy             23463\n",
       "wdct             22835\n",
       "gust             23305\n",
       "wdsp             22821\n",
       "station_code         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pozostałe nulle, którymi musimy się zająć\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb4227ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pozostała ilość rekordów: 321929\n"
     ]
    }
   ],
   "source": [
    "# podobnie usuwamy rekordy gdzie brakuje wiekszosci danych (musi byc wypelnionych przynajmniej 17 pol)\n",
    "# czyli usuwany dane z określonych dat/godzin zanim stacja zaczęła funkcjonować\n",
    "data = data.dropna(thresh=17)\n",
    "print(\"Pozostała ilość rekordów:\", data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a36a039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                 0\n",
       "hr                   0\n",
       "prcp                 0\n",
       "stp                  4\n",
       "smax                 0\n",
       "smin                 0\n",
       "gbrd            126416\n",
       "temp                10\n",
       "dewp               610\n",
       "tmax              1692\n",
       "tmin                 0\n",
       "dmax               232\n",
       "dmin               403\n",
       "hmax                 0\n",
       "hmin                42\n",
       "hmdy               478\n",
       "wdct             14054\n",
       "gust             14299\n",
       "wdsp             14042\n",
       "station_code         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30a3fcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prcp       0.0\n",
      "stp      976.0\n",
      "smax     976.5\n",
      "smin     975.9\n",
      "gbrd    1355.0\n",
      "temp      25.6\n",
      "dewp      20.0\n",
      "tmax      26.3\n",
      "tmin      25.0\n",
      "dmax      20.5\n",
      "dmin      19.5\n",
      "hmax      76.0\n",
      "hmin      69.0\n",
      "hmdy      73.0\n",
      "wdct     123.0\n",
      "gust       5.1\n",
      "wdsp       1.9\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Names\\AppData\\Local\\Temp\\ipykernel_12028\\243384962.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  medians = data.median(skipna=True)\n"
     ]
    }
   ],
   "source": [
    "# pozostale nulle zastepujemy mediana\n",
    "medians = data.median(skipna=True)\n",
    "print(medians)\n",
    "\n",
    "data['stp'] = data['stp'].fillna(medians[1])\n",
    "data['smax'] = data['smax'].fillna(medians[2])\n",
    "data['smin'] = data['smin'].fillna(medians[3])\n",
    "data['gbrd'] = data['gbrd'].fillna(medians[4])\n",
    "data['temp'] = data['temp'].fillna(medians[5])\n",
    "data['dewp'] = data['dewp'].fillna(medians[6])\n",
    "data['tmax'] = data['tmax'].fillna(medians[7])\n",
    "data['tmin'] = data['tmin'].fillna(medians[8])\n",
    "data['dmax'] = data['dmax'].fillna(medians[9])\n",
    "data['dmin'] = data['dmin'].fillna(medians[10])\n",
    "data['hmax'] = data['hmax'].fillna(medians[11])\n",
    "data['hmin'] = data['hmin'].fillna(medians[12])\n",
    "data['hmdy'] = data['hmdy'].fillna(medians[13])\n",
    "data['wdct'] = data['wdct'].fillna(medians[14])\n",
    "data['gust'] = data['gust'].fillna(medians[15])\n",
    "data['wdsp'] = data['wdsp'].fillna(medians[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19d56b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date            0\n",
       "hr              0\n",
       "prcp            0\n",
       "stp             0\n",
       "smax            0\n",
       "smin            0\n",
       "gbrd            0\n",
       "temp            0\n",
       "dewp            0\n",
       "tmax            0\n",
       "tmin            0\n",
       "dmax            0\n",
       "dmin            0\n",
       "hmax            0\n",
       "hmin            0\n",
       "hmdy            0\n",
       "wdct            0\n",
       "gust            0\n",
       "wdsp            0\n",
       "station_code    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d782b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 321929 entries, 262066 to 352795\n",
      "Data columns (total 20 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   date          321929 non-null  object \n",
      " 1   hr            321929 non-null  object \n",
      " 2   prcp          321929 non-null  float64\n",
      " 3   stp           321929 non-null  float64\n",
      " 4   smax          321929 non-null  float64\n",
      " 5   smin          321929 non-null  float64\n",
      " 6   gbrd          321929 non-null  float64\n",
      " 7   temp          321929 non-null  float64\n",
      " 8   dewp          321929 non-null  float64\n",
      " 9   tmax          321929 non-null  float64\n",
      " 10  tmin          321929 non-null  float64\n",
      " 11  dmax          321929 non-null  float64\n",
      " 12  dmin          321929 non-null  float64\n",
      " 13  hmax          321929 non-null  float64\n",
      " 14  hmin          321929 non-null  float64\n",
      " 15  hmdy          321929 non-null  float64\n",
      " 16  wdct          321929 non-null  float64\n",
      " 17  gust          321929 non-null  float64\n",
      " 18  wdsp          321929 non-null  float64\n",
      " 19  station_code  321929 non-null  object \n",
      "dtypes: float64(17), object(3)\n",
      "memory usage: 51.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f6ab0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prcp</th>\n",
       "      <th>stp</th>\n",
       "      <th>smax</th>\n",
       "      <th>smin</th>\n",
       "      <th>gbrd</th>\n",
       "      <th>temp</th>\n",
       "      <th>dewp</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>dmax</th>\n",
       "      <th>dmin</th>\n",
       "      <th>hmax</th>\n",
       "      <th>hmin</th>\n",
       "      <th>hmdy</th>\n",
       "      <th>wdct</th>\n",
       "      <th>gust</th>\n",
       "      <th>wdsp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.132762</td>\n",
       "      <td>973.221608</td>\n",
       "      <td>973.454855</td>\n",
       "      <td>972.909963</td>\n",
       "      <td>1412.990445</td>\n",
       "      <td>25.965979</td>\n",
       "      <td>19.168960</td>\n",
       "      <td>26.596658</td>\n",
       "      <td>25.316963</td>\n",
       "      <td>19.726319</td>\n",
       "      <td>18.655436</td>\n",
       "      <td>72.790941</td>\n",
       "      <td>67.098345</td>\n",
       "      <td>69.860121</td>\n",
       "      <td>139.128640</td>\n",
       "      <td>5.240913</td>\n",
       "      <td>2.130079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.226127</td>\n",
       "      <td>30.731061</td>\n",
       "      <td>30.737863</td>\n",
       "      <td>30.723507</td>\n",
       "      <td>907.902161</td>\n",
       "      <td>4.182204</td>\n",
       "      <td>4.031003</td>\n",
       "      <td>4.345986</td>\n",
       "      <td>3.981500</td>\n",
       "      <td>3.995272</td>\n",
       "      <td>4.072172</td>\n",
       "      <td>19.604386</td>\n",
       "      <td>20.798260</td>\n",
       "      <td>20.288464</td>\n",
       "      <td>81.662468</td>\n",
       "      <td>2.819915</td>\n",
       "      <td>1.498585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>865.000000</td>\n",
       "      <td>865.900000</td>\n",
       "      <td>865.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.400000</td>\n",
       "      <td>958.800000</td>\n",
       "      <td>967.000000</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>23.600000</td>\n",
       "      <td>22.700000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>976.000000</td>\n",
       "      <td>976.500000</td>\n",
       "      <td>975.900000</td>\n",
       "      <td>1355.000000</td>\n",
       "      <td>25.600000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>26.300000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>996.000000</td>\n",
       "      <td>996.700000</td>\n",
       "      <td>996.100000</td>\n",
       "      <td>1728.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>21.900000</td>\n",
       "      <td>29.800000</td>\n",
       "      <td>28.100000</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>3.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>89.200000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>1020.200000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>6776.000000</td>\n",
       "      <td>40.800000</td>\n",
       "      <td>32.800000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>39.200000</td>\n",
       "      <td>35.100000</td>\n",
       "      <td>32.100000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>40.100000</td>\n",
       "      <td>12.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                prcp            stp           smax           smin  \\\n",
       "count  321929.000000  321929.000000  321929.000000  321929.000000   \n",
       "mean        0.132762     973.221608     973.454855     972.909963   \n",
       "std         1.226127      30.731061      30.737863      30.723507   \n",
       "min         0.000000     865.000000     865.900000     865.400000   \n",
       "25%         0.000000     959.000000     959.400000     958.800000   \n",
       "50%         0.000000     976.000000     976.500000     975.900000   \n",
       "75%         0.000000     996.000000     996.700000     996.100000   \n",
       "max        89.200000    1020.000000    1020.200000    1020.000000   \n",
       "\n",
       "                gbrd           temp           dewp           tmax  \\\n",
       "count  321929.000000  321929.000000  321929.000000  321929.000000   \n",
       "mean     1412.990445      25.965979      19.168960      26.596658   \n",
       "std       907.902161       4.182204       4.031003       4.345986   \n",
       "min         0.000000      10.400000     -10.000000      10.700000   \n",
       "25%       967.000000      23.100000      17.300000      23.600000   \n",
       "50%      1355.000000      25.600000      20.000000      26.300000   \n",
       "75%      1728.000000      29.000000      21.900000      29.800000   \n",
       "max      6776.000000      40.800000      32.800000      41.000000   \n",
       "\n",
       "                tmin           dmax           dmin           hmax  \\\n",
       "count  321929.000000  321929.000000  321929.000000  321929.000000   \n",
       "mean       25.316963      19.726319      18.655436      72.790941   \n",
       "std         3.981500       3.995272       4.072172      19.604386   \n",
       "min        10.300000     -10.000000     -10.000000       7.000000   \n",
       "25%        22.700000      17.900000      16.700000      60.000000   \n",
       "50%        25.000000      20.500000      19.500000      76.000000   \n",
       "75%        28.100000      22.400000      21.500000      89.000000   \n",
       "max        39.200000      35.100000      32.100000     100.000000   \n",
       "\n",
       "                hmin           hmdy           wdct           gust  \\\n",
       "count  321929.000000  321929.000000  321929.000000  321929.000000   \n",
       "mean       67.098345      69.860121     139.128640       5.240913   \n",
       "std        20.798260      20.288464      81.662468       2.819915   \n",
       "min         7.000000       7.000000       1.000000       0.000000   \n",
       "25%        52.000000      56.000000      88.000000       3.100000   \n",
       "50%        69.000000      73.000000     123.000000       5.100000   \n",
       "75%        85.000000      87.000000     172.000000       7.200000   \n",
       "max       100.000000     100.000000     360.000000      40.100000   \n",
       "\n",
       "                wdsp  \n",
       "count  321929.000000  \n",
       "mean        2.130079  \n",
       "std         1.498585  \n",
       "min         0.000000  \n",
       "25%         1.000000  \n",
       "50%         1.900000  \n",
       "75%         3.100000  \n",
       "max        12.900000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79267427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dla kolumny prcp nadajemy wartosci T albo F, w zaleznosci od tego czy padalo, nie interesuje nas intensywnosc opadow\n",
    "data.loc[data['prcp'] > 0, \"prcp\"] = 1\n",
    "data.loc[data['prcp'] == 0, \"prcp\"] = 0\n",
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bef8166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prcp</th>\n",
       "      <th>stp</th>\n",
       "      <th>smax</th>\n",
       "      <th>smin</th>\n",
       "      <th>gbrd</th>\n",
       "      <th>temp</th>\n",
       "      <th>dewp</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>dmax</th>\n",
       "      <th>dmin</th>\n",
       "      <th>hmax</th>\n",
       "      <th>hmin</th>\n",
       "      <th>hmdy</th>\n",
       "      <th>wdct</th>\n",
       "      <th>gust</th>\n",
       "      <th>wdsp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>233827.860665</td>\n",
       "      <td>0.066403</td>\n",
       "      <td>973.221608</td>\n",
       "      <td>973.454855</td>\n",
       "      <td>972.909963</td>\n",
       "      <td>1412.990445</td>\n",
       "      <td>25.965979</td>\n",
       "      <td>19.168960</td>\n",
       "      <td>26.596658</td>\n",
       "      <td>25.316963</td>\n",
       "      <td>19.726319</td>\n",
       "      <td>18.655436</td>\n",
       "      <td>72.790941</td>\n",
       "      <td>67.098345</td>\n",
       "      <td>69.860121</td>\n",
       "      <td>139.128640</td>\n",
       "      <td>5.240913</td>\n",
       "      <td>2.130079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>117366.299109</td>\n",
       "      <td>0.248985</td>\n",
       "      <td>30.731061</td>\n",
       "      <td>30.737863</td>\n",
       "      <td>30.723507</td>\n",
       "      <td>907.902161</td>\n",
       "      <td>4.182204</td>\n",
       "      <td>4.031003</td>\n",
       "      <td>4.345986</td>\n",
       "      <td>3.981500</td>\n",
       "      <td>3.995272</td>\n",
       "      <td>4.072172</td>\n",
       "      <td>19.604386</td>\n",
       "      <td>20.798260</td>\n",
       "      <td>20.288464</td>\n",
       "      <td>81.662468</td>\n",
       "      <td>2.819915</td>\n",
       "      <td>1.498585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>865.000000</td>\n",
       "      <td>865.900000</td>\n",
       "      <td>865.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>131379.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.400000</td>\n",
       "      <td>958.800000</td>\n",
       "      <td>967.000000</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>23.600000</td>\n",
       "      <td>22.700000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>282860.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>976.000000</td>\n",
       "      <td>976.500000</td>\n",
       "      <td>975.900000</td>\n",
       "      <td>1355.000000</td>\n",
       "      <td>25.600000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>26.300000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>330881.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>996.000000</td>\n",
       "      <td>996.700000</td>\n",
       "      <td>996.100000</td>\n",
       "      <td>1728.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>21.900000</td>\n",
       "      <td>29.800000</td>\n",
       "      <td>28.100000</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>3.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>376873.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>1020.200000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>6776.000000</td>\n",
       "      <td>40.800000</td>\n",
       "      <td>32.800000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>39.200000</td>\n",
       "      <td>35.100000</td>\n",
       "      <td>32.100000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>40.100000</td>\n",
       "      <td>12.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               index           prcp            stp           smax  \\\n",
       "count  321929.000000  321929.000000  321929.000000  321929.000000   \n",
       "mean   233827.860665       0.066403     973.221608     973.454855   \n",
       "std    117366.299109       0.248985      30.731061      30.737863   \n",
       "min         0.000000       0.000000     865.000000     865.900000   \n",
       "25%    131379.000000       0.000000     959.000000     959.400000   \n",
       "50%    282860.000000       0.000000     976.000000     976.500000   \n",
       "75%    330881.000000       0.000000     996.000000     996.700000   \n",
       "max    376873.000000       1.000000    1020.000000    1020.200000   \n",
       "\n",
       "                smin           gbrd           temp           dewp  \\\n",
       "count  321929.000000  321929.000000  321929.000000  321929.000000   \n",
       "mean      972.909963    1412.990445      25.965979      19.168960   \n",
       "std        30.723507     907.902161       4.182204       4.031003   \n",
       "min       865.400000       0.000000      10.400000     -10.000000   \n",
       "25%       958.800000     967.000000      23.100000      17.300000   \n",
       "50%       975.900000    1355.000000      25.600000      20.000000   \n",
       "75%       996.100000    1728.000000      29.000000      21.900000   \n",
       "max      1020.000000    6776.000000      40.800000      32.800000   \n",
       "\n",
       "                tmax           tmin           dmax           dmin  \\\n",
       "count  321929.000000  321929.000000  321929.000000  321929.000000   \n",
       "mean       26.596658      25.316963      19.726319      18.655436   \n",
       "std         4.345986       3.981500       3.995272       4.072172   \n",
       "min        10.700000      10.300000     -10.000000     -10.000000   \n",
       "25%        23.600000      22.700000      17.900000      16.700000   \n",
       "50%        26.300000      25.000000      20.500000      19.500000   \n",
       "75%        29.800000      28.100000      22.400000      21.500000   \n",
       "max        41.000000      39.200000      35.100000      32.100000   \n",
       "\n",
       "                hmax           hmin           hmdy           wdct  \\\n",
       "count  321929.000000  321929.000000  321929.000000  321929.000000   \n",
       "mean       72.790941      67.098345      69.860121     139.128640   \n",
       "std        19.604386      20.798260      20.288464      81.662468   \n",
       "min         7.000000       7.000000       7.000000       1.000000   \n",
       "25%        60.000000      52.000000      56.000000      88.000000   \n",
       "50%        76.000000      69.000000      73.000000     123.000000   \n",
       "75%        89.000000      85.000000      87.000000     172.000000   \n",
       "max       100.000000     100.000000     100.000000     360.000000   \n",
       "\n",
       "                gust           wdsp  \n",
       "count  321929.000000  321929.000000  \n",
       "mean        5.240913       2.130079  \n",
       "std         2.819915       1.498585  \n",
       "min         0.000000       0.000000  \n",
       "25%         3.100000       1.000000  \n",
       "50%         5.100000       1.900000  \n",
       "75%         7.200000       3.100000  \n",
       "max        40.100000      12.900000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf88afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9df141ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzamy ktore zmienne sa najbardziej powiazane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6a131d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson': 0.022179027515327847, 'spearman': 0.047571345343204785, 'kendall': 0.03908761714459347}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4868: RuntimeWarning: overflow encountered in longlong_scalars\n",
      "  (2 * xtie * ytie) / m + x0 * y0 / (9 * m * (size - 2)))\n"
     ]
    }
   ],
   "source": [
    "tmpx = data['stp']\n",
    "tmpy = data['prcp']\n",
    "corr = {}\n",
    "corr['pearson'], _ = stats.pearsonr(tmpx,tmpy)\n",
    "corr['spearman'], _ = stats.spearmanr(tmpx,tmpy)\n",
    "corr['kendall'], _ = stats.kendalltau(tmpx,tmpy)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8891b9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson': 0.022172733430575516, 'spearman': 0.04772238893981379, 'kendall': 0.038990106863254666}\n"
     ]
    }
   ],
   "source": [
    "tmpx = data['smax']\n",
    "tmpy = data['prcp']\n",
    "corr = {}\n",
    "corr['pearson'], _ = stats.pearsonr(tmpx,tmpy)\n",
    "corr['spearman'], _ = stats.spearmanr(tmpx,tmpy)\n",
    "corr['kendall'], _ = stats.kendalltau(tmpx,tmpy)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76ccdc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson': 0.02172559553887567, 'spearman': 0.046959992046939944, 'kendall': 0.03836715176223585}\n"
     ]
    }
   ],
   "source": [
    "tmpx = data['smin']\n",
    "tmpy = data['prcp']\n",
    "corr = {}\n",
    "corr['pearson'], _ = stats.pearsonr(tmpx,tmpy)\n",
    "corr['spearman'], _ = stats.spearmanr(tmpx,tmpy)\n",
    "corr['kendall'], _ = stats.kendalltau(tmpx,tmpy)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53b55536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson': -0.11604084944059503, 'spearman': -0.11714107976578132, 'kendall': -0.10090362966245975}\n"
     ]
    }
   ],
   "source": [
    "tmpx = data['gbrd']\n",
    "tmpy = data['prcp']\n",
    "corr = {}\n",
    "corr['pearson'], _ = stats.pearsonr(tmpx,tmpy)\n",
    "corr['spearman'], _ = stats.spearmanr(tmpx,tmpy)\n",
    "corr['kendall'], _ = stats.kendalltau(tmpx,tmpy)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4184434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson': -0.15467127848074985, 'spearman': -0.16167343123562422, 'kendall': -0.13245178850688763}\n"
     ]
    }
   ],
   "source": [
    "tmpx = data['temp']\n",
    "tmpy = data['prcp']\n",
    "corr = {}\n",
    "corr['pearson'], _ = stats.pearsonr(tmpx,tmpy)\n",
    "corr['spearman'], _ = stats.spearmanr(tmpx,tmpy)\n",
    "corr['kendall'], _ = stats.kendalltau(tmpx,tmpy)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2b55a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson': 0.11874087436919514, 'spearman': 0.1371886994513931, 'kendall': 0.11250658066447411}\n"
     ]
    }
   ],
   "source": [
    "tmpx = data['dewp']\n",
    "tmpy = data['prcp']\n",
    "corr = {}\n",
    "corr['pearson'], _ = stats.pearsonr(tmpx,tmpy)\n",
    "corr['spearman'], _ = stats.spearmanr(tmpx,tmpy)\n",
    "corr['kendall'], _ = stats.kendalltau(tmpx,tmpy)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "facedcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson': -0.13591534969018815, 'spearman': -0.13883192964740343, 'kendall': -0.1137230204761406}\n"
     ]
    }
   ],
   "source": [
    "tmpx = data['tmax']\n",
    "tmpy = data['prcp']\n",
    "corr = {}\n",
    "corr['pearson'], _ = stats.pearsonr(tmpx,tmpy)\n",
    "corr['spearman'], _ = stats.spearmanr(tmpx,tmpy)\n",
    "corr['kendall'], _ = stats.kendalltau(tmpx,tmpy)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30633825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson': -0.152136825264263, 'spearman': -0.16060004227771346, 'kendall': -0.1315995211375438}\n"
     ]
    }
   ],
   "source": [
    "tmpx = data['tmin']\n",
    "tmpy = data['prcp']\n",
    "corr = {}\n",
    "corr['pearson'], _ = stats.pearsonr(tmpx,tmpy)\n",
    "corr['spearman'], _ = stats.spearmanr(tmpx,tmpy)\n",
    "corr['kendall'], _ = stats.kendalltau(tmpx,tmpy)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f63544d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson': 0.11554925791883429, 'spearman': 0.13407557203370649, 'kendall': 0.10995796434462024}\n"
     ]
    }
   ],
   "source": [
    "tmpx = data['dmax']\n",
    "tmpy = data['prcp']\n",
    "corr = {}\n",
    "corr['pearson'], _ = stats.pearsonr(tmpx,tmpy)\n",
    "corr['spearman'], _ = stats.spearmanr(tmpx,tmpy)\n",
    "corr['kendall'], _ = stats.kendalltau(tmpx,tmpy)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e50067a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson': 0.11431684233558596, 'spearman': 0.12874437136869707, 'kendall': 0.10557385778733222}\n"
     ]
    }
   ],
   "source": [
    "tmpx = data['dmin']\n",
    "tmpy = data['prcp']\n",
    "corr = {}\n",
    "corr['pearson'], _ = stats.pearsonr(tmpx,tmpy)\n",
    "corr['spearman'], _ = stats.spearmanr(tmpx,tmpy)\n",
    "corr['kendall'], _ = stats.kendalltau(tmpx,tmpy)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67faac59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson': 0.2153333373350016, 'spearman': 0.24029730937059465, 'kendall': 0.1978241672439579}\n"
     ]
    }
   ],
   "source": [
    "tmpx = data['hmax']\n",
    "tmpy = data['prcp']\n",
    "corr = {}\n",
    "corr['pearson'], _ = stats.pearsonr(tmpx,tmpy)\n",
    "corr['spearman'], _ = stats.spearmanr(tmpx,tmpy)\n",
    "corr['kendall'], _ = stats.kendalltau(tmpx,tmpy)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36ad7a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson': 0.19941354248231488, 'spearman': 0.2100364052032935, 'kendall': 0.1726994252835604}\n"
     ]
    }
   ],
   "source": [
    "tmpx = data['hmin']\n",
    "tmpy = data['prcp']\n",
    "corr = {}\n",
    "corr['pearson'], _ = stats.pearsonr(tmpx,tmpy)\n",
    "corr['spearman'], _ = stats.spearmanr(tmpx,tmpy)\n",
    "corr['kendall'], _ = stats.kendalltau(tmpx,tmpy)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85571871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson': 0.22062465263714415, 'spearman': 0.24021700045579866, 'kendall': 0.19761322557629282}\n"
     ]
    }
   ],
   "source": [
    "tmpx = data['hmdy']\n",
    "tmpy = data['prcp']\n",
    "corr = {}\n",
    "corr['pearson'], _ = stats.pearsonr(tmpx,tmpy)\n",
    "corr['spearman'], _ = stats.spearmanr(tmpx,tmpy)\n",
    "corr['kendall'], _ = stats.kendalltau(tmpx,tmpy)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12c128f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson': 0.05776051441597784, 'spearman': 0.06257515799351267, 'kendall': 0.05124994883984977}\n"
     ]
    }
   ],
   "source": [
    "tmpx = data['wdct']\n",
    "tmpy = data['prcp']\n",
    "corr = {}\n",
    "corr['pearson'], _ = stats.pearsonr(tmpx,tmpy)\n",
    "corr['spearman'], _ = stats.spearmanr(tmpx,tmpy)\n",
    "corr['kendall'], _ = stats.kendalltau(tmpx,tmpy)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "859a367c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson': 0.02285348126327728, 'spearman': 0.01245965939506392, 'kendall': 0.010239895949053786}\n"
     ]
    }
   ],
   "source": [
    "tmpx = data['gust']\n",
    "tmpy = data['prcp']\n",
    "corr = {}\n",
    "corr['pearson'], _ = stats.pearsonr(tmpx,tmpy)\n",
    "corr['spearman'], _ = stats.spearmanr(tmpx,tmpy)\n",
    "corr['kendall'], _ = stats.kendalltau(tmpx,tmpy)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32f3e6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pearson': -0.042162082244369056, 'spearman': -0.043992337154189945, 'kendall': -0.03638264289646001}\n"
     ]
    }
   ],
   "source": [
    "tmpx = data['wdsp']\n",
    "tmpy = data['prcp']\n",
    "corr = {}\n",
    "corr['pearson'], _ = stats.pearsonr(tmpx,tmpy)\n",
    "corr['spearman'], _ = stats.spearmanr(tmpx,tmpy)\n",
    "corr['kendall'], _ = stats.kendalltau(tmpx,tmpy)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1cf18551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prcp</th>\n",
       "      <th>stp</th>\n",
       "      <th>smax</th>\n",
       "      <th>smin</th>\n",
       "      <th>gbrd</th>\n",
       "      <th>temp</th>\n",
       "      <th>dewp</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>dmax</th>\n",
       "      <th>dmin</th>\n",
       "      <th>hmax</th>\n",
       "      <th>hmin</th>\n",
       "      <th>hmdy</th>\n",
       "      <th>wdct</th>\n",
       "      <th>gust</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>station_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "      <td>321929.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.066403</td>\n",
       "      <td>973.221608</td>\n",
       "      <td>973.454855</td>\n",
       "      <td>972.909963</td>\n",
       "      <td>1412.990445</td>\n",
       "      <td>25.965979</td>\n",
       "      <td>19.168960</td>\n",
       "      <td>26.596658</td>\n",
       "      <td>25.316963</td>\n",
       "      <td>19.726319</td>\n",
       "      <td>18.655436</td>\n",
       "      <td>72.790941</td>\n",
       "      <td>67.098345</td>\n",
       "      <td>69.860121</td>\n",
       "      <td>139.128640</td>\n",
       "      <td>5.240913</td>\n",
       "      <td>2.130079</td>\n",
       "      <td>55.838738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.248985</td>\n",
       "      <td>30.731061</td>\n",
       "      <td>30.737863</td>\n",
       "      <td>30.723507</td>\n",
       "      <td>907.902161</td>\n",
       "      <td>4.182204</td>\n",
       "      <td>4.031003</td>\n",
       "      <td>4.345986</td>\n",
       "      <td>3.981500</td>\n",
       "      <td>3.995272</td>\n",
       "      <td>4.072172</td>\n",
       "      <td>19.604386</td>\n",
       "      <td>20.798260</td>\n",
       "      <td>20.288464</td>\n",
       "      <td>81.662468</td>\n",
       "      <td>2.819915</td>\n",
       "      <td>1.498585</td>\n",
       "      <td>31.119279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>865.000000</td>\n",
       "      <td>865.900000</td>\n",
       "      <td>865.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>959.000000</td>\n",
       "      <td>959.400000</td>\n",
       "      <td>958.800000</td>\n",
       "      <td>967.000000</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>23.600000</td>\n",
       "      <td>22.700000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>976.000000</td>\n",
       "      <td>976.500000</td>\n",
       "      <td>975.900000</td>\n",
       "      <td>1355.000000</td>\n",
       "      <td>25.600000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>26.300000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>996.000000</td>\n",
       "      <td>996.700000</td>\n",
       "      <td>996.100000</td>\n",
       "      <td>1728.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>21.900000</td>\n",
       "      <td>29.800000</td>\n",
       "      <td>28.100000</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>82.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>1020.200000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>6776.000000</td>\n",
       "      <td>40.800000</td>\n",
       "      <td>32.800000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>39.200000</td>\n",
       "      <td>35.100000</td>\n",
       "      <td>32.100000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>40.100000</td>\n",
       "      <td>12.900000</td>\n",
       "      <td>111.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                prcp            stp           smax           smin  \\\n",
       "count  321929.000000  321929.000000  321929.000000  321929.000000   \n",
       "mean        0.066403     973.221608     973.454855     972.909963   \n",
       "std         0.248985      30.731061      30.737863      30.723507   \n",
       "min         0.000000     865.000000     865.900000     865.400000   \n",
       "25%         0.000000     959.000000     959.400000     958.800000   \n",
       "50%         0.000000     976.000000     976.500000     975.900000   \n",
       "75%         0.000000     996.000000     996.700000     996.100000   \n",
       "max         1.000000    1020.000000    1020.200000    1020.000000   \n",
       "\n",
       "                gbrd           temp           dewp           tmax  \\\n",
       "count  321929.000000  321929.000000  321929.000000  321929.000000   \n",
       "mean     1412.990445      25.965979      19.168960      26.596658   \n",
       "std       907.902161       4.182204       4.031003       4.345986   \n",
       "min         0.000000      10.400000     -10.000000      10.700000   \n",
       "25%       967.000000      23.100000      17.300000      23.600000   \n",
       "50%      1355.000000      25.600000      20.000000      26.300000   \n",
       "75%      1728.000000      29.000000      21.900000      29.800000   \n",
       "max      6776.000000      40.800000      32.800000      41.000000   \n",
       "\n",
       "                tmin           dmax           dmin           hmax  \\\n",
       "count  321929.000000  321929.000000  321929.000000  321929.000000   \n",
       "mean       25.316963      19.726319      18.655436      72.790941   \n",
       "std         3.981500       3.995272       4.072172      19.604386   \n",
       "min        10.300000     -10.000000     -10.000000       7.000000   \n",
       "25%        22.700000      17.900000      16.700000      60.000000   \n",
       "50%        25.000000      20.500000      19.500000      76.000000   \n",
       "75%        28.100000      22.400000      21.500000      89.000000   \n",
       "max        39.200000      35.100000      32.100000     100.000000   \n",
       "\n",
       "                hmin           hmdy           wdct           gust  \\\n",
       "count  321929.000000  321929.000000  321929.000000  321929.000000   \n",
       "mean       67.098345      69.860121     139.128640       5.240913   \n",
       "std        20.798260      20.288464      81.662468       2.819915   \n",
       "min         7.000000       7.000000       1.000000       0.000000   \n",
       "25%        52.000000      56.000000      88.000000       3.100000   \n",
       "50%        69.000000      73.000000     123.000000       5.100000   \n",
       "75%        85.000000      87.000000     172.000000       7.200000   \n",
       "max       100.000000     100.000000     360.000000      40.100000   \n",
       "\n",
       "                wdsp  station_label  \n",
       "count  321929.000000  321929.000000  \n",
       "mean        2.130079      55.838738  \n",
       "std         1.498585      31.119279  \n",
       "min         0.000000       1.000000  \n",
       "25%         1.000000      29.000000  \n",
       "50%         1.900000      55.000000  \n",
       "75%         3.100000      82.000000  \n",
       "max        12.900000     111.000000  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zmieniamy dane kateryczne ze stringa na inty\n",
    "data['station_label'] = data['station_code'].rank(method='dense', ascending=False).astype(int)\n",
    "data = data.drop(['station_code', 'index'], axis=1)\n",
    "# data = data.drop(['index'], axis=1)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "caf3c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "978835ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.concat([zeros_subset[1:500], ones_subset[1:500]], ignore_index=True)\n",
    "# df.value_counts()\n",
    "# sns.pairplot(df)#, kind=\"reg\"\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "988a1f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Names\\AppData\\Local\\Temp\\ipykernel_12028\\3097445852.py:6: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask = np.zeros_like(corr, dtype=np.bool)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x648 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prcp</th>\n",
       "      <th>stp</th>\n",
       "      <th>smax</th>\n",
       "      <th>smin</th>\n",
       "      <th>gbrd</th>\n",
       "      <th>temp</th>\n",
       "      <th>dewp</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>dmax</th>\n",
       "      <th>dmin</th>\n",
       "      <th>hmax</th>\n",
       "      <th>hmin</th>\n",
       "      <th>hmdy</th>\n",
       "      <th>wdct</th>\n",
       "      <th>gust</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>station_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prcp</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022179</td>\n",
       "      <td>0.022173</td>\n",
       "      <td>0.021726</td>\n",
       "      <td>-0.116041</td>\n",
       "      <td>-0.154671</td>\n",
       "      <td>0.118741</td>\n",
       "      <td>-0.135915</td>\n",
       "      <td>-0.152137</td>\n",
       "      <td>0.115549</td>\n",
       "      <td>0.114317</td>\n",
       "      <td>0.215333</td>\n",
       "      <td>0.199414</td>\n",
       "      <td>0.220625</td>\n",
       "      <td>0.057761</td>\n",
       "      <td>0.022853</td>\n",
       "      <td>-0.042162</td>\n",
       "      <td>0.021773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stp</th>\n",
       "      <td>0.022179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.999905</td>\n",
       "      <td>-0.023383</td>\n",
       "      <td>0.365138</td>\n",
       "      <td>0.480004</td>\n",
       "      <td>0.338611</td>\n",
       "      <td>0.382162</td>\n",
       "      <td>0.483252</td>\n",
       "      <td>0.480118</td>\n",
       "      <td>0.088358</td>\n",
       "      <td>0.086347</td>\n",
       "      <td>0.082822</td>\n",
       "      <td>0.017325</td>\n",
       "      <td>-0.123748</td>\n",
       "      <td>-0.128619</td>\n",
       "      <td>0.211562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smax</th>\n",
       "      <td>0.022173</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999952</td>\n",
       "      <td>-0.016664</td>\n",
       "      <td>0.370064</td>\n",
       "      <td>0.479166</td>\n",
       "      <td>0.343287</td>\n",
       "      <td>0.386683</td>\n",
       "      <td>0.483063</td>\n",
       "      <td>0.479089</td>\n",
       "      <td>0.084656</td>\n",
       "      <td>0.082521</td>\n",
       "      <td>0.078756</td>\n",
       "      <td>0.017055</td>\n",
       "      <td>-0.121357</td>\n",
       "      <td>-0.127062</td>\n",
       "      <td>0.211773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smin</th>\n",
       "      <td>0.021726</td>\n",
       "      <td>0.999905</td>\n",
       "      <td>0.999952</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.018900</td>\n",
       "      <td>0.367461</td>\n",
       "      <td>0.479323</td>\n",
       "      <td>0.340637</td>\n",
       "      <td>0.384285</td>\n",
       "      <td>0.482934</td>\n",
       "      <td>0.479452</td>\n",
       "      <td>0.086454</td>\n",
       "      <td>0.084526</td>\n",
       "      <td>0.080689</td>\n",
       "      <td>0.017119</td>\n",
       "      <td>-0.122878</td>\n",
       "      <td>-0.127917</td>\n",
       "      <td>0.211370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbrd</th>\n",
       "      <td>-0.116041</td>\n",
       "      <td>-0.023383</td>\n",
       "      <td>-0.016664</td>\n",
       "      <td>-0.018900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.451300</td>\n",
       "      <td>-0.145543</td>\n",
       "      <td>0.418678</td>\n",
       "      <td>0.392360</td>\n",
       "      <td>-0.079027</td>\n",
       "      <td>-0.159748</td>\n",
       "      <td>-0.383408</td>\n",
       "      <td>-0.408508</td>\n",
       "      <td>-0.439958</td>\n",
       "      <td>-0.080359</td>\n",
       "      <td>0.260289</td>\n",
       "      <td>0.231242</td>\n",
       "      <td>-0.043262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>-0.154671</td>\n",
       "      <td>0.365138</td>\n",
       "      <td>0.370064</td>\n",
       "      <td>0.367461</td>\n",
       "      <td>0.451300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007118</td>\n",
       "      <td>0.980845</td>\n",
       "      <td>0.980493</td>\n",
       "      <td>0.056717</td>\n",
       "      <td>-0.047140</td>\n",
       "      <td>-0.701884</td>\n",
       "      <td>-0.727741</td>\n",
       "      <td>-0.735149</td>\n",
       "      <td>-0.121481</td>\n",
       "      <td>0.325811</td>\n",
       "      <td>0.246469</td>\n",
       "      <td>0.163822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dewp</th>\n",
       "      <td>0.118741</td>\n",
       "      <td>0.480004</td>\n",
       "      <td>0.479166</td>\n",
       "      <td>0.479323</td>\n",
       "      <td>-0.145543</td>\n",
       "      <td>-0.007118</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024969</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.981042</td>\n",
       "      <td>0.979447</td>\n",
       "      <td>0.669371</td>\n",
       "      <td>0.620962</td>\n",
       "      <td>0.647487</td>\n",
       "      <td>0.047654</td>\n",
       "      <td>-0.227228</td>\n",
       "      <td>-0.187721</td>\n",
       "      <td>0.221175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tmax</th>\n",
       "      <td>-0.135915</td>\n",
       "      <td>0.338611</td>\n",
       "      <td>0.343287</td>\n",
       "      <td>0.340637</td>\n",
       "      <td>0.418678</td>\n",
       "      <td>0.980845</td>\n",
       "      <td>-0.024969</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974856</td>\n",
       "      <td>0.038550</td>\n",
       "      <td>-0.070731</td>\n",
       "      <td>-0.715646</td>\n",
       "      <td>-0.755798</td>\n",
       "      <td>-0.738481</td>\n",
       "      <td>-0.116005</td>\n",
       "      <td>0.354490</td>\n",
       "      <td>0.263888</td>\n",
       "      <td>0.150056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tmin</th>\n",
       "      <td>-0.152137</td>\n",
       "      <td>0.382162</td>\n",
       "      <td>0.386683</td>\n",
       "      <td>0.384285</td>\n",
       "      <td>0.392360</td>\n",
       "      <td>0.980493</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.974856</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057987</td>\n",
       "      <td>-0.031229</td>\n",
       "      <td>-0.705077</td>\n",
       "      <td>-0.710815</td>\n",
       "      <td>-0.707618</td>\n",
       "      <td>-0.115220</td>\n",
       "      <td>0.308934</td>\n",
       "      <td>0.229243</td>\n",
       "      <td>0.180049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmax</th>\n",
       "      <td>0.115549</td>\n",
       "      <td>0.483252</td>\n",
       "      <td>0.483063</td>\n",
       "      <td>0.482934</td>\n",
       "      <td>-0.079027</td>\n",
       "      <td>0.056717</td>\n",
       "      <td>0.981042</td>\n",
       "      <td>0.038550</td>\n",
       "      <td>0.057987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964109</td>\n",
       "      <td>0.632560</td>\n",
       "      <td>0.567341</td>\n",
       "      <td>0.589085</td>\n",
       "      <td>0.037550</td>\n",
       "      <td>-0.189885</td>\n",
       "      <td>-0.155964</td>\n",
       "      <td>0.217005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dmin</th>\n",
       "      <td>0.114317</td>\n",
       "      <td>0.480118</td>\n",
       "      <td>0.479089</td>\n",
       "      <td>0.479452</td>\n",
       "      <td>-0.159748</td>\n",
       "      <td>-0.047140</td>\n",
       "      <td>0.979447</td>\n",
       "      <td>-0.070731</td>\n",
       "      <td>-0.031229</td>\n",
       "      <td>0.964109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.694333</td>\n",
       "      <td>0.663016</td>\n",
       "      <td>0.670956</td>\n",
       "      <td>0.050464</td>\n",
       "      <td>-0.252762</td>\n",
       "      <td>-0.206457</td>\n",
       "      <td>0.224864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hmax</th>\n",
       "      <td>0.215333</td>\n",
       "      <td>0.088358</td>\n",
       "      <td>0.084656</td>\n",
       "      <td>0.086454</td>\n",
       "      <td>-0.383408</td>\n",
       "      <td>-0.701884</td>\n",
       "      <td>0.669371</td>\n",
       "      <td>-0.715646</td>\n",
       "      <td>-0.705077</td>\n",
       "      <td>0.632560</td>\n",
       "      <td>0.694333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972853</td>\n",
       "      <td>0.979095</td>\n",
       "      <td>0.131051</td>\n",
       "      <td>-0.410701</td>\n",
       "      <td>-0.330667</td>\n",
       "      <td>0.031376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hmin</th>\n",
       "      <td>0.199414</td>\n",
       "      <td>0.086347</td>\n",
       "      <td>0.082521</td>\n",
       "      <td>0.084526</td>\n",
       "      <td>-0.408508</td>\n",
       "      <td>-0.727741</td>\n",
       "      <td>0.620962</td>\n",
       "      <td>-0.755798</td>\n",
       "      <td>-0.710815</td>\n",
       "      <td>0.567341</td>\n",
       "      <td>0.663016</td>\n",
       "      <td>0.972853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982250</td>\n",
       "      <td>0.138509</td>\n",
       "      <td>-0.452334</td>\n",
       "      <td>-0.366613</td>\n",
       "      <td>0.051754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hmdy</th>\n",
       "      <td>0.220625</td>\n",
       "      <td>0.082822</td>\n",
       "      <td>0.078756</td>\n",
       "      <td>0.080689</td>\n",
       "      <td>-0.439958</td>\n",
       "      <td>-0.735149</td>\n",
       "      <td>0.647487</td>\n",
       "      <td>-0.738481</td>\n",
       "      <td>-0.707618</td>\n",
       "      <td>0.589085</td>\n",
       "      <td>0.670956</td>\n",
       "      <td>0.979095</td>\n",
       "      <td>0.982250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.141169</td>\n",
       "      <td>-0.428277</td>\n",
       "      <td>-0.352856</td>\n",
       "      <td>0.041345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wdct</th>\n",
       "      <td>0.057761</td>\n",
       "      <td>0.017325</td>\n",
       "      <td>0.017055</td>\n",
       "      <td>0.017119</td>\n",
       "      <td>-0.080359</td>\n",
       "      <td>-0.121481</td>\n",
       "      <td>0.047654</td>\n",
       "      <td>-0.116005</td>\n",
       "      <td>-0.115220</td>\n",
       "      <td>0.037550</td>\n",
       "      <td>0.050464</td>\n",
       "      <td>0.131051</td>\n",
       "      <td>0.138509</td>\n",
       "      <td>0.141169</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.221013</td>\n",
       "      <td>-0.222895</td>\n",
       "      <td>-0.007111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gust</th>\n",
       "      <td>0.022853</td>\n",
       "      <td>-0.123748</td>\n",
       "      <td>-0.121357</td>\n",
       "      <td>-0.122878</td>\n",
       "      <td>0.260289</td>\n",
       "      <td>0.325811</td>\n",
       "      <td>-0.227228</td>\n",
       "      <td>0.354490</td>\n",
       "      <td>0.308934</td>\n",
       "      <td>-0.189885</td>\n",
       "      <td>-0.252762</td>\n",
       "      <td>-0.410701</td>\n",
       "      <td>-0.452334</td>\n",
       "      <td>-0.428277</td>\n",
       "      <td>-0.221013</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.854106</td>\n",
       "      <td>-0.003957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wdsp</th>\n",
       "      <td>-0.042162</td>\n",
       "      <td>-0.128619</td>\n",
       "      <td>-0.127062</td>\n",
       "      <td>-0.127917</td>\n",
       "      <td>0.231242</td>\n",
       "      <td>0.246469</td>\n",
       "      <td>-0.187721</td>\n",
       "      <td>0.263888</td>\n",
       "      <td>0.229243</td>\n",
       "      <td>-0.155964</td>\n",
       "      <td>-0.206457</td>\n",
       "      <td>-0.330667</td>\n",
       "      <td>-0.366613</td>\n",
       "      <td>-0.352856</td>\n",
       "      <td>-0.222895</td>\n",
       "      <td>0.854106</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.046875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_label</th>\n",
       "      <td>0.021773</td>\n",
       "      <td>0.211562</td>\n",
       "      <td>0.211773</td>\n",
       "      <td>0.211370</td>\n",
       "      <td>-0.043262</td>\n",
       "      <td>0.163822</td>\n",
       "      <td>0.221175</td>\n",
       "      <td>0.150056</td>\n",
       "      <td>0.180049</td>\n",
       "      <td>0.217005</td>\n",
       "      <td>0.224864</td>\n",
       "      <td>0.031376</td>\n",
       "      <td>0.051754</td>\n",
       "      <td>0.041345</td>\n",
       "      <td>-0.007111</td>\n",
       "      <td>-0.003957</td>\n",
       "      <td>-0.046875</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   prcp       stp      smax      smin      gbrd      temp  \\\n",
       "prcp           1.000000  0.022179  0.022173  0.021726 -0.116041 -0.154671   \n",
       "stp            0.022179  1.000000  0.999889  0.999905 -0.023383  0.365138   \n",
       "smax           0.022173  0.999889  1.000000  0.999952 -0.016664  0.370064   \n",
       "smin           0.021726  0.999905  0.999952  1.000000 -0.018900  0.367461   \n",
       "gbrd          -0.116041 -0.023383 -0.016664 -0.018900  1.000000  0.451300   \n",
       "temp          -0.154671  0.365138  0.370064  0.367461  0.451300  1.000000   \n",
       "dewp           0.118741  0.480004  0.479166  0.479323 -0.145543 -0.007118   \n",
       "tmax          -0.135915  0.338611  0.343287  0.340637  0.418678  0.980845   \n",
       "tmin          -0.152137  0.382162  0.386683  0.384285  0.392360  0.980493   \n",
       "dmax           0.115549  0.483252  0.483063  0.482934 -0.079027  0.056717   \n",
       "dmin           0.114317  0.480118  0.479089  0.479452 -0.159748 -0.047140   \n",
       "hmax           0.215333  0.088358  0.084656  0.086454 -0.383408 -0.701884   \n",
       "hmin           0.199414  0.086347  0.082521  0.084526 -0.408508 -0.727741   \n",
       "hmdy           0.220625  0.082822  0.078756  0.080689 -0.439958 -0.735149   \n",
       "wdct           0.057761  0.017325  0.017055  0.017119 -0.080359 -0.121481   \n",
       "gust           0.022853 -0.123748 -0.121357 -0.122878  0.260289  0.325811   \n",
       "wdsp          -0.042162 -0.128619 -0.127062 -0.127917  0.231242  0.246469   \n",
       "station_label  0.021773  0.211562  0.211773  0.211370 -0.043262  0.163822   \n",
       "\n",
       "                   dewp      tmax      tmin      dmax      dmin      hmax  \\\n",
       "prcp           0.118741 -0.135915 -0.152137  0.115549  0.114317  0.215333   \n",
       "stp            0.480004  0.338611  0.382162  0.483252  0.480118  0.088358   \n",
       "smax           0.479166  0.343287  0.386683  0.483063  0.479089  0.084656   \n",
       "smin           0.479323  0.340637  0.384285  0.482934  0.479452  0.086454   \n",
       "gbrd          -0.145543  0.418678  0.392360 -0.079027 -0.159748 -0.383408   \n",
       "temp          -0.007118  0.980845  0.980493  0.056717 -0.047140 -0.701884   \n",
       "dewp           1.000000 -0.024969  0.004491  0.981042  0.979447  0.669371   \n",
       "tmax          -0.024969  1.000000  0.974856  0.038550 -0.070731 -0.715646   \n",
       "tmin           0.004491  0.974856  1.000000  0.057987 -0.031229 -0.705077   \n",
       "dmax           0.981042  0.038550  0.057987  1.000000  0.964109  0.632560   \n",
       "dmin           0.979447 -0.070731 -0.031229  0.964109  1.000000  0.694333   \n",
       "hmax           0.669371 -0.715646 -0.705077  0.632560  0.694333  1.000000   \n",
       "hmin           0.620962 -0.755798 -0.710815  0.567341  0.663016  0.972853   \n",
       "hmdy           0.647487 -0.738481 -0.707618  0.589085  0.670956  0.979095   \n",
       "wdct           0.047654 -0.116005 -0.115220  0.037550  0.050464  0.131051   \n",
       "gust          -0.227228  0.354490  0.308934 -0.189885 -0.252762 -0.410701   \n",
       "wdsp          -0.187721  0.263888  0.229243 -0.155964 -0.206457 -0.330667   \n",
       "station_label  0.221175  0.150056  0.180049  0.217005  0.224864  0.031376   \n",
       "\n",
       "                   hmin      hmdy      wdct      gust      wdsp  station_label  \n",
       "prcp           0.199414  0.220625  0.057761  0.022853 -0.042162       0.021773  \n",
       "stp            0.086347  0.082822  0.017325 -0.123748 -0.128619       0.211562  \n",
       "smax           0.082521  0.078756  0.017055 -0.121357 -0.127062       0.211773  \n",
       "smin           0.084526  0.080689  0.017119 -0.122878 -0.127917       0.211370  \n",
       "gbrd          -0.408508 -0.439958 -0.080359  0.260289  0.231242      -0.043262  \n",
       "temp          -0.727741 -0.735149 -0.121481  0.325811  0.246469       0.163822  \n",
       "dewp           0.620962  0.647487  0.047654 -0.227228 -0.187721       0.221175  \n",
       "tmax          -0.755798 -0.738481 -0.116005  0.354490  0.263888       0.150056  \n",
       "tmin          -0.710815 -0.707618 -0.115220  0.308934  0.229243       0.180049  \n",
       "dmax           0.567341  0.589085  0.037550 -0.189885 -0.155964       0.217005  \n",
       "dmin           0.663016  0.670956  0.050464 -0.252762 -0.206457       0.224864  \n",
       "hmax           0.972853  0.979095  0.131051 -0.410701 -0.330667       0.031376  \n",
       "hmin           1.000000  0.982250  0.138509 -0.452334 -0.366613       0.051754  \n",
       "hmdy           0.982250  1.000000  0.141169 -0.428277 -0.352856       0.041345  \n",
       "wdct           0.138509  0.141169  1.000000 -0.221013 -0.222895      -0.007111  \n",
       "gust          -0.452334 -0.428277 -0.221013  1.000000  0.854106      -0.003957  \n",
       "wdsp          -0.366613 -0.352856 -0.222895  0.854106  1.000000      -0.046875  \n",
       "station_label  0.051754  0.041345 -0.007111 -0.003957 -0.046875       1.000000  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "corr = df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1,\n",
    "            square=True, xticklabels=5, yticklabels=5,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "plt.show()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef2fa05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# porzucamy dane słabo skorelowane\n",
    "data = data.drop(['stp', 'smax', 'smin', 'wdct', 'gust', 'wdsp', 'date', 'hr',], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6efb983b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    300552\n",
       "1.0     21377\n",
       "Name: prcp, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset nie jest zbyt zbalansowany, mamu dużo więcej przypadków w których nie pada, niż pada\n",
    "data['prcp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8cf5977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rzedy danych wydzielone na validation: 4276\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# balansujemy dataset\n",
    "ones_subset = data.loc[data[\"prcp\"] == 1, :]\n",
    "number_of_1s = len(ones_subset)\n",
    "\n",
    "zeros_subset = data.loc[data[\"prcp\"] == 0, :]\n",
    "sampled_zeros = zeros_subset.sample(number_of_1s)\n",
    "\n",
    "# wydzielamy zbior walidacyjny\n",
    "val_rows = round(0.1*number_of_1s)\n",
    "\n",
    "X_val = pd.concat([ones_subset[0:val_rows], sampled_zeros[0:val_rows]], ignore_index=True)\n",
    "y_val = X_val['prcp'].values\n",
    "\n",
    "X_val = X_val.drop(['prcp'], axis=1)\n",
    "\n",
    "print(\"rzedy danych wydzielone na validation:\", len(X_val))\n",
    "\n",
    "data = pd.concat([ones_subset[val_rows:], sampled_zeros[val_rows:]], ignore_index=True)\n",
    "\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60290bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    38478.000000\n",
       "mean         0.500000\n",
       "std          0.500006\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.500000\n",
       "75%          1.000000\n",
       "max          1.000000\n",
       "Name: prcp, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['prcp'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7398ae8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    19239\n",
       "0.0    19239\n",
       "Name: prcp, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['prcp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e07672e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dzielimy na train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d031924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop(['prcp'], axis=1)\n",
    "y = data['prcp'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "667b1d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "import mlxtend\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aa4a8e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=123\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8c14284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30782, 11) (30782,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a768e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "36de8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import uniform, randint\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_distribution_xgb = {\n",
    "    'preprocessing': [None, StandardScaler()],\n",
    "    'max_depth': randint(3, 11),\n",
    "    'learning_rate': uniform(0.001, 0.1-0.001),\n",
    "    'n_estimators': randint(50, 400),\n",
    "    'gamma': uniform(0,2),\n",
    "    'colsample_bytree': uniform(0.5, 0.5),\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    'min_child_weight': randint(1, 11)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec8d6b4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:00:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:00:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:00:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:00:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:00:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:00:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:00:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:00:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:00:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:00:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:00:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:01:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:01:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:01:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:01:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:01:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:01:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:01:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:01:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:01:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:01:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:01:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:02:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:02:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:02:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:02:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:02:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:02:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:02:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:02:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:03:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:03:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:03:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:03:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:03:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:03:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:03:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:03:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:04:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:04:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:04:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:04:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:04:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:04:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:05:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:05:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:05:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:05:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:05:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:05:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:06:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:06:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:06:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:06:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:06:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:06:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"preprocessing\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "0.825872594340235\n"
     ]
    }
   ],
   "source": [
    "pipe1 = XGBClassifier(n_jobs=-1)\n",
    "\n",
    "grid_1 = RandomizedSearchCV(\n",
    "    estimator=pipe1, \n",
    "    n_iter=20, \n",
    "    param_distributions=param_distribution_xgb, \n",
    "    cv=kfold, \n",
    "    return_train_score=True, \n",
    "    verbose=0)\n",
    "\n",
    "grid_1.fit(X_train, y_train)\n",
    "grid_1.best_params_\n",
    "print(grid_1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b3a8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC z jądrem rbf albo linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7ec9baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribution_svc = {\n",
    "    'preprocessing': [None, StandardScaler()],\n",
    "    'classifier__C': uniform(0.001, 1000),\n",
    "    'classifier__max_iter': randint(10, 400),\n",
    "    'classifier__gamma': uniform(0,2),\n",
    "    'classifier__kernel': ['rbf', 'linear']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "15e34cca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=296).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=296).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=296).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=296).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=296).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=245).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=245).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=245).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=245).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=245).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=182).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=182).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=182).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=182).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=182).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=324).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=324).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=324).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=324).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=324).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=114).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=114).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=114).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=114).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=114).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=191).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=191).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=191).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=191).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=191).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=287).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=287).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=287).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=287).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=287).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=119).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=119).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=119).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=119).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=119).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=196).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=196).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=196).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=196).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=196).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=378).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=378).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=378).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=378).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=378).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=378).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=378).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=378).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=378).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=378).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=18).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=18).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=18).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=18).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=18).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=305).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=305).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=305).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=305).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=305).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=313).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=313).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=313).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=313).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=313).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=293).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=293).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=293).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=293).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=293).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=297).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=297).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=297).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=297).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=297).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=280).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=280).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=280).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=280).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=280).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=82).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=82).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=82).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=82).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=82).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=48).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=48).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=48).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=48).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=48).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=312).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=312).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=312).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=312).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=312).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6099651653511332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\names\\desktop\\psi\\configs\\desktopppsiprojekt\\lib\\site-packages\\sklearn\\svm\\_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=18).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pipe2 = Pipeline([('preprocessing', StandardScaler()), ('classifier', SVC(kernel='rbf'))])\n",
    "\n",
    "grid_2 = RandomizedSearchCV(pipe2, n_iter=20, param_distributions=param_distribution_svc, cv=kfold, return_train_score=True, verbose=0)\n",
    "grid_2.fit(X_train, y_train)\n",
    "\n",
    "grid_2.best_params_\n",
    "print(grid_2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6778d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "261874df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b23b5d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skalujemy dane\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8a81131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wybieramy metode aktywacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e40d8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      " sigmoid \n",
      "\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 200)               2400      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 150)               30150     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                7550      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,151\n",
      "Trainable params: 40,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.5664 - accuracy: 0.7137 - val_loss: 0.5428 - val_accuracy: 0.7327\n",
      "Epoch 2/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5399 - accuracy: 0.7308 - val_loss: 0.5327 - val_accuracy: 0.7344\n",
      "Epoch 3/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5325 - accuracy: 0.7333 - val_loss: 0.5278 - val_accuracy: 0.7310\n",
      "Epoch 4/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5278 - accuracy: 0.7360 - val_loss: 0.5272 - val_accuracy: 0.7349\n",
      "Epoch 5/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5222 - accuracy: 0.7364 - val_loss: 0.5192 - val_accuracy: 0.7378\n",
      "Epoch 6/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5169 - accuracy: 0.7384 - val_loss: 0.5140 - val_accuracy: 0.7401\n",
      "Epoch 7/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5135 - accuracy: 0.7399 - val_loss: 0.5166 - val_accuracy: 0.7400\n",
      "Epoch 8/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5106 - accuracy: 0.7442 - val_loss: 0.5103 - val_accuracy: 0.7418\n",
      "Epoch 9/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5091 - accuracy: 0.7463 - val_loss: 0.5090 - val_accuracy: 0.7462\n",
      "Epoch 10/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5076 - accuracy: 0.7466 - val_loss: 0.5152 - val_accuracy: 0.7471\n",
      "Epoch 11/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5070 - accuracy: 0.7479 - val_loss: 0.5082 - val_accuracy: 0.7460\n",
      "Epoch 12/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5052 - accuracy: 0.7497 - val_loss: 0.5061 - val_accuracy: 0.7540\n",
      "Epoch 13/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5042 - accuracy: 0.7497 - val_loss: 0.5038 - val_accuracy: 0.7483\n",
      "Epoch 14/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5025 - accuracy: 0.7519 - val_loss: 0.5073 - val_accuracy: 0.7514\n",
      "Epoch 15/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5004 - accuracy: 0.7516 - val_loss: 0.5019 - val_accuracy: 0.7530\n",
      "Epoch 16/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5001 - accuracy: 0.7532 - val_loss: 0.5021 - val_accuracy: 0.7527\n",
      "Epoch 17/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4986 - accuracy: 0.7528 - val_loss: 0.5006 - val_accuracy: 0.7512\n",
      "Epoch 18/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4975 - accuracy: 0.7541 - val_loss: 0.4995 - val_accuracy: 0.7488\n",
      "Epoch 19/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4959 - accuracy: 0.7554 - val_loss: 0.4965 - val_accuracy: 0.7548\n",
      "Epoch 20/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4950 - accuracy: 0.7556 - val_loss: 0.4985 - val_accuracy: 0.7549\n",
      "Epoch 21/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4945 - accuracy: 0.7564 - val_loss: 0.4979 - val_accuracy: 0.7548\n",
      "Epoch 22/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4928 - accuracy: 0.7583 - val_loss: 0.4947 - val_accuracy: 0.7526\n",
      "Epoch 23/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4922 - accuracy: 0.7587 - val_loss: 0.4970 - val_accuracy: 0.7525\n",
      "Epoch 24/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4912 - accuracy: 0.7582 - val_loss: 0.4947 - val_accuracy: 0.7560\n",
      "Epoch 25/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4904 - accuracy: 0.7592 - val_loss: 0.4951 - val_accuracy: 0.7551\n",
      "Epoch 26/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4895 - accuracy: 0.7592 - val_loss: 0.5103 - val_accuracy: 0.7475\n",
      "Epoch 27/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4890 - accuracy: 0.7600 - val_loss: 0.4913 - val_accuracy: 0.7568\n",
      "Epoch 28/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4881 - accuracy: 0.7603 - val_loss: 0.4953 - val_accuracy: 0.7558\n",
      "Epoch 29/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4871 - accuracy: 0.7621 - val_loss: 0.4893 - val_accuracy: 0.7552\n",
      "Epoch 30/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4865 - accuracy: 0.7632 - val_loss: 0.4938 - val_accuracy: 0.7581\n",
      "Epoch 31/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4852 - accuracy: 0.7611 - val_loss: 0.5082 - val_accuracy: 0.7509\n",
      "Epoch 32/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4843 - accuracy: 0.7627 - val_loss: 0.4885 - val_accuracy: 0.7609\n",
      "Epoch 33/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4837 - accuracy: 0.7638 - val_loss: 0.4875 - val_accuracy: 0.7600\n",
      "Epoch 34/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4824 - accuracy: 0.7633 - val_loss: 0.4851 - val_accuracy: 0.7579\n",
      "Epoch 35/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4819 - accuracy: 0.7658 - val_loss: 0.4993 - val_accuracy: 0.7523\n",
      "Epoch 36/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4813 - accuracy: 0.7678 - val_loss: 0.4948 - val_accuracy: 0.7595\n",
      "Epoch 37/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4805 - accuracy: 0.7662 - val_loss: 0.4881 - val_accuracy: 0.7584\n",
      "Epoch 38/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4793 - accuracy: 0.7672 - val_loss: 0.4860 - val_accuracy: 0.7581\n",
      "Epoch 39/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4790 - accuracy: 0.7671 - val_loss: 0.4842 - val_accuracy: 0.7616\n",
      "Epoch 40/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4789 - accuracy: 0.7671 - val_loss: 0.4842 - val_accuracy: 0.7594\n",
      "Epoch 41/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4769 - accuracy: 0.7682 - val_loss: 0.5016 - val_accuracy: 0.7564\n",
      "Epoch 42/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4772 - accuracy: 0.7672 - val_loss: 0.4826 - val_accuracy: 0.7570\n",
      "Epoch 43/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4763 - accuracy: 0.7693 - val_loss: 0.4875 - val_accuracy: 0.7583\n",
      "Epoch 44/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4757 - accuracy: 0.7666 - val_loss: 0.4895 - val_accuracy: 0.7575\n",
      "Epoch 45/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4755 - accuracy: 0.7687 - val_loss: 0.4804 - val_accuracy: 0.7613\n",
      "Epoch 46/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4754 - accuracy: 0.7699 - val_loss: 0.4806 - val_accuracy: 0.7586\n",
      "Epoch 47/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4746 - accuracy: 0.7691 - val_loss: 0.4881 - val_accuracy: 0.7564\n",
      "Epoch 48/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4734 - accuracy: 0.7689 - val_loss: 0.4876 - val_accuracy: 0.7582\n",
      "Epoch 49/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4734 - accuracy: 0.7695 - val_loss: 0.4797 - val_accuracy: 0.7643\n",
      "Epoch 50/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4733 - accuracy: 0.7719 - val_loss: 0.4817 - val_accuracy: 0.7622\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4720 - accuracy: 0.7712 - val_loss: 0.4772 - val_accuracy: 0.7640\n",
      "Epoch 52/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4716 - accuracy: 0.7722 - val_loss: 0.4806 - val_accuracy: 0.7622\n",
      "Epoch 53/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4719 - accuracy: 0.7717 - val_loss: 0.4792 - val_accuracy: 0.7651\n",
      "Epoch 54/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4703 - accuracy: 0.7724 - val_loss: 0.4827 - val_accuracy: 0.7634\n",
      "Epoch 55/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4708 - accuracy: 0.7727 - val_loss: 0.4758 - val_accuracy: 0.7656\n",
      "Epoch 56/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4698 - accuracy: 0.7730 - val_loss: 0.4776 - val_accuracy: 0.7648\n",
      "Epoch 57/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4691 - accuracy: 0.7740 - val_loss: 0.4866 - val_accuracy: 0.7584\n",
      "Epoch 58/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4692 - accuracy: 0.7736 - val_loss: 0.4741 - val_accuracy: 0.7685\n",
      "Epoch 59/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4690 - accuracy: 0.7733 - val_loss: 0.4763 - val_accuracy: 0.7664\n",
      "Epoch 60/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4681 - accuracy: 0.7744 - val_loss: 0.4824 - val_accuracy: 0.7646\n",
      "Epoch 61/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4675 - accuracy: 0.7749 - val_loss: 0.4747 - val_accuracy: 0.7634\n",
      "Epoch 62/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4669 - accuracy: 0.7750 - val_loss: 0.4766 - val_accuracy: 0.7682\n",
      "Epoch 63/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4663 - accuracy: 0.7758 - val_loss: 0.4759 - val_accuracy: 0.7669\n",
      "Epoch 64/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4661 - accuracy: 0.7757 - val_loss: 0.4749 - val_accuracy: 0.7616\n",
      "Epoch 65/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4650 - accuracy: 0.7746 - val_loss: 0.4747 - val_accuracy: 0.7656\n",
      "Epoch 66/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4646 - accuracy: 0.7755 - val_loss: 0.4869 - val_accuracy: 0.7665\n",
      "Epoch 67/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4646 - accuracy: 0.7741 - val_loss: 0.4710 - val_accuracy: 0.7696\n",
      "Epoch 68/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4634 - accuracy: 0.7763 - val_loss: 0.4713 - val_accuracy: 0.7704\n",
      "Epoch 69/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4629 - accuracy: 0.7773 - val_loss: 0.4732 - val_accuracy: 0.7683\n",
      "Epoch 70/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.4622 - accuracy: 0.7756 - val_loss: 0.4756 - val_accuracy: 0.7672\n",
      "Epoch 71/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4625 - accuracy: 0.7770 - val_loss: 0.4685 - val_accuracy: 0.7682\n",
      "Epoch 72/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4614 - accuracy: 0.7780 - val_loss: 0.4704 - val_accuracy: 0.7707\n",
      "Epoch 73/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.4612 - accuracy: 0.7780 - val_loss: 0.4736 - val_accuracy: 0.7717\n",
      "Epoch 74/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4612 - accuracy: 0.7770 - val_loss: 0.4729 - val_accuracy: 0.7710\n",
      "Epoch 75/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4598 - accuracy: 0.7786 - val_loss: 0.4702 - val_accuracy: 0.7712\n",
      "Epoch 76/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4594 - accuracy: 0.7804 - val_loss: 0.4782 - val_accuracy: 0.7677\n",
      "Epoch 77/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4588 - accuracy: 0.7804 - val_loss: 0.4732 - val_accuracy: 0.7716\n",
      "Epoch 78/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4586 - accuracy: 0.7802 - val_loss: 0.4719 - val_accuracy: 0.7685\n",
      "Epoch 79/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4574 - accuracy: 0.7810 - val_loss: 0.4673 - val_accuracy: 0.7725\n",
      "Epoch 80/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.4573 - accuracy: 0.7802 - val_loss: 0.4714 - val_accuracy: 0.7752\n",
      "Epoch 81/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4565 - accuracy: 0.7812 - val_loss: 0.4718 - val_accuracy: 0.7700\n",
      "Epoch 82/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4567 - accuracy: 0.7819 - val_loss: 0.4685 - val_accuracy: 0.7761\n",
      "Epoch 83/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.4560 - accuracy: 0.7817 - val_loss: 0.4665 - val_accuracy: 0.7752\n",
      "Epoch 84/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.4553 - accuracy: 0.7825 - val_loss: 0.4680 - val_accuracy: 0.7727\n",
      "Epoch 85/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4549 - accuracy: 0.7824 - val_loss: 0.4663 - val_accuracy: 0.7726\n",
      "Epoch 86/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4541 - accuracy: 0.7822 - val_loss: 0.4749 - val_accuracy: 0.7729\n",
      "Epoch 87/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4539 - accuracy: 0.7822 - val_loss: 0.4710 - val_accuracy: 0.7734\n",
      "Epoch 88/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4539 - accuracy: 0.7843 - val_loss: 0.4659 - val_accuracy: 0.7775\n",
      "Epoch 89/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4530 - accuracy: 0.7845 - val_loss: 0.4663 - val_accuracy: 0.7751\n",
      "Epoch 90/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4525 - accuracy: 0.7820 - val_loss: 0.4636 - val_accuracy: 0.7794\n",
      "Epoch 91/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4519 - accuracy: 0.7833 - val_loss: 0.4651 - val_accuracy: 0.7746\n",
      "Epoch 92/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4514 - accuracy: 0.7849 - val_loss: 0.4659 - val_accuracy: 0.7739\n",
      "Epoch 93/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.4508 - accuracy: 0.7829 - val_loss: 0.4704 - val_accuracy: 0.7764\n",
      "Epoch 94/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.4497 - accuracy: 0.7856 - val_loss: 0.4670 - val_accuracy: 0.7777\n",
      "Epoch 95/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4492 - accuracy: 0.7867 - val_loss: 0.4801 - val_accuracy: 0.7751\n",
      "Epoch 96/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4489 - accuracy: 0.7860 - val_loss: 0.4843 - val_accuracy: 0.7661\n",
      "Epoch 97/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4490 - accuracy: 0.7841 - val_loss: 0.4682 - val_accuracy: 0.7726\n",
      "Epoch 98/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4478 - accuracy: 0.7874 - val_loss: 0.4675 - val_accuracy: 0.7772\n",
      "Epoch 99/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4477 - accuracy: 0.7855 - val_loss: 0.4621 - val_accuracy: 0.7783\n",
      "Epoch 100/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4478 - accuracy: 0.7883 - val_loss: 0.4714 - val_accuracy: 0.7757\n",
      "Epoch 101/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4477 - accuracy: 0.7868 - val_loss: 0.4657 - val_accuracy: 0.7777\n",
      "Epoch 102/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4461 - accuracy: 0.7872 - val_loss: 0.4639 - val_accuracy: 0.7804\n",
      "Epoch 103/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4459 - accuracy: 0.7874 - val_loss: 0.4663 - val_accuracy: 0.7734\n",
      "Epoch 104/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4454 - accuracy: 0.7885 - val_loss: 0.4620 - val_accuracy: 0.7798\n",
      "Epoch 105/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4452 - accuracy: 0.7878 - val_loss: 0.4671 - val_accuracy: 0.7761\n",
      "Epoch 106/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4448 - accuracy: 0.7892 - val_loss: 0.4621 - val_accuracy: 0.7768\n",
      "Epoch 107/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4439 - accuracy: 0.7909 - val_loss: 0.4611 - val_accuracy: 0.7805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4440 - accuracy: 0.7893 - val_loss: 0.4757 - val_accuracy: 0.7752\n",
      "Epoch 109/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4441 - accuracy: 0.7888 - val_loss: 0.4688 - val_accuracy: 0.7773\n",
      "Epoch 110/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4423 - accuracy: 0.7902 - val_loss: 0.4613 - val_accuracy: 0.7801\n",
      "Epoch 111/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4421 - accuracy: 0.7895 - val_loss: 0.4684 - val_accuracy: 0.7714\n",
      "Epoch 112/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4417 - accuracy: 0.7902 - val_loss: 0.4619 - val_accuracy: 0.7812\n",
      "Epoch 113/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4412 - accuracy: 0.7916 - val_loss: 0.4614 - val_accuracy: 0.7826\n",
      "Epoch 114/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4415 - accuracy: 0.7912 - val_loss: 0.4667 - val_accuracy: 0.7770\n",
      "Epoch 115/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4402 - accuracy: 0.7929 - val_loss: 0.4563 - val_accuracy: 0.7835\n",
      "Epoch 116/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4398 - accuracy: 0.7923 - val_loss: 0.4685 - val_accuracy: 0.7735\n",
      "Epoch 117/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4408 - accuracy: 0.7909 - val_loss: 0.4577 - val_accuracy: 0.7817\n",
      "Epoch 118/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4391 - accuracy: 0.7918 - val_loss: 0.4659 - val_accuracy: 0.7817\n",
      "Epoch 119/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4394 - accuracy: 0.7941 - val_loss: 0.4602 - val_accuracy: 0.7852\n",
      "Epoch 120/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4382 - accuracy: 0.7916 - val_loss: 0.4572 - val_accuracy: 0.7817\n",
      "Epoch 121/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4382 - accuracy: 0.7935 - val_loss: 0.4631 - val_accuracy: 0.7796\n",
      "Epoch 122/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4387 - accuracy: 0.7935 - val_loss: 0.4578 - val_accuracy: 0.7821\n",
      "Epoch 123/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4376 - accuracy: 0.7927 - val_loss: 0.4610 - val_accuracy: 0.7818\n",
      "Epoch 124/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4372 - accuracy: 0.7946 - val_loss: 0.4600 - val_accuracy: 0.7807\n",
      "Epoch 125/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4361 - accuracy: 0.7948 - val_loss: 0.4589 - val_accuracy: 0.7814\n",
      "Epoch 126/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4370 - accuracy: 0.7954 - val_loss: 0.4615 - val_accuracy: 0.7798\n",
      "Epoch 127/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4362 - accuracy: 0.7946 - val_loss: 0.4583 - val_accuracy: 0.7807\n",
      "Epoch 128/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4354 - accuracy: 0.7946 - val_loss: 0.4561 - val_accuracy: 0.7817\n",
      "Epoch 129/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4347 - accuracy: 0.7972 - val_loss: 0.4577 - val_accuracy: 0.7800\n",
      "Epoch 130/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4361 - accuracy: 0.7953 - val_loss: 0.4562 - val_accuracy: 0.7838\n",
      "Epoch 131/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4340 - accuracy: 0.7959 - val_loss: 0.4657 - val_accuracy: 0.7769\n",
      "Epoch 132/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4341 - accuracy: 0.7972 - val_loss: 0.4551 - val_accuracy: 0.7852\n",
      "Epoch 133/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4339 - accuracy: 0.7953 - val_loss: 0.4637 - val_accuracy: 0.7800\n",
      "Epoch 134/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4335 - accuracy: 0.7955 - val_loss: 0.4611 - val_accuracy: 0.7785\n",
      "Epoch 135/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4339 - accuracy: 0.7952 - val_loss: 0.4555 - val_accuracy: 0.7848\n",
      "Epoch 136/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4333 - accuracy: 0.7959 - val_loss: 0.4506 - val_accuracy: 0.7844\n",
      "Epoch 137/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4329 - accuracy: 0.7960 - val_loss: 0.4554 - val_accuracy: 0.7822\n",
      "Epoch 138/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4325 - accuracy: 0.7974 - val_loss: 0.4666 - val_accuracy: 0.7808\n",
      "Epoch 139/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4323 - accuracy: 0.7962 - val_loss: 0.4631 - val_accuracy: 0.7814\n",
      "Epoch 140/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4306 - accuracy: 0.7978 - val_loss: 0.4595 - val_accuracy: 0.7808\n",
      "Epoch 141/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4312 - accuracy: 0.7969 - val_loss: 0.4603 - val_accuracy: 0.7816\n",
      "Epoch 142/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4297 - accuracy: 0.7970 - val_loss: 0.4582 - val_accuracy: 0.7812\n",
      "Epoch 143/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4298 - accuracy: 0.7991 - val_loss: 0.4611 - val_accuracy: 0.7807\n",
      "Epoch 144/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4305 - accuracy: 0.7973 - val_loss: 0.4570 - val_accuracy: 0.7787\n",
      "Epoch 145/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4303 - accuracy: 0.7977 - val_loss: 0.4532 - val_accuracy: 0.7818\n",
      "Epoch 146/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4303 - accuracy: 0.7984 - val_loss: 0.4564 - val_accuracy: 0.7846\n",
      "Epoch 147/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4297 - accuracy: 0.7980 - val_loss: 0.4537 - val_accuracy: 0.7833\n",
      "Epoch 148/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4285 - accuracy: 0.7983 - val_loss: 0.4644 - val_accuracy: 0.7812\n",
      "Epoch 149/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4293 - accuracy: 0.7983 - val_loss: 0.4590 - val_accuracy: 0.7822\n",
      "Epoch 150/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4289 - accuracy: 0.7989 - val_loss: 0.4525 - val_accuracy: 0.7846\n",
      "Epoch 151/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4284 - accuracy: 0.7997 - val_loss: 0.4616 - val_accuracy: 0.7761\n",
      "Epoch 152/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4288 - accuracy: 0.7992 - val_loss: 0.4625 - val_accuracy: 0.7783\n",
      "Epoch 153/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4274 - accuracy: 0.8002 - val_loss: 0.4673 - val_accuracy: 0.7833\n",
      "Epoch 154/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4279 - accuracy: 0.7983 - val_loss: 0.4523 - val_accuracy: 0.7874\n",
      "Epoch 155/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4270 - accuracy: 0.7989 - val_loss: 0.4542 - val_accuracy: 0.7824\n",
      "Epoch 156/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4270 - accuracy: 0.7986 - val_loss: 0.4564 - val_accuracy: 0.7856\n",
      "Epoch 157/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4264 - accuracy: 0.8001 - val_loss: 0.4573 - val_accuracy: 0.7864\n",
      "Epoch 158/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4259 - accuracy: 0.7998 - val_loss: 0.4646 - val_accuracy: 0.7879\n",
      "Epoch 159/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4259 - accuracy: 0.8011 - val_loss: 0.4589 - val_accuracy: 0.7813\n",
      "Epoch 160/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4255 - accuracy: 0.8007 - val_loss: 0.4625 - val_accuracy: 0.7837\n",
      "Epoch 161/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4250 - accuracy: 0.8009 - val_loss: 0.4592 - val_accuracy: 0.7799\n",
      "Epoch 162/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4252 - accuracy: 0.8002 - val_loss: 0.4554 - val_accuracy: 0.7816\n",
      "Epoch 163/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4251 - accuracy: 0.8033 - val_loss: 0.4602 - val_accuracy: 0.7804\n",
      "Epoch 164/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4243 - accuracy: 0.7997 - val_loss: 0.4623 - val_accuracy: 0.7805\n",
      "Epoch 165/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4242 - accuracy: 0.8023 - val_loss: 0.4588 - val_accuracy: 0.7840\n",
      "Epoch 166/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4237 - accuracy: 0.8032 - val_loss: 0.4564 - val_accuracy: 0.7844\n",
      "Epoch 167/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4239 - accuracy: 0.8012 - val_loss: 0.4596 - val_accuracy: 0.7855\n",
      "Epoch 168/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4240 - accuracy: 0.8007 - val_loss: 0.4608 - val_accuracy: 0.7825\n",
      "Epoch 169/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4238 - accuracy: 0.8024 - val_loss: 0.4544 - val_accuracy: 0.7834\n",
      "Epoch 170/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4230 - accuracy: 0.8016 - val_loss: 0.4579 - val_accuracy: 0.7855\n",
      "Epoch 171/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4227 - accuracy: 0.8033 - val_loss: 0.4536 - val_accuracy: 0.7864\n",
      "Epoch 172/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4227 - accuracy: 0.8034 - val_loss: 0.4549 - val_accuracy: 0.7843\n",
      "Epoch 173/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4222 - accuracy: 0.8033 - val_loss: 0.4555 - val_accuracy: 0.7837\n",
      "Epoch 174/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4225 - accuracy: 0.8022 - val_loss: 0.4542 - val_accuracy: 0.7866\n",
      "Epoch 175/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4223 - accuracy: 0.8039 - val_loss: 0.4582 - val_accuracy: 0.7826\n",
      "Epoch 176/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4215 - accuracy: 0.8015 - val_loss: 0.4662 - val_accuracy: 0.7825\n",
      "Epoch 177/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4217 - accuracy: 0.8028 - val_loss: 0.4567 - val_accuracy: 0.7855\n",
      "Epoch 178/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4207 - accuracy: 0.8040 - val_loss: 0.4648 - val_accuracy: 0.7821\n",
      "Epoch 179/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4206 - accuracy: 0.8045 - val_loss: 0.4617 - val_accuracy: 0.7851\n",
      "Epoch 180/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4212 - accuracy: 0.8051 - val_loss: 0.4605 - val_accuracy: 0.7824\n",
      "Epoch 181/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4202 - accuracy: 0.8042 - val_loss: 0.4681 - val_accuracy: 0.7811\n",
      "Epoch 182/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4211 - accuracy: 0.8039 - val_loss: 0.4553 - val_accuracy: 0.7827\n",
      "Epoch 183/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4201 - accuracy: 0.8059 - val_loss: 0.4577 - val_accuracy: 0.7855\n",
      "Epoch 184/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4197 - accuracy: 0.8037 - val_loss: 0.4556 - val_accuracy: 0.7868\n",
      "Epoch 185/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4197 - accuracy: 0.8051 - val_loss: 0.4527 - val_accuracy: 0.7890\n",
      "Epoch 186/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4198 - accuracy: 0.8030 - val_loss: 0.4574 - val_accuracy: 0.7856\n",
      "Epoch 187/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4192 - accuracy: 0.8062 - val_loss: 0.4558 - val_accuracy: 0.7866\n",
      "Epoch 188/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4190 - accuracy: 0.8073 - val_loss: 0.4563 - val_accuracy: 0.7838\n",
      "Epoch 189/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4200 - accuracy: 0.8031 - val_loss: 0.4608 - val_accuracy: 0.7879\n",
      "Epoch 190/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4191 - accuracy: 0.8043 - val_loss: 0.4559 - val_accuracy: 0.7813\n",
      "Epoch 191/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4187 - accuracy: 0.8054 - val_loss: 0.4589 - val_accuracy: 0.7820\n",
      "Epoch 192/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4193 - accuracy: 0.8046 - val_loss: 0.4551 - val_accuracy: 0.7874\n",
      "Epoch 193/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4187 - accuracy: 0.8037 - val_loss: 0.4522 - val_accuracy: 0.7860\n",
      "Epoch 194/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4184 - accuracy: 0.8040 - val_loss: 0.4537 - val_accuracy: 0.7878\n",
      "Epoch 195/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4181 - accuracy: 0.8055 - val_loss: 0.4563 - val_accuracy: 0.7848\n",
      "Epoch 196/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4176 - accuracy: 0.8070 - val_loss: 0.4562 - val_accuracy: 0.7882\n",
      "Epoch 197/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4183 - accuracy: 0.8047 - val_loss: 0.4553 - val_accuracy: 0.7857\n",
      "Epoch 198/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4167 - accuracy: 0.8054 - val_loss: 0.4557 - val_accuracy: 0.7853\n",
      "Epoch 199/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4161 - accuracy: 0.8059 - val_loss: 0.4560 - val_accuracy: 0.7887\n",
      "Epoch 200/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4164 - accuracy: 0.8063 - val_loss: 0.4580 - val_accuracy: 0.7837\n",
      "Epoch 201/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4171 - accuracy: 0.8069 - val_loss: 0.4561 - val_accuracy: 0.7840\n",
      "Epoch 202/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4163 - accuracy: 0.8075 - val_loss: 0.4545 - val_accuracy: 0.7839\n",
      "Epoch 203/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4159 - accuracy: 0.8065 - val_loss: 0.4518 - val_accuracy: 0.7865\n",
      "Epoch 204/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4150 - accuracy: 0.8062 - val_loss: 0.4520 - val_accuracy: 0.7847\n",
      "Epoch 205/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4154 - accuracy: 0.8059 - val_loss: 0.4558 - val_accuracy: 0.7853\n",
      "Epoch 206/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4149 - accuracy: 0.8095 - val_loss: 0.4494 - val_accuracy: 0.7903\n",
      "Epoch 207/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4139 - accuracy: 0.8076 - val_loss: 0.4547 - val_accuracy: 0.7843\n",
      "Epoch 208/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4140 - accuracy: 0.8060 - val_loss: 0.4573 - val_accuracy: 0.7886\n",
      "Epoch 209/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4144 - accuracy: 0.8077 - val_loss: 0.4589 - val_accuracy: 0.7842\n",
      "Epoch 210/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.4135 - accuracy: 0.8086 - val_loss: 0.4619 - val_accuracy: 0.7808\n",
      "Epoch 211/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.4143 - accuracy: 0.8063 - val_loss: 0.4612 - val_accuracy: 0.7820\n",
      "Epoch 212/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4134 - accuracy: 0.8074 - val_loss: 0.4520 - val_accuracy: 0.7886\n",
      "Epoch 213/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4127 - accuracy: 0.8084 - val_loss: 0.4613 - val_accuracy: 0.7850\n",
      "Epoch 214/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4138 - accuracy: 0.8068 - val_loss: 0.4599 - val_accuracy: 0.7792\n",
      "Epoch 215/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4124 - accuracy: 0.8094 - val_loss: 0.4591 - val_accuracy: 0.7896\n",
      "Epoch 216/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4129 - accuracy: 0.8079 - val_loss: 0.4621 - val_accuracy: 0.7855\n",
      "Epoch 217/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4125 - accuracy: 0.8080 - val_loss: 0.4607 - val_accuracy: 0.7856\n",
      "Epoch 218/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.4127 - accuracy: 0.8087 - val_loss: 0.4547 - val_accuracy: 0.7878\n",
      "Epoch 219/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.4117 - accuracy: 0.8100 - val_loss: 0.4625 - val_accuracy: 0.7878\n",
      "Epoch 220/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4122 - accuracy: 0.8089 - val_loss: 0.4518 - val_accuracy: 0.7908\n",
      "Epoch 221/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4123 - accuracy: 0.8096 - val_loss: 0.4589 - val_accuracy: 0.7842\n",
      "Epoch 222/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4115 - accuracy: 0.8104 - val_loss: 0.4567 - val_accuracy: 0.7922\n",
      "Epoch 223/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4123 - accuracy: 0.8076 - val_loss: 0.4564 - val_accuracy: 0.7861\n",
      "Epoch 224/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4118 - accuracy: 0.8091 - val_loss: 0.4635 - val_accuracy: 0.7853\n",
      "Epoch 225/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4102 - accuracy: 0.8096 - val_loss: 0.4558 - val_accuracy: 0.7844\n",
      "Epoch 226/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4105 - accuracy: 0.8082 - val_loss: 0.4625 - val_accuracy: 0.7853\n",
      "Epoch 227/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4112 - accuracy: 0.8083 - val_loss: 0.4547 - val_accuracy: 0.7913\n",
      "Epoch 228/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4112 - accuracy: 0.8090 - val_loss: 0.4542 - val_accuracy: 0.7889\n",
      "Epoch 229/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4105 - accuracy: 0.8098 - val_loss: 0.4607 - val_accuracy: 0.7896\n",
      "Epoch 230/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4104 - accuracy: 0.8091 - val_loss: 0.4521 - val_accuracy: 0.7900\n",
      "Epoch 231/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4103 - accuracy: 0.8099 - val_loss: 0.4573 - val_accuracy: 0.7894\n",
      "Epoch 232/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4093 - accuracy: 0.8094 - val_loss: 0.4583 - val_accuracy: 0.7896\n",
      "Epoch 233/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4091 - accuracy: 0.8107 - val_loss: 0.4587 - val_accuracy: 0.7857\n",
      "Epoch 234/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4086 - accuracy: 0.8122 - val_loss: 0.4613 - val_accuracy: 0.7898\n",
      "Epoch 235/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4088 - accuracy: 0.8101 - val_loss: 0.4571 - val_accuracy: 0.7904\n",
      "Epoch 236/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4100 - accuracy: 0.8099 - val_loss: 0.4622 - val_accuracy: 0.7861\n",
      "Epoch 237/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4081 - accuracy: 0.8117 - val_loss: 0.4578 - val_accuracy: 0.7864\n",
      "Epoch 238/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4081 - accuracy: 0.8104 - val_loss: 0.4562 - val_accuracy: 0.7887\n",
      "Epoch 239/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4079 - accuracy: 0.8118 - val_loss: 0.4545 - val_accuracy: 0.7859\n",
      "Epoch 240/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4079 - accuracy: 0.8117 - val_loss: 0.4637 - val_accuracy: 0.7855\n",
      "Epoch 241/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4085 - accuracy: 0.8099 - val_loss: 0.4550 - val_accuracy: 0.7935\n",
      "Epoch 242/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4072 - accuracy: 0.8117 - val_loss: 0.4556 - val_accuracy: 0.7874\n",
      "Epoch 243/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4073 - accuracy: 0.8108 - val_loss: 0.4603 - val_accuracy: 0.7861\n",
      "Epoch 244/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4081 - accuracy: 0.8111 - val_loss: 0.4557 - val_accuracy: 0.7878\n",
      "Epoch 245/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4065 - accuracy: 0.8120 - val_loss: 0.4627 - val_accuracy: 0.7869\n",
      "Epoch 246/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4068 - accuracy: 0.8119 - val_loss: 0.4664 - val_accuracy: 0.7773\n",
      "Epoch 247/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4061 - accuracy: 0.8128 - val_loss: 0.4615 - val_accuracy: 0.7898\n",
      "Epoch 248/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4058 - accuracy: 0.8124 - val_loss: 0.4744 - val_accuracy: 0.7853\n",
      "Epoch 249/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4060 - accuracy: 0.8130 - val_loss: 0.4552 - val_accuracy: 0.7935\n",
      "Epoch 250/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4056 - accuracy: 0.8126 - val_loss: 0.4584 - val_accuracy: 0.7915\n",
      "Epoch 251/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4052 - accuracy: 0.8133 - val_loss: 0.4595 - val_accuracy: 0.7912\n",
      "Epoch 252/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4055 - accuracy: 0.8120 - val_loss: 0.4623 - val_accuracy: 0.7850\n",
      "Epoch 253/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4042 - accuracy: 0.8131 - val_loss: 0.4557 - val_accuracy: 0.7898\n",
      "Epoch 254/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4048 - accuracy: 0.8118 - val_loss: 0.4580 - val_accuracy: 0.7879\n",
      "Epoch 255/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4045 - accuracy: 0.8126 - val_loss: 0.4595 - val_accuracy: 0.7898\n",
      "Epoch 256/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4035 - accuracy: 0.8117 - val_loss: 0.4733 - val_accuracy: 0.7765\n",
      "Epoch 257/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4044 - accuracy: 0.8138 - val_loss: 0.4702 - val_accuracy: 0.7891\n",
      "Epoch 258/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4040 - accuracy: 0.8155 - val_loss: 0.4596 - val_accuracy: 0.7877\n",
      "Epoch 259/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4043 - accuracy: 0.8140 - val_loss: 0.4525 - val_accuracy: 0.7891\n",
      "Epoch 260/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4042 - accuracy: 0.8149 - val_loss: 0.4667 - val_accuracy: 0.7843\n",
      "Epoch 261/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4026 - accuracy: 0.8152 - val_loss: 0.4579 - val_accuracy: 0.7903\n",
      "Epoch 262/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4033 - accuracy: 0.8136 - val_loss: 0.4584 - val_accuracy: 0.7881\n",
      "Epoch 263/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4027 - accuracy: 0.8150 - val_loss: 0.4629 - val_accuracy: 0.7878\n",
      "Epoch 264/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4020 - accuracy: 0.8161 - val_loss: 0.4562 - val_accuracy: 0.7909\n",
      "Epoch 265/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4023 - accuracy: 0.8137 - val_loss: 0.4553 - val_accuracy: 0.7894\n",
      "Epoch 266/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4027 - accuracy: 0.8148 - val_loss: 0.4549 - val_accuracy: 0.7943\n",
      "Epoch 267/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4020 - accuracy: 0.8150 - val_loss: 0.4599 - val_accuracy: 0.7883\n",
      "Epoch 268/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4026 - accuracy: 0.8140 - val_loss: 0.4609 - val_accuracy: 0.7861\n",
      "Epoch 269/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4013 - accuracy: 0.8158 - val_loss: 0.4610 - val_accuracy: 0.7890\n",
      "Epoch 270/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4022 - accuracy: 0.8154 - val_loss: 0.4595 - val_accuracy: 0.7900\n",
      "Epoch 271/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4020 - accuracy: 0.8162 - val_loss: 0.4605 - val_accuracy: 0.7876\n",
      "Epoch 272/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4012 - accuracy: 0.8160 - val_loss: 0.4643 - val_accuracy: 0.7826\n",
      "Epoch 273/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4010 - accuracy: 0.8146 - val_loss: 0.4613 - val_accuracy: 0.7894\n",
      "Epoch 274/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4009 - accuracy: 0.8153 - val_loss: 0.4603 - val_accuracy: 0.7881\n",
      "Epoch 275/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4007 - accuracy: 0.8160 - val_loss: 0.4617 - val_accuracy: 0.7872\n",
      "Epoch 276/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4001 - accuracy: 0.8147 - val_loss: 0.4562 - val_accuracy: 0.7929\n",
      "Epoch 277/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4006 - accuracy: 0.8154 - val_loss: 0.4652 - val_accuracy: 0.7869\n",
      "Epoch 278/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4005 - accuracy: 0.8167 - val_loss: 0.4711 - val_accuracy: 0.7804\n",
      "Epoch 279/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4004 - accuracy: 0.8158 - val_loss: 0.4555 - val_accuracy: 0.7922\n",
      "Epoch 280/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4007 - accuracy: 0.8177 - val_loss: 0.4530 - val_accuracy: 0.7904\n",
      "Epoch 281/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3996 - accuracy: 0.8169 - val_loss: 0.4669 - val_accuracy: 0.7881\n",
      "Epoch 282/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3993 - accuracy: 0.8156 - val_loss: 0.4629 - val_accuracy: 0.7881\n",
      "Epoch 283/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4000 - accuracy: 0.8155 - val_loss: 0.4602 - val_accuracy: 0.7837\n",
      "Epoch 284/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3993 - accuracy: 0.8159 - val_loss: 0.4584 - val_accuracy: 0.7892\n",
      "Epoch 285/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3989 - accuracy: 0.8174 - val_loss: 0.4587 - val_accuracy: 0.7876\n",
      "Epoch 286/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3988 - accuracy: 0.8186 - val_loss: 0.4590 - val_accuracy: 0.7912\n",
      "Epoch 287/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3990 - accuracy: 0.8167 - val_loss: 0.4583 - val_accuracy: 0.7912\n",
      "Epoch 288/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3968 - accuracy: 0.8195 - val_loss: 0.4607 - val_accuracy: 0.7905\n",
      "Epoch 289/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3982 - accuracy: 0.8172 - val_loss: 0.4598 - val_accuracy: 0.7951\n",
      "Epoch 290/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3971 - accuracy: 0.8176 - val_loss: 0.4539 - val_accuracy: 0.7924\n",
      "Epoch 291/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3970 - accuracy: 0.8188 - val_loss: 0.4572 - val_accuracy: 0.7930\n",
      "Epoch 292/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3971 - accuracy: 0.8189 - val_loss: 0.4644 - val_accuracy: 0.7904\n",
      "Epoch 293/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3979 - accuracy: 0.8187 - val_loss: 0.4628 - val_accuracy: 0.7921\n",
      "Epoch 294/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3978 - accuracy: 0.8176 - val_loss: 0.4519 - val_accuracy: 0.7926\n",
      "Epoch 295/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3975 - accuracy: 0.8183 - val_loss: 0.4517 - val_accuracy: 0.7912\n",
      "Epoch 296/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3972 - accuracy: 0.8180 - val_loss: 0.4710 - val_accuracy: 0.7842\n",
      "Epoch 297/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3980 - accuracy: 0.8189 - val_loss: 0.4555 - val_accuracy: 0.7916\n",
      "Epoch 298/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3971 - accuracy: 0.8186 - val_loss: 0.4591 - val_accuracy: 0.7920\n",
      "Epoch 299/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3967 - accuracy: 0.8177 - val_loss: 0.4598 - val_accuracy: 0.7894\n",
      "Epoch 300/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3965 - accuracy: 0.8173 - val_loss: 0.4537 - val_accuracy: 0.7934\n",
      "Epoch 301/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3957 - accuracy: 0.8191 - val_loss: 0.4595 - val_accuracy: 0.7870\n",
      "Epoch 302/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3962 - accuracy: 0.8165 - val_loss: 0.4540 - val_accuracy: 0.7934\n",
      "Epoch 303/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3963 - accuracy: 0.8182 - val_loss: 0.4565 - val_accuracy: 0.7898\n",
      "Epoch 304/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3960 - accuracy: 0.8188 - val_loss: 0.4640 - val_accuracy: 0.7863\n",
      "Epoch 305/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3953 - accuracy: 0.8173 - val_loss: 0.4586 - val_accuracy: 0.7927\n",
      "Epoch 306/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3963 - accuracy: 0.8199 - val_loss: 0.4591 - val_accuracy: 0.7929\n",
      "Epoch 307/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3954 - accuracy: 0.8186 - val_loss: 0.4572 - val_accuracy: 0.7917\n",
      "Epoch 308/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3959 - accuracy: 0.8203 - val_loss: 0.4640 - val_accuracy: 0.7818\n",
      "Epoch 309/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3949 - accuracy: 0.8179 - val_loss: 0.4570 - val_accuracy: 0.7930\n",
      "Epoch 310/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3953 - accuracy: 0.8176 - val_loss: 0.4593 - val_accuracy: 0.7896\n",
      "Epoch 311/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3944 - accuracy: 0.8198 - val_loss: 0.4666 - val_accuracy: 0.7865\n",
      "Epoch 312/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3948 - accuracy: 0.8207 - val_loss: 0.4612 - val_accuracy: 0.7885\n",
      "Epoch 313/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3943 - accuracy: 0.8206 - val_loss: 0.4618 - val_accuracy: 0.7850\n",
      "Epoch 314/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3937 - accuracy: 0.8206 - val_loss: 0.4629 - val_accuracy: 0.7876\n",
      "Epoch 315/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3939 - accuracy: 0.8201 - val_loss: 0.4626 - val_accuracy: 0.7948\n",
      "Epoch 316/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3943 - accuracy: 0.8195 - val_loss: 0.4599 - val_accuracy: 0.7912\n",
      "Epoch 317/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3937 - accuracy: 0.8183 - val_loss: 0.4668 - val_accuracy: 0.7900\n",
      "Epoch 318/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3925 - accuracy: 0.8197 - val_loss: 0.4603 - val_accuracy: 0.7883\n",
      "Epoch 319/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3936 - accuracy: 0.8196 - val_loss: 0.4619 - val_accuracy: 0.7881\n",
      "Epoch 320/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3925 - accuracy: 0.8198 - val_loss: 0.4599 - val_accuracy: 0.7921\n",
      "Epoch 321/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3940 - accuracy: 0.8197 - val_loss: 0.4604 - val_accuracy: 0.7896\n",
      "Epoch 322/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3939 - accuracy: 0.8197 - val_loss: 0.4576 - val_accuracy: 0.7922\n",
      "Epoch 323/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3931 - accuracy: 0.8202 - val_loss: 0.4589 - val_accuracy: 0.7946\n",
      "Epoch 324/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3932 - accuracy: 0.8199 - val_loss: 0.4720 - val_accuracy: 0.7852\n",
      "Epoch 325/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3929 - accuracy: 0.8200 - val_loss: 0.4532 - val_accuracy: 0.7938\n",
      "Epoch 326/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3930 - accuracy: 0.8212 - val_loss: 0.4567 - val_accuracy: 0.7917\n",
      "Epoch 327/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3921 - accuracy: 0.8210 - val_loss: 0.4618 - val_accuracy: 0.7886\n",
      "Epoch 328/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3921 - accuracy: 0.8207 - val_loss: 0.4577 - val_accuracy: 0.7904\n",
      "Epoch 329/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3916 - accuracy: 0.8216 - val_loss: 0.4541 - val_accuracy: 0.7921\n",
      "Epoch 330/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3924 - accuracy: 0.8207 - val_loss: 0.4557 - val_accuracy: 0.7925\n",
      "Epoch 331/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3914 - accuracy: 0.8216 - val_loss: 0.4594 - val_accuracy: 0.7895\n",
      "Epoch 332/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3912 - accuracy: 0.8203 - val_loss: 0.4560 - val_accuracy: 0.7930\n",
      "Epoch 333/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3913 - accuracy: 0.8210 - val_loss: 0.4662 - val_accuracy: 0.7915\n",
      "Epoch 334/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3912 - accuracy: 0.8204 - val_loss: 0.4620 - val_accuracy: 0.7909\n",
      "Epoch 335/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3914 - accuracy: 0.8215 - val_loss: 0.4585 - val_accuracy: 0.7912\n",
      "Epoch 336/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3922 - accuracy: 0.8194 - val_loss: 0.4647 - val_accuracy: 0.7930\n",
      "Epoch 337/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3909 - accuracy: 0.8211 - val_loss: 0.4644 - val_accuracy: 0.7879\n",
      "Epoch 338/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3904 - accuracy: 0.8217 - val_loss: 0.4591 - val_accuracy: 0.7899\n",
      "Epoch 339/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3904 - accuracy: 0.8205 - val_loss: 0.4567 - val_accuracy: 0.7953\n",
      "Epoch 340/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3901 - accuracy: 0.8205 - val_loss: 0.4642 - val_accuracy: 0.7881\n",
      "Epoch 341/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3911 - accuracy: 0.8217 - val_loss: 0.4532 - val_accuracy: 0.7955\n",
      "Epoch 342/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3895 - accuracy: 0.8215 - val_loss: 0.4595 - val_accuracy: 0.7892\n",
      "Epoch 343/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3899 - accuracy: 0.8218 - val_loss: 0.4532 - val_accuracy: 0.7964\n",
      "Epoch 344/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3901 - accuracy: 0.8210 - val_loss: 0.4626 - val_accuracy: 0.7927\n",
      "Epoch 345/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3892 - accuracy: 0.8239 - val_loss: 0.4637 - val_accuracy: 0.7909\n",
      "Epoch 346/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3901 - accuracy: 0.8204 - val_loss: 0.4608 - val_accuracy: 0.7912\n",
      "Epoch 347/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3895 - accuracy: 0.8208 - val_loss: 0.4574 - val_accuracy: 0.7933\n",
      "Epoch 348/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3890 - accuracy: 0.8206 - val_loss: 0.4571 - val_accuracy: 0.7943\n",
      "Epoch 349/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3887 - accuracy: 0.8248 - val_loss: 0.4618 - val_accuracy: 0.7930\n",
      "Epoch 350/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3873 - accuracy: 0.8229 - val_loss: 0.4545 - val_accuracy: 0.7938\n",
      "Epoch 351/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3875 - accuracy: 0.8241 - val_loss: 0.4624 - val_accuracy: 0.7900\n",
      "Epoch 352/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3887 - accuracy: 0.8238 - val_loss: 0.4547 - val_accuracy: 0.7970\n",
      "Epoch 353/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3890 - accuracy: 0.8226 - val_loss: 0.4557 - val_accuracy: 0.7918\n",
      "Epoch 354/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3880 - accuracy: 0.8233 - val_loss: 0.4599 - val_accuracy: 0.7889\n",
      "Epoch 355/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3876 - accuracy: 0.8223 - val_loss: 0.4617 - val_accuracy: 0.7900\n",
      "Epoch 356/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3877 - accuracy: 0.8248 - val_loss: 0.4588 - val_accuracy: 0.7883\n",
      "Epoch 357/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3884 - accuracy: 0.8231 - val_loss: 0.4561 - val_accuracy: 0.7917\n",
      "Epoch 358/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3868 - accuracy: 0.8229 - val_loss: 0.4662 - val_accuracy: 0.7883\n",
      "Epoch 359/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3873 - accuracy: 0.8236 - val_loss: 0.4680 - val_accuracy: 0.7863\n",
      "Epoch 360/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3866 - accuracy: 0.8221 - val_loss: 0.4490 - val_accuracy: 0.7960\n",
      "Epoch 361/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3861 - accuracy: 0.8241 - val_loss: 0.4743 - val_accuracy: 0.7876\n",
      "Epoch 362/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3879 - accuracy: 0.8237 - val_loss: 0.4573 - val_accuracy: 0.7933\n",
      "Epoch 363/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3865 - accuracy: 0.8244 - val_loss: 0.4676 - val_accuracy: 0.7892\n",
      "Epoch 364/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3862 - accuracy: 0.8227 - val_loss: 0.4604 - val_accuracy: 0.7907\n",
      "Epoch 365/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3867 - accuracy: 0.8246 - val_loss: 0.4543 - val_accuracy: 0.7892\n",
      "Epoch 366/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3869 - accuracy: 0.8232 - val_loss: 0.4697 - val_accuracy: 0.7861\n",
      "Epoch 367/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3862 - accuracy: 0.8220 - val_loss: 0.4631 - val_accuracy: 0.7970\n",
      "Epoch 368/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3864 - accuracy: 0.8244 - val_loss: 0.4614 - val_accuracy: 0.7924\n",
      "Epoch 369/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3863 - accuracy: 0.8235 - val_loss: 0.4613 - val_accuracy: 0.7912\n",
      "Epoch 370/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3848 - accuracy: 0.8252 - val_loss: 0.4626 - val_accuracy: 0.7892\n",
      "Epoch 371/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3859 - accuracy: 0.8244 - val_loss: 0.4701 - val_accuracy: 0.7861\n",
      "Epoch 372/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3861 - accuracy: 0.8225 - val_loss: 0.4609 - val_accuracy: 0.7953\n",
      "Epoch 373/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3854 - accuracy: 0.8261 - val_loss: 0.4623 - val_accuracy: 0.7904\n",
      "Epoch 374/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3840 - accuracy: 0.8252 - val_loss: 0.4586 - val_accuracy: 0.7937\n",
      "Epoch 375/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3849 - accuracy: 0.8253 - val_loss: 0.4527 - val_accuracy: 0.7944\n",
      "Epoch 376/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3833 - accuracy: 0.8236 - val_loss: 0.4737 - val_accuracy: 0.7831\n",
      "Epoch 377/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3844 - accuracy: 0.8252 - val_loss: 0.4640 - val_accuracy: 0.7900\n",
      "Epoch 378/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3825 - accuracy: 0.8253 - val_loss: 0.4578 - val_accuracy: 0.7953\n",
      "Epoch 379/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3834 - accuracy: 0.8249 - val_loss: 0.4603 - val_accuracy: 0.7952\n",
      "Epoch 380/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3843 - accuracy: 0.8245 - val_loss: 0.4626 - val_accuracy: 0.7857\n",
      "Epoch 381/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3844 - accuracy: 0.8245 - val_loss: 0.4543 - val_accuracy: 0.7955\n",
      "Epoch 382/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3830 - accuracy: 0.8248 - val_loss: 0.4575 - val_accuracy: 0.7935\n",
      "Epoch 383/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3834 - accuracy: 0.8263 - val_loss: 0.4631 - val_accuracy: 0.7899\n",
      "Epoch 384/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3826 - accuracy: 0.8270 - val_loss: 0.4615 - val_accuracy: 0.7907\n",
      "Epoch 385/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3840 - accuracy: 0.8249 - val_loss: 0.4576 - val_accuracy: 0.7944\n",
      "Epoch 386/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3834 - accuracy: 0.8264 - val_loss: 0.4583 - val_accuracy: 0.7931\n",
      "Epoch 387/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3839 - accuracy: 0.8253 - val_loss: 0.4625 - val_accuracy: 0.7902\n",
      "Epoch 388/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3829 - accuracy: 0.8253 - val_loss: 0.4589 - val_accuracy: 0.7929\n",
      "Epoch 389/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3825 - accuracy: 0.8270 - val_loss: 0.4582 - val_accuracy: 0.7953\n",
      "Epoch 390/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3826 - accuracy: 0.8267 - val_loss: 0.4643 - val_accuracy: 0.7885\n",
      "Epoch 391/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3820 - accuracy: 0.8255 - val_loss: 0.4635 - val_accuracy: 0.7982\n",
      "Epoch 392/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3822 - accuracy: 0.8261 - val_loss: 0.4716 - val_accuracy: 0.7891\n",
      "Epoch 393/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3826 - accuracy: 0.8260 - val_loss: 0.4633 - val_accuracy: 0.7946\n",
      "Epoch 394/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3810 - accuracy: 0.8264 - val_loss: 0.4662 - val_accuracy: 0.7874\n",
      "Epoch 395/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3815 - accuracy: 0.8262 - val_loss: 0.4600 - val_accuracy: 0.7918\n",
      "Epoch 396/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3813 - accuracy: 0.8250 - val_loss: 0.4625 - val_accuracy: 0.7912\n",
      "Epoch 397/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3812 - accuracy: 0.8278 - val_loss: 0.4578 - val_accuracy: 0.7927\n",
      "Epoch 398/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3816 - accuracy: 0.8254 - val_loss: 0.4643 - val_accuracy: 0.7904\n",
      "Epoch 399/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3805 - accuracy: 0.8269 - val_loss: 0.4642 - val_accuracy: 0.7940\n",
      "Epoch 400/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3813 - accuracy: 0.8258 - val_loss: 0.4646 - val_accuracy: 0.7939\n",
      "Epoch 401/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3807 - accuracy: 0.8268 - val_loss: 0.4578 - val_accuracy: 0.7943\n",
      "Epoch 402/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3803 - accuracy: 0.8267 - val_loss: 0.4655 - val_accuracy: 0.7903\n",
      "Epoch 403/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3797 - accuracy: 0.8273 - val_loss: 0.4618 - val_accuracy: 0.7876\n",
      "Epoch 404/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3795 - accuracy: 0.8281 - val_loss: 0.4635 - val_accuracy: 0.7913\n",
      "Epoch 405/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3783 - accuracy: 0.8274 - val_loss: 0.4619 - val_accuracy: 0.7920\n",
      "Epoch 406/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3793 - accuracy: 0.8284 - val_loss: 0.4734 - val_accuracy: 0.7824\n",
      "Epoch 407/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3800 - accuracy: 0.8271 - val_loss: 0.4608 - val_accuracy: 0.7964\n",
      "Epoch 408/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3801 - accuracy: 0.8261 - val_loss: 0.4599 - val_accuracy: 0.7964\n",
      "Epoch 409/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3783 - accuracy: 0.8288 - val_loss: 0.4677 - val_accuracy: 0.7931\n",
      "Epoch 410/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3798 - accuracy: 0.8270 - val_loss: 0.4666 - val_accuracy: 0.7883\n",
      "Epoch 411/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3790 - accuracy: 0.8269 - val_loss: 0.4610 - val_accuracy: 0.7960\n",
      "Epoch 412/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3792 - accuracy: 0.8288 - val_loss: 0.4714 - val_accuracy: 0.7912\n",
      "Epoch 413/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3800 - accuracy: 0.8281 - val_loss: 0.4632 - val_accuracy: 0.7886\n",
      "Epoch 414/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3779 - accuracy: 0.8279 - val_loss: 0.4586 - val_accuracy: 0.7946\n",
      "Epoch 415/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3770 - accuracy: 0.8297 - val_loss: 0.4698 - val_accuracy: 0.7857\n",
      "Epoch 416/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3781 - accuracy: 0.8300 - val_loss: 0.4794 - val_accuracy: 0.7829\n",
      "Epoch 417/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3795 - accuracy: 0.8292 - val_loss: 0.4699 - val_accuracy: 0.7908\n",
      "Epoch 418/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3770 - accuracy: 0.8294 - val_loss: 0.4614 - val_accuracy: 0.7940\n",
      "Epoch 419/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3773 - accuracy: 0.8288 - val_loss: 0.4695 - val_accuracy: 0.7887\n",
      "Epoch 420/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3773 - accuracy: 0.8290 - val_loss: 0.4716 - val_accuracy: 0.7863\n",
      "Epoch 421/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3770 - accuracy: 0.8304 - val_loss: 0.4619 - val_accuracy: 0.7938\n",
      "Epoch 422/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3779 - accuracy: 0.8286 - val_loss: 0.4656 - val_accuracy: 0.7883\n",
      "Epoch 423/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3762 - accuracy: 0.8287 - val_loss: 0.4695 - val_accuracy: 0.7877\n",
      "Epoch 424/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3767 - accuracy: 0.8294 - val_loss: 0.4628 - val_accuracy: 0.7909\n",
      "Epoch 425/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3766 - accuracy: 0.8303 - val_loss: 0.4686 - val_accuracy: 0.7916\n",
      "Epoch 426/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3766 - accuracy: 0.8298 - val_loss: 0.4679 - val_accuracy: 0.7879\n",
      "Epoch 427/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3766 - accuracy: 0.8301 - val_loss: 0.4770 - val_accuracy: 0.7859\n",
      "Epoch 428/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3764 - accuracy: 0.8299 - val_loss: 0.4666 - val_accuracy: 0.7886\n",
      "Epoch 429/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3752 - accuracy: 0.8292 - val_loss: 0.4683 - val_accuracy: 0.7944\n",
      "Epoch 430/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3742 - accuracy: 0.8307 - val_loss: 0.4641 - val_accuracy: 0.7918\n",
      "Epoch 431/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3758 - accuracy: 0.8295 - val_loss: 0.4682 - val_accuracy: 0.7934\n",
      "Epoch 432/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3761 - accuracy: 0.8302 - val_loss: 0.4719 - val_accuracy: 0.7902\n",
      "Epoch 433/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3749 - accuracy: 0.8302 - val_loss: 0.4729 - val_accuracy: 0.7882\n",
      "Epoch 434/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3767 - accuracy: 0.8284 - val_loss: 0.4708 - val_accuracy: 0.7879\n",
      "Epoch 435/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3751 - accuracy: 0.8315 - val_loss: 0.4693 - val_accuracy: 0.7929\n",
      "Epoch 436/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3752 - accuracy: 0.8306 - val_loss: 0.4635 - val_accuracy: 0.7908\n",
      "Epoch 437/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3743 - accuracy: 0.8317 - val_loss: 0.4708 - val_accuracy: 0.7865\n",
      "Epoch 438/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3740 - accuracy: 0.8309 - val_loss: 0.4703 - val_accuracy: 0.7856\n",
      "Epoch 439/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3744 - accuracy: 0.8316 - val_loss: 0.4753 - val_accuracy: 0.7853\n",
      "Epoch 440/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3744 - accuracy: 0.8309 - val_loss: 0.4767 - val_accuracy: 0.7909\n",
      "Epoch 441/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3741 - accuracy: 0.8306 - val_loss: 0.4706 - val_accuracy: 0.7907\n",
      "Epoch 442/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3748 - accuracy: 0.8307 - val_loss: 0.4697 - val_accuracy: 0.7917\n",
      "Epoch 443/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3726 - accuracy: 0.8309 - val_loss: 0.4738 - val_accuracy: 0.7909\n",
      "Epoch 444/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3744 - accuracy: 0.8302 - val_loss: 0.4716 - val_accuracy: 0.7924\n",
      "Epoch 445/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3743 - accuracy: 0.8314 - val_loss: 0.4700 - val_accuracy: 0.7886\n",
      "Epoch 446/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3720 - accuracy: 0.8325 - val_loss: 0.4687 - val_accuracy: 0.7904\n",
      "Epoch 447/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3736 - accuracy: 0.8323 - val_loss: 0.4713 - val_accuracy: 0.7850\n",
      "Epoch 448/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3721 - accuracy: 0.8313 - val_loss: 0.4691 - val_accuracy: 0.7929\n",
      "Epoch 449/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3728 - accuracy: 0.8319 - val_loss: 0.4828 - val_accuracy: 0.7907\n",
      "Epoch 450/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3739 - accuracy: 0.8323 - val_loss: 0.4753 - val_accuracy: 0.7861\n",
      "Epoch 451/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3735 - accuracy: 0.8315 - val_loss: 0.4665 - val_accuracy: 0.7904\n",
      "Epoch 452/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3729 - accuracy: 0.8309 - val_loss: 0.4694 - val_accuracy: 0.7930\n",
      "Epoch 453/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3723 - accuracy: 0.8329 - val_loss: 0.4720 - val_accuracy: 0.7907\n",
      "Epoch 454/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3731 - accuracy: 0.8325 - val_loss: 0.4762 - val_accuracy: 0.7881\n",
      "Epoch 455/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3725 - accuracy: 0.8306 - val_loss: 0.4746 - val_accuracy: 0.7927\n",
      "Epoch 456/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3716 - accuracy: 0.8320 - val_loss: 0.4692 - val_accuracy: 0.7903\n",
      "Epoch 457/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3723 - accuracy: 0.8309 - val_loss: 0.4861 - val_accuracy: 0.7882\n",
      "Epoch 458/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3719 - accuracy: 0.8329 - val_loss: 0.4707 - val_accuracy: 0.7925\n",
      "Epoch 459/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3716 - accuracy: 0.8317 - val_loss: 0.4728 - val_accuracy: 0.7878\n",
      "Epoch 460/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3713 - accuracy: 0.8350 - val_loss: 0.4713 - val_accuracy: 0.7894\n",
      "Epoch 461/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3722 - accuracy: 0.8305 - val_loss: 0.4682 - val_accuracy: 0.7934\n",
      "Epoch 462/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3718 - accuracy: 0.8324 - val_loss: 0.4728 - val_accuracy: 0.7868\n",
      "Epoch 463/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3724 - accuracy: 0.8313 - val_loss: 0.4665 - val_accuracy: 0.7925\n",
      "Epoch 464/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3706 - accuracy: 0.8317 - val_loss: 0.4778 - val_accuracy: 0.7927\n",
      "Epoch 465/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3703 - accuracy: 0.8328 - val_loss: 0.4695 - val_accuracy: 0.7877\n",
      "Epoch 466/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3705 - accuracy: 0.8329 - val_loss: 0.4730 - val_accuracy: 0.7883\n",
      "Epoch 467/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3704 - accuracy: 0.8330 - val_loss: 0.4819 - val_accuracy: 0.7790\n",
      "Epoch 468/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3701 - accuracy: 0.8325 - val_loss: 0.4709 - val_accuracy: 0.7900\n",
      "Epoch 469/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3701 - accuracy: 0.8331 - val_loss: 0.4718 - val_accuracy: 0.7886\n",
      "Epoch 470/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3706 - accuracy: 0.8338 - val_loss: 0.4777 - val_accuracy: 0.7895\n",
      "Epoch 471/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3712 - accuracy: 0.8328 - val_loss: 0.4735 - val_accuracy: 0.7889\n",
      "Epoch 472/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3698 - accuracy: 0.8333 - val_loss: 0.4712 - val_accuracy: 0.7922\n",
      "Epoch 473/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3702 - accuracy: 0.8335 - val_loss: 0.4761 - val_accuracy: 0.7883\n",
      "Epoch 474/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3711 - accuracy: 0.8336 - val_loss: 0.4723 - val_accuracy: 0.7933\n",
      "Epoch 475/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3701 - accuracy: 0.8337 - val_loss: 0.4836 - val_accuracy: 0.7853\n",
      "Epoch 476/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3714 - accuracy: 0.8328 - val_loss: 0.4796 - val_accuracy: 0.7890\n",
      "Epoch 477/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3704 - accuracy: 0.8342 - val_loss: 0.4649 - val_accuracy: 0.7915\n",
      "Epoch 478/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3709 - accuracy: 0.8331 - val_loss: 0.4695 - val_accuracy: 0.7921\n",
      "Epoch 479/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3706 - accuracy: 0.8332 - val_loss: 0.4748 - val_accuracy: 0.7848\n",
      "Epoch 480/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3687 - accuracy: 0.8344 - val_loss: 0.4756 - val_accuracy: 0.7900\n",
      "Epoch 481/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3702 - accuracy: 0.8343 - val_loss: 0.4670 - val_accuracy: 0.7935\n",
      "Epoch 482/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3702 - accuracy: 0.8329 - val_loss: 0.4690 - val_accuracy: 0.7921\n",
      "Epoch 483/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3702 - accuracy: 0.8324 - val_loss: 0.4734 - val_accuracy: 0.7876\n",
      "Epoch 484/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3699 - accuracy: 0.8323 - val_loss: 0.4767 - val_accuracy: 0.7900\n",
      "Epoch 485/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3692 - accuracy: 0.8333 - val_loss: 0.4786 - val_accuracy: 0.7952\n",
      "Epoch 486/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3701 - accuracy: 0.8324 - val_loss: 0.4767 - val_accuracy: 0.7889\n",
      "Epoch 487/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3704 - accuracy: 0.8344 - val_loss: 0.4756 - val_accuracy: 0.7905\n",
      "Epoch 488/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3695 - accuracy: 0.8338 - val_loss: 0.4845 - val_accuracy: 0.7831\n",
      "Epoch 489/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3691 - accuracy: 0.8323 - val_loss: 0.4859 - val_accuracy: 0.7866\n",
      "Epoch 490/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3697 - accuracy: 0.8341 - val_loss: 0.4730 - val_accuracy: 0.7896\n",
      "Epoch 491/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3687 - accuracy: 0.8324 - val_loss: 0.4781 - val_accuracy: 0.7859\n",
      "Epoch 492/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3700 - accuracy: 0.8344 - val_loss: 0.4741 - val_accuracy: 0.7925\n",
      "Epoch 493/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3692 - accuracy: 0.8344 - val_loss: 0.4757 - val_accuracy: 0.7922\n",
      "Epoch 494/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3686 - accuracy: 0.8332 - val_loss: 0.4788 - val_accuracy: 0.7887\n",
      "Epoch 495/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3697 - accuracy: 0.8342 - val_loss: 0.4779 - val_accuracy: 0.7905\n",
      "Epoch 496/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3682 - accuracy: 0.8329 - val_loss: 0.4794 - val_accuracy: 0.7872\n",
      "Epoch 497/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3691 - accuracy: 0.8346 - val_loss: 0.4702 - val_accuracy: 0.7937\n",
      "Epoch 498/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3676 - accuracy: 0.8355 - val_loss: 0.4793 - val_accuracy: 0.7886\n",
      "Epoch 499/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3677 - accuracy: 0.8356 - val_loss: 0.4796 - val_accuracy: 0.7891\n",
      "Epoch 500/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3685 - accuracy: 0.8336 - val_loss: 0.4762 - val_accuracy: 0.7904\n",
      "241/241 [==============================] - 0s 1ms/step - loss: 0.4762 - accuracy: 0.7904\n",
      "\n",
      "--------------\n",
      " [0.47623974084854126, 0.790410578250885]\n",
      "241/241 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.7904106029106029\n",
      "Recall: 0.8506611355976147\n",
      "Precision: 0.7598425196850394\n",
      "F1: 0.8026911314984709\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      " tanh \n",
      "\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 200)               2400      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 150)               30150     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 50)                7550      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,151\n",
      "Trainable params: 40,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n",
      "962/962 [==============================] - 3s 2ms/step - loss: 0.5222 - accuracy: 0.7377 - val_loss: 0.5212 - val_accuracy: 0.7431\n",
      "Epoch 2/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5027 - accuracy: 0.7506 - val_loss: 0.4946 - val_accuracy: 0.7518\n",
      "Epoch 3/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4937 - accuracy: 0.7578 - val_loss: 0.4928 - val_accuracy: 0.7600\n",
      "Epoch 4/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4866 - accuracy: 0.7616 - val_loss: 0.4905 - val_accuracy: 0.7549\n",
      "Epoch 5/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4806 - accuracy: 0.7649 - val_loss: 0.4863 - val_accuracy: 0.7552\n",
      "Epoch 6/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4752 - accuracy: 0.7679 - val_loss: 0.4849 - val_accuracy: 0.7633\n",
      "Epoch 7/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4703 - accuracy: 0.7730 - val_loss: 0.4997 - val_accuracy: 0.7596\n",
      "Epoch 8/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4658 - accuracy: 0.7761 - val_loss: 0.4743 - val_accuracy: 0.7716\n",
      "Epoch 9/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4622 - accuracy: 0.7740 - val_loss: 0.4852 - val_accuracy: 0.7672\n",
      "Epoch 10/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4594 - accuracy: 0.7790 - val_loss: 0.4756 - val_accuracy: 0.7690\n",
      "Epoch 11/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4564 - accuracy: 0.7811 - val_loss: 0.4750 - val_accuracy: 0.7685\n",
      "Epoch 12/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4539 - accuracy: 0.7818 - val_loss: 0.4673 - val_accuracy: 0.7752\n",
      "Epoch 13/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4504 - accuracy: 0.7847 - val_loss: 0.4686 - val_accuracy: 0.7768\n",
      "Epoch 14/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4486 - accuracy: 0.7858 - val_loss: 0.4775 - val_accuracy: 0.7746\n",
      "Epoch 15/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4462 - accuracy: 0.7866 - val_loss: 0.4680 - val_accuracy: 0.7730\n",
      "Epoch 16/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4431 - accuracy: 0.7909 - val_loss: 0.4592 - val_accuracy: 0.7804\n",
      "Epoch 17/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4412 - accuracy: 0.7900 - val_loss: 0.4610 - val_accuracy: 0.7822\n",
      "Epoch 18/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4397 - accuracy: 0.7913 - val_loss: 0.4615 - val_accuracy: 0.7794\n",
      "Epoch 19/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4365 - accuracy: 0.7922 - val_loss: 0.4601 - val_accuracy: 0.7781\n",
      "Epoch 20/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4342 - accuracy: 0.7954 - val_loss: 0.4599 - val_accuracy: 0.7811\n",
      "Epoch 21/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4334 - accuracy: 0.7952 - val_loss: 0.4628 - val_accuracy: 0.7766\n",
      "Epoch 22/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4302 - accuracy: 0.7968 - val_loss: 0.4544 - val_accuracy: 0.7821\n",
      "Epoch 23/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4290 - accuracy: 0.7980 - val_loss: 0.4598 - val_accuracy: 0.7794\n",
      "Epoch 24/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4275 - accuracy: 0.7979 - val_loss: 0.4591 - val_accuracy: 0.7811\n",
      "Epoch 25/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4253 - accuracy: 0.8005 - val_loss: 0.4608 - val_accuracy: 0.7827\n",
      "Epoch 26/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4229 - accuracy: 0.8033 - val_loss: 0.4634 - val_accuracy: 0.7817\n",
      "Epoch 27/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4211 - accuracy: 0.8036 - val_loss: 0.4717 - val_accuracy: 0.7678\n",
      "Epoch 28/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4202 - accuracy: 0.8047 - val_loss: 0.4547 - val_accuracy: 0.7853\n",
      "Epoch 29/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4185 - accuracy: 0.8043 - val_loss: 0.4550 - val_accuracy: 0.7811\n",
      "Epoch 30/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4168 - accuracy: 0.8047 - val_loss: 0.4559 - val_accuracy: 0.7833\n",
      "Epoch 31/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4158 - accuracy: 0.8060 - val_loss: 0.4560 - val_accuracy: 0.7869\n",
      "Epoch 32/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4126 - accuracy: 0.8064 - val_loss: 0.4545 - val_accuracy: 0.7851\n",
      "Epoch 33/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4119 - accuracy: 0.8065 - val_loss: 0.4671 - val_accuracy: 0.7818\n",
      "Epoch 34/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4104 - accuracy: 0.8071 - val_loss: 0.4548 - val_accuracy: 0.7856\n",
      "Epoch 35/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4084 - accuracy: 0.8127 - val_loss: 0.4585 - val_accuracy: 0.7865\n",
      "Epoch 36/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4071 - accuracy: 0.8111 - val_loss: 0.4619 - val_accuracy: 0.7792\n",
      "Epoch 37/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.4052 - accuracy: 0.8110 - val_loss: 0.4597 - val_accuracy: 0.7837\n",
      "Epoch 38/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4045 - accuracy: 0.8140 - val_loss: 0.4576 - val_accuracy: 0.7859\n",
      "Epoch 39/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4021 - accuracy: 0.8142 - val_loss: 0.4588 - val_accuracy: 0.7853\n",
      "Epoch 40/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4016 - accuracy: 0.8133 - val_loss: 0.4579 - val_accuracy: 0.7869\n",
      "Epoch 41/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3999 - accuracy: 0.8141 - val_loss: 0.4619 - val_accuracy: 0.7873\n",
      "Epoch 42/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3987 - accuracy: 0.8150 - val_loss: 0.4681 - val_accuracy: 0.7788\n",
      "Epoch 43/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3964 - accuracy: 0.8167 - val_loss: 0.4676 - val_accuracy: 0.7756\n",
      "Epoch 44/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3952 - accuracy: 0.8172 - val_loss: 0.4655 - val_accuracy: 0.7855\n",
      "Epoch 45/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3933 - accuracy: 0.8201 - val_loss: 0.4625 - val_accuracy: 0.7822\n",
      "Epoch 46/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3921 - accuracy: 0.8182 - val_loss: 0.4695 - val_accuracy: 0.7827\n",
      "Epoch 47/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3908 - accuracy: 0.8188 - val_loss: 0.4656 - val_accuracy: 0.7838\n",
      "Epoch 48/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3885 - accuracy: 0.8234 - val_loss: 0.4617 - val_accuracy: 0.7844\n",
      "Epoch 49/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3880 - accuracy: 0.8228 - val_loss: 0.4620 - val_accuracy: 0.7878\n",
      "Epoch 50/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3846 - accuracy: 0.8235 - val_loss: 0.4703 - val_accuracy: 0.7757\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3840 - accuracy: 0.8221 - val_loss: 0.4638 - val_accuracy: 0.7847\n",
      "Epoch 52/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3836 - accuracy: 0.8232 - val_loss: 0.4693 - val_accuracy: 0.7846\n",
      "Epoch 53/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3805 - accuracy: 0.8254 - val_loss: 0.4664 - val_accuracy: 0.7838\n",
      "Epoch 54/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3794 - accuracy: 0.8277 - val_loss: 0.4843 - val_accuracy: 0.7843\n",
      "Epoch 55/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3778 - accuracy: 0.8275 - val_loss: 0.4760 - val_accuracy: 0.7816\n",
      "Epoch 56/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3763 - accuracy: 0.8276 - val_loss: 0.4716 - val_accuracy: 0.7839\n",
      "Epoch 57/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3756 - accuracy: 0.8272 - val_loss: 0.4675 - val_accuracy: 0.7859\n",
      "Epoch 58/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3726 - accuracy: 0.8291 - val_loss: 0.4684 - val_accuracy: 0.7831\n",
      "Epoch 59/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3724 - accuracy: 0.8287 - val_loss: 0.4692 - val_accuracy: 0.7853\n",
      "Epoch 60/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3712 - accuracy: 0.8307 - val_loss: 0.4773 - val_accuracy: 0.7729\n",
      "Epoch 61/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3680 - accuracy: 0.8312 - val_loss: 0.4713 - val_accuracy: 0.7825\n",
      "Epoch 62/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3668 - accuracy: 0.8308 - val_loss: 0.4723 - val_accuracy: 0.7887\n",
      "Epoch 63/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3650 - accuracy: 0.8318 - val_loss: 0.4805 - val_accuracy: 0.7813\n",
      "Epoch 64/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3637 - accuracy: 0.8350 - val_loss: 0.4786 - val_accuracy: 0.7904\n",
      "Epoch 65/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3631 - accuracy: 0.8344 - val_loss: 0.4756 - val_accuracy: 0.7813\n",
      "Epoch 66/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3610 - accuracy: 0.8342 - val_loss: 0.4813 - val_accuracy: 0.7863\n",
      "Epoch 67/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3592 - accuracy: 0.8365 - val_loss: 0.4770 - val_accuracy: 0.7830\n",
      "Epoch 68/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3574 - accuracy: 0.8354 - val_loss: 0.4818 - val_accuracy: 0.7869\n",
      "Epoch 69/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3568 - accuracy: 0.8386 - val_loss: 0.4835 - val_accuracy: 0.7795\n",
      "Epoch 70/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3552 - accuracy: 0.8379 - val_loss: 0.4901 - val_accuracy: 0.7791\n",
      "Epoch 71/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3520 - accuracy: 0.8392 - val_loss: 0.4827 - val_accuracy: 0.7852\n",
      "Epoch 72/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3512 - accuracy: 0.8398 - val_loss: 0.4862 - val_accuracy: 0.7760\n",
      "Epoch 73/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3489 - accuracy: 0.8412 - val_loss: 0.4836 - val_accuracy: 0.7803\n",
      "Epoch 74/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3487 - accuracy: 0.8407 - val_loss: 0.4958 - val_accuracy: 0.7916\n",
      "Epoch 75/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3461 - accuracy: 0.8416 - val_loss: 0.4908 - val_accuracy: 0.7791\n",
      "Epoch 76/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3461 - accuracy: 0.8444 - val_loss: 0.4954 - val_accuracy: 0.7821\n",
      "Epoch 77/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3446 - accuracy: 0.8458 - val_loss: 0.4900 - val_accuracy: 0.7835\n",
      "Epoch 78/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3416 - accuracy: 0.8463 - val_loss: 0.4870 - val_accuracy: 0.7851\n",
      "Epoch 79/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3414 - accuracy: 0.8468 - val_loss: 0.4875 - val_accuracy: 0.7814\n",
      "Epoch 80/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3378 - accuracy: 0.8486 - val_loss: 0.5003 - val_accuracy: 0.7721\n",
      "Epoch 81/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3384 - accuracy: 0.8476 - val_loss: 0.4992 - val_accuracy: 0.7790\n",
      "Epoch 82/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3354 - accuracy: 0.8471 - val_loss: 0.4972 - val_accuracy: 0.7844\n",
      "Epoch 83/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3341 - accuracy: 0.8504 - val_loss: 0.5000 - val_accuracy: 0.7814\n",
      "Epoch 84/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3333 - accuracy: 0.8499 - val_loss: 0.4964 - val_accuracy: 0.7817\n",
      "Epoch 85/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3320 - accuracy: 0.8515 - val_loss: 0.5090 - val_accuracy: 0.7775\n",
      "Epoch 86/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3302 - accuracy: 0.8513 - val_loss: 0.4992 - val_accuracy: 0.7835\n",
      "Epoch 87/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3289 - accuracy: 0.8533 - val_loss: 0.5073 - val_accuracy: 0.7761\n",
      "Epoch 88/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3276 - accuracy: 0.8522 - val_loss: 0.5080 - val_accuracy: 0.7759\n",
      "Epoch 89/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3249 - accuracy: 0.8561 - val_loss: 0.5063 - val_accuracy: 0.7813\n",
      "Epoch 90/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3234 - accuracy: 0.8574 - val_loss: 0.5021 - val_accuracy: 0.7837\n",
      "Epoch 91/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3236 - accuracy: 0.8558 - val_loss: 0.5077 - val_accuracy: 0.7812\n",
      "Epoch 92/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3214 - accuracy: 0.8566 - val_loss: 0.5083 - val_accuracy: 0.7861\n",
      "Epoch 93/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3205 - accuracy: 0.8564 - val_loss: 0.5064 - val_accuracy: 0.7842\n",
      "Epoch 94/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3185 - accuracy: 0.8579 - val_loss: 0.5140 - val_accuracy: 0.7796\n",
      "Epoch 95/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3171 - accuracy: 0.8574 - val_loss: 0.5189 - val_accuracy: 0.7803\n",
      "Epoch 96/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3159 - accuracy: 0.8595 - val_loss: 0.5161 - val_accuracy: 0.7800\n",
      "Epoch 97/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3146 - accuracy: 0.8597 - val_loss: 0.5175 - val_accuracy: 0.7794\n",
      "Epoch 98/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3116 - accuracy: 0.8606 - val_loss: 0.5189 - val_accuracy: 0.7782\n",
      "Epoch 99/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3125 - accuracy: 0.8593 - val_loss: 0.5227 - val_accuracy: 0.7834\n",
      "Epoch 100/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3102 - accuracy: 0.8628 - val_loss: 0.5326 - val_accuracy: 0.7783\n",
      "Epoch 101/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3107 - accuracy: 0.8614 - val_loss: 0.5299 - val_accuracy: 0.7779\n",
      "Epoch 102/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3074 - accuracy: 0.8619 - val_loss: 0.5336 - val_accuracy: 0.7720\n",
      "Epoch 103/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3075 - accuracy: 0.8603 - val_loss: 0.5249 - val_accuracy: 0.7790\n",
      "Epoch 104/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3067 - accuracy: 0.8619 - val_loss: 0.5318 - val_accuracy: 0.7752\n",
      "Epoch 105/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3040 - accuracy: 0.8641 - val_loss: 0.5356 - val_accuracy: 0.7735\n",
      "Epoch 106/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3039 - accuracy: 0.8656 - val_loss: 0.5308 - val_accuracy: 0.7826\n",
      "Epoch 107/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3021 - accuracy: 0.8663 - val_loss: 0.5368 - val_accuracy: 0.7727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3012 - accuracy: 0.8661 - val_loss: 0.5305 - val_accuracy: 0.7795\n",
      "Epoch 109/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2995 - accuracy: 0.8674 - val_loss: 0.5386 - val_accuracy: 0.7765\n",
      "Epoch 110/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2979 - accuracy: 0.8690 - val_loss: 0.5359 - val_accuracy: 0.7775\n",
      "Epoch 111/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2967 - accuracy: 0.8692 - val_loss: 0.5442 - val_accuracy: 0.7769\n",
      "Epoch 112/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2958 - accuracy: 0.8689 - val_loss: 0.5391 - val_accuracy: 0.7791\n",
      "Epoch 113/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2954 - accuracy: 0.8689 - val_loss: 0.5459 - val_accuracy: 0.7708\n",
      "Epoch 114/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2935 - accuracy: 0.8691 - val_loss: 0.5467 - val_accuracy: 0.7727\n",
      "Epoch 115/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2929 - accuracy: 0.8700 - val_loss: 0.5448 - val_accuracy: 0.7769\n",
      "Epoch 116/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2909 - accuracy: 0.8717 - val_loss: 0.5497 - val_accuracy: 0.7748\n",
      "Epoch 117/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2899 - accuracy: 0.8726 - val_loss: 0.5514 - val_accuracy: 0.7768\n",
      "Epoch 118/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2887 - accuracy: 0.8727 - val_loss: 0.5508 - val_accuracy: 0.7811\n",
      "Epoch 119/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2890 - accuracy: 0.8729 - val_loss: 0.5536 - val_accuracy: 0.7743\n",
      "Epoch 120/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2868 - accuracy: 0.8740 - val_loss: 0.5507 - val_accuracy: 0.7731\n",
      "Epoch 121/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2869 - accuracy: 0.8717 - val_loss: 0.5508 - val_accuracy: 0.7714\n",
      "Epoch 122/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2848 - accuracy: 0.8761 - val_loss: 0.5519 - val_accuracy: 0.7749\n",
      "Epoch 123/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2843 - accuracy: 0.8756 - val_loss: 0.5525 - val_accuracy: 0.7764\n",
      "Epoch 124/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2805 - accuracy: 0.8758 - val_loss: 0.5543 - val_accuracy: 0.7792\n",
      "Epoch 125/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2810 - accuracy: 0.8768 - val_loss: 0.5649 - val_accuracy: 0.7727\n",
      "Epoch 126/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2809 - accuracy: 0.8778 - val_loss: 0.5564 - val_accuracy: 0.7775\n",
      "Epoch 127/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2794 - accuracy: 0.8772 - val_loss: 0.5680 - val_accuracy: 0.7752\n",
      "Epoch 128/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2804 - accuracy: 0.8770 - val_loss: 0.5735 - val_accuracy: 0.7617\n",
      "Epoch 129/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2772 - accuracy: 0.8793 - val_loss: 0.5747 - val_accuracy: 0.7730\n",
      "Epoch 130/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2759 - accuracy: 0.8784 - val_loss: 0.5637 - val_accuracy: 0.7749\n",
      "Epoch 131/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2750 - accuracy: 0.8791 - val_loss: 0.5795 - val_accuracy: 0.7773\n",
      "Epoch 132/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2728 - accuracy: 0.8809 - val_loss: 0.5707 - val_accuracy: 0.7709\n",
      "Epoch 133/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2742 - accuracy: 0.8789 - val_loss: 0.5711 - val_accuracy: 0.7748\n",
      "Epoch 134/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2712 - accuracy: 0.8822 - val_loss: 0.5825 - val_accuracy: 0.7777\n",
      "Epoch 135/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2718 - accuracy: 0.8811 - val_loss: 0.5794 - val_accuracy: 0.7699\n",
      "Epoch 136/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2711 - accuracy: 0.8819 - val_loss: 0.5679 - val_accuracy: 0.7761\n",
      "Epoch 137/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2685 - accuracy: 0.8824 - val_loss: 0.5903 - val_accuracy: 0.7729\n",
      "Epoch 138/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2673 - accuracy: 0.8834 - val_loss: 0.5808 - val_accuracy: 0.7782\n",
      "Epoch 139/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2661 - accuracy: 0.8835 - val_loss: 0.5834 - val_accuracy: 0.7672\n",
      "Epoch 140/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2664 - accuracy: 0.8829 - val_loss: 0.5805 - val_accuracy: 0.7762\n",
      "Epoch 141/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2662 - accuracy: 0.8860 - val_loss: 0.5872 - val_accuracy: 0.7786\n",
      "Epoch 142/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2651 - accuracy: 0.8852 - val_loss: 0.5866 - val_accuracy: 0.7712\n",
      "Epoch 143/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2627 - accuracy: 0.8867 - val_loss: 0.5883 - val_accuracy: 0.7747\n",
      "Epoch 144/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2634 - accuracy: 0.8856 - val_loss: 0.5972 - val_accuracy: 0.7738\n",
      "Epoch 145/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2615 - accuracy: 0.8865 - val_loss: 0.5973 - val_accuracy: 0.7736\n",
      "Epoch 146/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2611 - accuracy: 0.8870 - val_loss: 0.6062 - val_accuracy: 0.7733\n",
      "Epoch 147/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2597 - accuracy: 0.8885 - val_loss: 0.6001 - val_accuracy: 0.7716\n",
      "Epoch 148/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2581 - accuracy: 0.8883 - val_loss: 0.5999 - val_accuracy: 0.7661\n",
      "Epoch 149/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2572 - accuracy: 0.8892 - val_loss: 0.6090 - val_accuracy: 0.7705\n",
      "Epoch 150/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2583 - accuracy: 0.8878 - val_loss: 0.6126 - val_accuracy: 0.7768\n",
      "Epoch 151/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2552 - accuracy: 0.8899 - val_loss: 0.6085 - val_accuracy: 0.7709\n",
      "Epoch 152/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2542 - accuracy: 0.8899 - val_loss: 0.6089 - val_accuracy: 0.7713\n",
      "Epoch 153/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2562 - accuracy: 0.8892 - val_loss: 0.6015 - val_accuracy: 0.7768\n",
      "Epoch 154/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2540 - accuracy: 0.8905 - val_loss: 0.6077 - val_accuracy: 0.7690\n",
      "Epoch 155/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2543 - accuracy: 0.8904 - val_loss: 0.6161 - val_accuracy: 0.7627\n",
      "Epoch 156/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2525 - accuracy: 0.8895 - val_loss: 0.6100 - val_accuracy: 0.7720\n",
      "Epoch 157/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2506 - accuracy: 0.8942 - val_loss: 0.6129 - val_accuracy: 0.7699\n",
      "Epoch 158/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2515 - accuracy: 0.8914 - val_loss: 0.6157 - val_accuracy: 0.7705\n",
      "Epoch 159/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2482 - accuracy: 0.8920 - val_loss: 0.6188 - val_accuracy: 0.7688\n",
      "Epoch 160/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2484 - accuracy: 0.8934 - val_loss: 0.6262 - val_accuracy: 0.7633\n",
      "Epoch 161/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2483 - accuracy: 0.8938 - val_loss: 0.6329 - val_accuracy: 0.7688\n",
      "Epoch 162/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2472 - accuracy: 0.8951 - val_loss: 0.6192 - val_accuracy: 0.7725\n",
      "Epoch 163/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2433 - accuracy: 0.8976 - val_loss: 0.6215 - val_accuracy: 0.7736\n",
      "Epoch 164/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2453 - accuracy: 0.8945 - val_loss: 0.6263 - val_accuracy: 0.7669\n",
      "Epoch 165/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2432 - accuracy: 0.8955 - val_loss: 0.6309 - val_accuracy: 0.7716\n",
      "Epoch 166/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2429 - accuracy: 0.8964 - val_loss: 0.6300 - val_accuracy: 0.7721\n",
      "Epoch 167/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2436 - accuracy: 0.8956 - val_loss: 0.6275 - val_accuracy: 0.7688\n",
      "Epoch 168/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2426 - accuracy: 0.8955 - val_loss: 0.6234 - val_accuracy: 0.7730\n",
      "Epoch 169/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2410 - accuracy: 0.8962 - val_loss: 0.6245 - val_accuracy: 0.7700\n",
      "Epoch 170/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2395 - accuracy: 0.8982 - val_loss: 0.6455 - val_accuracy: 0.7683\n",
      "Epoch 171/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2408 - accuracy: 0.8974 - val_loss: 0.6373 - val_accuracy: 0.7664\n",
      "Epoch 172/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2381 - accuracy: 0.8980 - val_loss: 0.6386 - val_accuracy: 0.7686\n",
      "Epoch 173/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2385 - accuracy: 0.8979 - val_loss: 0.6467 - val_accuracy: 0.7662\n",
      "Epoch 174/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2373 - accuracy: 0.8997 - val_loss: 0.6349 - val_accuracy: 0.7710\n",
      "Epoch 175/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2365 - accuracy: 0.8981 - val_loss: 0.6397 - val_accuracy: 0.7661\n",
      "Epoch 176/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2364 - accuracy: 0.8985 - val_loss: 0.6505 - val_accuracy: 0.7699\n",
      "Epoch 177/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2361 - accuracy: 0.8987 - val_loss: 0.6485 - val_accuracy: 0.7704\n",
      "Epoch 178/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2323 - accuracy: 0.9006 - val_loss: 0.6509 - val_accuracy: 0.7630\n",
      "Epoch 179/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2320 - accuracy: 0.9004 - val_loss: 0.6529 - val_accuracy: 0.7704\n",
      "Epoch 180/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2322 - accuracy: 0.9008 - val_loss: 0.6607 - val_accuracy: 0.7687\n",
      "Epoch 181/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2323 - accuracy: 0.9019 - val_loss: 0.6672 - val_accuracy: 0.7626\n",
      "Epoch 182/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2291 - accuracy: 0.9023 - val_loss: 0.6528 - val_accuracy: 0.7631\n",
      "Epoch 183/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2311 - accuracy: 0.9010 - val_loss: 0.6491 - val_accuracy: 0.7659\n",
      "Epoch 184/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2280 - accuracy: 0.9030 - val_loss: 0.6653 - val_accuracy: 0.7698\n",
      "Epoch 185/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2289 - accuracy: 0.9031 - val_loss: 0.6599 - val_accuracy: 0.7664\n",
      "Epoch 186/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2305 - accuracy: 0.9024 - val_loss: 0.6663 - val_accuracy: 0.7699\n",
      "Epoch 187/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2259 - accuracy: 0.9045 - val_loss: 0.6836 - val_accuracy: 0.7609\n",
      "Epoch 188/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2276 - accuracy: 0.9034 - val_loss: 0.6589 - val_accuracy: 0.7688\n",
      "Epoch 189/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2265 - accuracy: 0.9059 - val_loss: 0.6766 - val_accuracy: 0.7651\n",
      "Epoch 190/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2261 - accuracy: 0.9035 - val_loss: 0.6746 - val_accuracy: 0.7682\n",
      "Epoch 191/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2225 - accuracy: 0.9048 - val_loss: 0.6634 - val_accuracy: 0.7716\n",
      "Epoch 192/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2235 - accuracy: 0.9069 - val_loss: 0.6757 - val_accuracy: 0.7672\n",
      "Epoch 193/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2247 - accuracy: 0.9073 - val_loss: 0.6791 - val_accuracy: 0.7652\n",
      "Epoch 194/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2239 - accuracy: 0.9066 - val_loss: 0.6743 - val_accuracy: 0.7675\n",
      "Epoch 195/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2222 - accuracy: 0.9067 - val_loss: 0.6705 - val_accuracy: 0.7707\n",
      "Epoch 196/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2221 - accuracy: 0.9076 - val_loss: 0.6747 - val_accuracy: 0.7692\n",
      "Epoch 197/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2187 - accuracy: 0.9076 - val_loss: 0.6809 - val_accuracy: 0.7708\n",
      "Epoch 198/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2209 - accuracy: 0.9073 - val_loss: 0.6924 - val_accuracy: 0.7733\n",
      "Epoch 199/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2181 - accuracy: 0.9087 - val_loss: 0.6797 - val_accuracy: 0.7707\n",
      "Epoch 200/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2175 - accuracy: 0.9089 - val_loss: 0.6840 - val_accuracy: 0.7673\n",
      "Epoch 201/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2182 - accuracy: 0.9095 - val_loss: 0.6824 - val_accuracy: 0.7707\n",
      "Epoch 202/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2176 - accuracy: 0.9089 - val_loss: 0.6928 - val_accuracy: 0.7662\n",
      "Epoch 203/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2173 - accuracy: 0.9094 - val_loss: 0.6909 - val_accuracy: 0.7666\n",
      "Epoch 204/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2159 - accuracy: 0.9105 - val_loss: 0.6890 - val_accuracy: 0.7700\n",
      "Epoch 205/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2132 - accuracy: 0.9116 - val_loss: 0.6986 - val_accuracy: 0.7696\n",
      "Epoch 206/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2156 - accuracy: 0.9074 - val_loss: 0.6944 - val_accuracy: 0.7690\n",
      "Epoch 207/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2140 - accuracy: 0.9098 - val_loss: 0.7027 - val_accuracy: 0.7704\n",
      "Epoch 208/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2139 - accuracy: 0.9110 - val_loss: 0.6979 - val_accuracy: 0.7668\n",
      "Epoch 209/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2117 - accuracy: 0.9120 - val_loss: 0.6954 - val_accuracy: 0.7682\n",
      "Epoch 210/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2127 - accuracy: 0.9101 - val_loss: 0.7066 - val_accuracy: 0.7672\n",
      "Epoch 211/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2126 - accuracy: 0.9115 - val_loss: 0.7002 - val_accuracy: 0.7705\n",
      "Epoch 212/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2112 - accuracy: 0.9122 - val_loss: 0.6966 - val_accuracy: 0.7622\n",
      "Epoch 213/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2109 - accuracy: 0.9111 - val_loss: 0.6981 - val_accuracy: 0.7700\n",
      "Epoch 214/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2085 - accuracy: 0.9131 - val_loss: 0.7111 - val_accuracy: 0.7640\n",
      "Epoch 215/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2101 - accuracy: 0.9123 - val_loss: 0.7042 - val_accuracy: 0.7709\n",
      "Epoch 216/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2057 - accuracy: 0.9133 - val_loss: 0.7149 - val_accuracy: 0.7635\n",
      "Epoch 217/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2077 - accuracy: 0.9130 - val_loss: 0.7222 - val_accuracy: 0.7622\n",
      "Epoch 218/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2071 - accuracy: 0.9141 - val_loss: 0.7233 - val_accuracy: 0.7729\n",
      "Epoch 219/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2067 - accuracy: 0.9151 - val_loss: 0.7112 - val_accuracy: 0.7668\n",
      "Epoch 220/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2043 - accuracy: 0.9163 - val_loss: 0.7212 - val_accuracy: 0.7626\n",
      "Epoch 221/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2056 - accuracy: 0.9139 - val_loss: 0.7217 - val_accuracy: 0.7655\n",
      "Epoch 222/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2048 - accuracy: 0.9160 - val_loss: 0.7153 - val_accuracy: 0.7685\n",
      "Epoch 223/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2047 - accuracy: 0.9144 - val_loss: 0.7316 - val_accuracy: 0.7621\n",
      "Epoch 224/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2040 - accuracy: 0.9167 - val_loss: 0.7245 - val_accuracy: 0.7618\n",
      "Epoch 225/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2030 - accuracy: 0.9152 - val_loss: 0.7378 - val_accuracy: 0.7668\n",
      "Epoch 226/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2051 - accuracy: 0.9175 - val_loss: 0.7156 - val_accuracy: 0.7672\n",
      "Epoch 227/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2013 - accuracy: 0.9172 - val_loss: 0.7352 - val_accuracy: 0.7685\n",
      "Epoch 228/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2018 - accuracy: 0.9168 - val_loss: 0.7327 - val_accuracy: 0.7679\n",
      "Epoch 229/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2014 - accuracy: 0.9162 - val_loss: 0.7226 - val_accuracy: 0.7744\n",
      "Epoch 230/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2019 - accuracy: 0.9164 - val_loss: 0.7351 - val_accuracy: 0.7609\n",
      "Epoch 231/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2027 - accuracy: 0.9166 - val_loss: 0.7414 - val_accuracy: 0.7633\n",
      "Epoch 232/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2010 - accuracy: 0.9165 - val_loss: 0.7281 - val_accuracy: 0.7664\n",
      "Epoch 233/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.2018 - accuracy: 0.9165 - val_loss: 0.7467 - val_accuracy: 0.7668\n",
      "Epoch 234/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1981 - accuracy: 0.9183 - val_loss: 0.7418 - val_accuracy: 0.7596\n",
      "Epoch 235/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1984 - accuracy: 0.9173 - val_loss: 0.7389 - val_accuracy: 0.7647\n",
      "Epoch 236/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1958 - accuracy: 0.9189 - val_loss: 0.7348 - val_accuracy: 0.7649\n",
      "Epoch 237/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1975 - accuracy: 0.9192 - val_loss: 0.7423 - val_accuracy: 0.7720\n",
      "Epoch 238/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1982 - accuracy: 0.9182 - val_loss: 0.7502 - val_accuracy: 0.7638\n",
      "Epoch 239/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1982 - accuracy: 0.9174 - val_loss: 0.7416 - val_accuracy: 0.7666\n",
      "Epoch 240/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1974 - accuracy: 0.9192 - val_loss: 0.7379 - val_accuracy: 0.7685\n",
      "Epoch 241/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9207 - val_loss: 0.7466 - val_accuracy: 0.7643\n",
      "Epoch 242/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9211 - val_loss: 0.7441 - val_accuracy: 0.7675\n",
      "Epoch 243/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9216 - val_loss: 0.7519 - val_accuracy: 0.7612\n",
      "Epoch 244/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1956 - accuracy: 0.9209 - val_loss: 0.7466 - val_accuracy: 0.7651\n",
      "Epoch 245/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1941 - accuracy: 0.9205 - val_loss: 0.7684 - val_accuracy: 0.7677\n",
      "Epoch 246/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1916 - accuracy: 0.9217 - val_loss: 0.7481 - val_accuracy: 0.7687\n",
      "Epoch 247/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9224 - val_loss: 0.7563 - val_accuracy: 0.7635\n",
      "Epoch 248/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1937 - accuracy: 0.9190 - val_loss: 0.7661 - val_accuracy: 0.7579\n",
      "Epoch 249/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9201 - val_loss: 0.7530 - val_accuracy: 0.7586\n",
      "Epoch 250/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1918 - accuracy: 0.9203 - val_loss: 0.7690 - val_accuracy: 0.7669\n",
      "Epoch 251/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1916 - accuracy: 0.9238 - val_loss: 0.7644 - val_accuracy: 0.7670\n",
      "Epoch 252/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1896 - accuracy: 0.9220 - val_loss: 0.7536 - val_accuracy: 0.7681\n",
      "Epoch 253/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9225 - val_loss: 0.7670 - val_accuracy: 0.7573\n",
      "Epoch 254/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1902 - accuracy: 0.9223 - val_loss: 0.7711 - val_accuracy: 0.7594\n",
      "Epoch 255/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9228 - val_loss: 0.7751 - val_accuracy: 0.7636\n",
      "Epoch 256/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9222 - val_loss: 0.7691 - val_accuracy: 0.7669\n",
      "Epoch 257/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9240 - val_loss: 0.7800 - val_accuracy: 0.7623\n",
      "Epoch 258/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9235 - val_loss: 0.7681 - val_accuracy: 0.7649\n",
      "Epoch 259/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9227 - val_loss: 0.7710 - val_accuracy: 0.7668\n",
      "Epoch 260/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9247 - val_loss: 0.7678 - val_accuracy: 0.7635\n",
      "Epoch 261/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9236 - val_loss: 0.7743 - val_accuracy: 0.7646\n",
      "Epoch 262/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9214 - val_loss: 0.7823 - val_accuracy: 0.7698\n",
      "Epoch 263/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9245 - val_loss: 0.7771 - val_accuracy: 0.7629\n",
      "Epoch 264/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9250 - val_loss: 0.7834 - val_accuracy: 0.7582\n",
      "Epoch 265/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9238 - val_loss: 0.7839 - val_accuracy: 0.7723\n",
      "Epoch 266/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9238 - val_loss: 0.7853 - val_accuracy: 0.7655\n",
      "Epoch 267/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9253 - val_loss: 0.7824 - val_accuracy: 0.7642\n",
      "Epoch 268/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9256 - val_loss: 0.7851 - val_accuracy: 0.7695\n",
      "Epoch 269/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9271 - val_loss: 0.7956 - val_accuracy: 0.7682\n",
      "Epoch 270/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9246 - val_loss: 0.7867 - val_accuracy: 0.7651\n",
      "Epoch 271/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9247 - val_loss: 0.7871 - val_accuracy: 0.7703\n",
      "Epoch 272/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9262 - val_loss: 0.7900 - val_accuracy: 0.7640\n",
      "Epoch 273/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9263 - val_loss: 0.7850 - val_accuracy: 0.7679\n",
      "Epoch 274/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9245 - val_loss: 0.7768 - val_accuracy: 0.7698\n",
      "Epoch 275/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9246 - val_loss: 0.7941 - val_accuracy: 0.7618\n",
      "Epoch 276/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9252 - val_loss: 0.7925 - val_accuracy: 0.7638\n",
      "Epoch 277/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9267 - val_loss: 0.7924 - val_accuracy: 0.7603\n",
      "Epoch 278/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9273 - val_loss: 0.7994 - val_accuracy: 0.7649\n",
      "Epoch 279/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9279 - val_loss: 0.7961 - val_accuracy: 0.7660\n",
      "Epoch 280/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9257 - val_loss: 0.7991 - val_accuracy: 0.7584\n",
      "Epoch 281/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1788 - accuracy: 0.9279 - val_loss: 0.7952 - val_accuracy: 0.7627\n",
      "Epoch 282/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1777 - accuracy: 0.9285 - val_loss: 0.8072 - val_accuracy: 0.7570\n",
      "Epoch 283/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1788 - accuracy: 0.9268 - val_loss: 0.7979 - val_accuracy: 0.7623\n",
      "Epoch 284/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9262 - val_loss: 0.7965 - val_accuracy: 0.7670\n",
      "Epoch 285/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1795 - accuracy: 0.9270 - val_loss: 0.7989 - val_accuracy: 0.7661\n",
      "Epoch 286/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9264 - val_loss: 0.8157 - val_accuracy: 0.7562\n",
      "Epoch 287/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1786 - accuracy: 0.9254 - val_loss: 0.8037 - val_accuracy: 0.7646\n",
      "Epoch 288/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1764 - accuracy: 0.9295 - val_loss: 0.8078 - val_accuracy: 0.7636\n",
      "Epoch 289/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1762 - accuracy: 0.9288 - val_loss: 0.8185 - val_accuracy: 0.7556\n",
      "Epoch 290/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1778 - accuracy: 0.9259 - val_loss: 0.8156 - val_accuracy: 0.7627\n",
      "Epoch 291/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1742 - accuracy: 0.9296 - val_loss: 0.8020 - val_accuracy: 0.7674\n",
      "Epoch 292/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1756 - accuracy: 0.9282 - val_loss: 0.8195 - val_accuracy: 0.7655\n",
      "Epoch 293/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1780 - accuracy: 0.9289 - val_loss: 0.8139 - val_accuracy: 0.7656\n",
      "Epoch 294/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1759 - accuracy: 0.9292 - val_loss: 0.8154 - val_accuracy: 0.7642\n",
      "Epoch 295/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1770 - accuracy: 0.9280 - val_loss: 0.8036 - val_accuracy: 0.7677\n",
      "Epoch 296/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1751 - accuracy: 0.9293 - val_loss: 0.8306 - val_accuracy: 0.7649\n",
      "Epoch 297/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1754 - accuracy: 0.9294 - val_loss: 0.8189 - val_accuracy: 0.7682\n",
      "Epoch 298/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1767 - accuracy: 0.9288 - val_loss: 0.8114 - val_accuracy: 0.7648\n",
      "Epoch 299/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1749 - accuracy: 0.9293 - val_loss: 0.8245 - val_accuracy: 0.7564\n",
      "Epoch 300/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1745 - accuracy: 0.9303 - val_loss: 0.8158 - val_accuracy: 0.7630\n",
      "Epoch 301/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1740 - accuracy: 0.9295 - val_loss: 0.8401 - val_accuracy: 0.7651\n",
      "Epoch 302/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1744 - accuracy: 0.9282 - val_loss: 0.8178 - val_accuracy: 0.7631\n",
      "Epoch 303/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1716 - accuracy: 0.9311 - val_loss: 0.8223 - val_accuracy: 0.7612\n",
      "Epoch 304/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1715 - accuracy: 0.9313 - val_loss: 0.8218 - val_accuracy: 0.7573\n",
      "Epoch 305/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1717 - accuracy: 0.9298 - val_loss: 0.8221 - val_accuracy: 0.7494\n",
      "Epoch 306/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1712 - accuracy: 0.9315 - val_loss: 0.8427 - val_accuracy: 0.7570\n",
      "Epoch 307/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1721 - accuracy: 0.9303 - val_loss: 0.8228 - val_accuracy: 0.7612\n",
      "Epoch 308/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1707 - accuracy: 0.9316 - val_loss: 0.8351 - val_accuracy: 0.7605\n",
      "Epoch 309/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1706 - accuracy: 0.9305 - val_loss: 0.8401 - val_accuracy: 0.7599\n",
      "Epoch 310/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1690 - accuracy: 0.9319 - val_loss: 0.8313 - val_accuracy: 0.7661\n",
      "Epoch 311/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1697 - accuracy: 0.9305 - val_loss: 0.8345 - val_accuracy: 0.7613\n",
      "Epoch 312/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1683 - accuracy: 0.9326 - val_loss: 0.8418 - val_accuracy: 0.7565\n",
      "Epoch 313/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1667 - accuracy: 0.9325 - val_loss: 0.8475 - val_accuracy: 0.7571\n",
      "Epoch 314/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1675 - accuracy: 0.9323 - val_loss: 0.8553 - val_accuracy: 0.7577\n",
      "Epoch 315/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1685 - accuracy: 0.9325 - val_loss: 0.8516 - val_accuracy: 0.7665\n",
      "Epoch 316/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1680 - accuracy: 0.9323 - val_loss: 0.8412 - val_accuracy: 0.7648\n",
      "Epoch 317/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1667 - accuracy: 0.9334 - val_loss: 0.8271 - val_accuracy: 0.7639\n",
      "Epoch 318/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1687 - accuracy: 0.9316 - val_loss: 0.8336 - val_accuracy: 0.7639\n",
      "Epoch 319/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1659 - accuracy: 0.9332 - val_loss: 0.8451 - val_accuracy: 0.7614\n",
      "Epoch 320/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1670 - accuracy: 0.9323 - val_loss: 0.8360 - val_accuracy: 0.7604\n",
      "Epoch 321/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1655 - accuracy: 0.9328 - val_loss: 0.8547 - val_accuracy: 0.7623\n",
      "Epoch 322/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1664 - accuracy: 0.9347 - val_loss: 0.8637 - val_accuracy: 0.7681\n",
      "Epoch 323/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1691 - accuracy: 0.9314 - val_loss: 0.8456 - val_accuracy: 0.7622\n",
      "Epoch 324/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1653 - accuracy: 0.9333 - val_loss: 0.8398 - val_accuracy: 0.7584\n",
      "Epoch 325/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1671 - accuracy: 0.9323 - val_loss: 0.8465 - val_accuracy: 0.7640\n",
      "Epoch 326/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1648 - accuracy: 0.9332 - val_loss: 0.8502 - val_accuracy: 0.7669\n",
      "Epoch 327/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1659 - accuracy: 0.9341 - val_loss: 0.8571 - val_accuracy: 0.7594\n",
      "Epoch 328/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1640 - accuracy: 0.9351 - val_loss: 0.8618 - val_accuracy: 0.7574\n",
      "Epoch 329/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1639 - accuracy: 0.9345 - val_loss: 0.8468 - val_accuracy: 0.7610\n",
      "Epoch 330/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1662 - accuracy: 0.9312 - val_loss: 0.8560 - val_accuracy: 0.7651\n",
      "Epoch 331/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1632 - accuracy: 0.9348 - val_loss: 0.8521 - val_accuracy: 0.7562\n",
      "Epoch 332/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1643 - accuracy: 0.9341 - val_loss: 0.8615 - val_accuracy: 0.7586\n",
      "Epoch 333/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1634 - accuracy: 0.9328 - val_loss: 0.8695 - val_accuracy: 0.7516\n",
      "Epoch 334/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1651 - accuracy: 0.9338 - val_loss: 0.8643 - val_accuracy: 0.7627\n",
      "Epoch 335/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1619 - accuracy: 0.9352 - val_loss: 0.8405 - val_accuracy: 0.7635\n",
      "Epoch 336/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1620 - accuracy: 0.9341 - val_loss: 0.8550 - val_accuracy: 0.7556\n",
      "Epoch 337/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1628 - accuracy: 0.9340 - val_loss: 0.8657 - val_accuracy: 0.7685\n",
      "Epoch 338/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1621 - accuracy: 0.9343 - val_loss: 0.8532 - val_accuracy: 0.7644\n",
      "Epoch 339/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1637 - accuracy: 0.9345 - val_loss: 0.8659 - val_accuracy: 0.7604\n",
      "Epoch 340/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1606 - accuracy: 0.9354 - val_loss: 0.8789 - val_accuracy: 0.7653\n",
      "Epoch 341/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1626 - accuracy: 0.9349 - val_loss: 0.8680 - val_accuracy: 0.7640\n",
      "Epoch 342/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1631 - accuracy: 0.9353 - val_loss: 0.8592 - val_accuracy: 0.7640\n",
      "Epoch 343/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1613 - accuracy: 0.9350 - val_loss: 0.8864 - val_accuracy: 0.7544\n",
      "Epoch 344/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1620 - accuracy: 0.9343 - val_loss: 0.8788 - val_accuracy: 0.7569\n",
      "Epoch 345/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1622 - accuracy: 0.9361 - val_loss: 0.8665 - val_accuracy: 0.7634\n",
      "Epoch 346/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1628 - accuracy: 0.9356 - val_loss: 0.8829 - val_accuracy: 0.7597\n",
      "Epoch 347/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1597 - accuracy: 0.9358 - val_loss: 0.8515 - val_accuracy: 0.7605\n",
      "Epoch 348/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1622 - accuracy: 0.9350 - val_loss: 0.8812 - val_accuracy: 0.7613\n",
      "Epoch 349/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1636 - accuracy: 0.9337 - val_loss: 0.8704 - val_accuracy: 0.7595\n",
      "Epoch 350/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1589 - accuracy: 0.9353 - val_loss: 0.8872 - val_accuracy: 0.7618\n",
      "Epoch 351/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1589 - accuracy: 0.9366 - val_loss: 0.8703 - val_accuracy: 0.7587\n",
      "Epoch 352/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1591 - accuracy: 0.9362 - val_loss: 0.8763 - val_accuracy: 0.7608\n",
      "Epoch 353/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1570 - accuracy: 0.9380 - val_loss: 0.8862 - val_accuracy: 0.7649\n",
      "Epoch 354/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1600 - accuracy: 0.9360 - val_loss: 0.8750 - val_accuracy: 0.7631\n",
      "Epoch 355/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1572 - accuracy: 0.9374 - val_loss: 0.8798 - val_accuracy: 0.7571\n",
      "Epoch 356/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1569 - accuracy: 0.9367 - val_loss: 0.8859 - val_accuracy: 0.7597\n",
      "Epoch 357/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1576 - accuracy: 0.9364 - val_loss: 0.8777 - val_accuracy: 0.7612\n",
      "Epoch 358/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1556 - accuracy: 0.9382 - val_loss: 0.8750 - val_accuracy: 0.7581\n",
      "Epoch 359/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1583 - accuracy: 0.9370 - val_loss: 0.8919 - val_accuracy: 0.7575\n",
      "Epoch 360/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1540 - accuracy: 0.9373 - val_loss: 0.8843 - val_accuracy: 0.7525\n",
      "Epoch 361/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1562 - accuracy: 0.9367 - val_loss: 0.8887 - val_accuracy: 0.7623\n",
      "Epoch 362/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1559 - accuracy: 0.9381 - val_loss: 0.8804 - val_accuracy: 0.7560\n",
      "Epoch 363/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1560 - accuracy: 0.9382 - val_loss: 0.8962 - val_accuracy: 0.7564\n",
      "Epoch 364/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1558 - accuracy: 0.9380 - val_loss: 0.8817 - val_accuracy: 0.7621\n",
      "Epoch 365/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1551 - accuracy: 0.9385 - val_loss: 0.8943 - val_accuracy: 0.7555\n",
      "Epoch 366/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1519 - accuracy: 0.9391 - val_loss: 0.9041 - val_accuracy: 0.7617\n",
      "Epoch 367/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1561 - accuracy: 0.9381 - val_loss: 0.8852 - val_accuracy: 0.7591\n",
      "Epoch 368/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1549 - accuracy: 0.9387 - val_loss: 0.8858 - val_accuracy: 0.7583\n",
      "Epoch 369/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1541 - accuracy: 0.9389 - val_loss: 0.8933 - val_accuracy: 0.7553\n",
      "Epoch 370/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1532 - accuracy: 0.9389 - val_loss: 0.8852 - val_accuracy: 0.7668\n",
      "Epoch 371/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1538 - accuracy: 0.9376 - val_loss: 0.8929 - val_accuracy: 0.7581\n",
      "Epoch 372/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1557 - accuracy: 0.9376 - val_loss: 0.9056 - val_accuracy: 0.7596\n",
      "Epoch 373/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1556 - accuracy: 0.9380 - val_loss: 0.8860 - val_accuracy: 0.7682\n",
      "Epoch 374/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1549 - accuracy: 0.9380 - val_loss: 0.8960 - val_accuracy: 0.7614\n",
      "Epoch 375/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1547 - accuracy: 0.9388 - val_loss: 0.8961 - val_accuracy: 0.7592\n",
      "Epoch 376/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1534 - accuracy: 0.9386 - val_loss: 0.9045 - val_accuracy: 0.7535\n",
      "Epoch 377/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1530 - accuracy: 0.9380 - val_loss: 0.8953 - val_accuracy: 0.7571\n",
      "Epoch 378/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1543 - accuracy: 0.9392 - val_loss: 0.8940 - val_accuracy: 0.7539\n",
      "Epoch 379/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1532 - accuracy: 0.9396 - val_loss: 0.8922 - val_accuracy: 0.7560\n",
      "Epoch 380/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1527 - accuracy: 0.9380 - val_loss: 0.9012 - val_accuracy: 0.7583\n",
      "Epoch 381/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1526 - accuracy: 0.9390 - val_loss: 0.9095 - val_accuracy: 0.7568\n",
      "Epoch 382/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1515 - accuracy: 0.9404 - val_loss: 0.9003 - val_accuracy: 0.7614\n",
      "Epoch 383/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1524 - accuracy: 0.9393 - val_loss: 0.9192 - val_accuracy: 0.7625\n",
      "Epoch 384/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1503 - accuracy: 0.9398 - val_loss: 0.9065 - val_accuracy: 0.7549\n",
      "Epoch 385/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.1529 - accuracy: 0.9408 - val_loss: 0.9217 - val_accuracy: 0.7571\n",
      "Epoch 386/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.1527 - accuracy: 0.9391 - val_loss: 0.9287 - val_accuracy: 0.7564\n",
      "Epoch 387/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1514 - accuracy: 0.9386 - val_loss: 0.9153 - val_accuracy: 0.7532\n",
      "Epoch 388/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1508 - accuracy: 0.9387 - val_loss: 0.9271 - val_accuracy: 0.7526\n",
      "Epoch 389/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1496 - accuracy: 0.9391 - val_loss: 0.9145 - val_accuracy: 0.7597\n",
      "Epoch 390/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1485 - accuracy: 0.9409 - val_loss: 0.9062 - val_accuracy: 0.7640\n",
      "Epoch 391/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1507 - accuracy: 0.9405 - val_loss: 0.9188 - val_accuracy: 0.7638\n",
      "Epoch 392/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1498 - accuracy: 0.9399 - val_loss: 0.9173 - val_accuracy: 0.7553\n",
      "Epoch 393/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1506 - accuracy: 0.9387 - val_loss: 0.9188 - val_accuracy: 0.7627\n",
      "Epoch 394/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1499 - accuracy: 0.9399 - val_loss: 0.9331 - val_accuracy: 0.7605\n",
      "Epoch 395/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1499 - accuracy: 0.9404 - val_loss: 0.9285 - val_accuracy: 0.7612\n",
      "Epoch 396/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1503 - accuracy: 0.9406 - val_loss: 0.9231 - val_accuracy: 0.7516\n",
      "Epoch 397/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1489 - accuracy: 0.9423 - val_loss: 0.9327 - val_accuracy: 0.7518\n",
      "Epoch 398/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1515 - accuracy: 0.9408 - val_loss: 0.9170 - val_accuracy: 0.7578\n",
      "Epoch 399/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1497 - accuracy: 0.9405 - val_loss: 0.9322 - val_accuracy: 0.7562\n",
      "Epoch 400/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1463 - accuracy: 0.9415 - val_loss: 0.9309 - val_accuracy: 0.7522\n",
      "Epoch 401/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1486 - accuracy: 0.9407 - val_loss: 0.9198 - val_accuracy: 0.7605\n",
      "Epoch 402/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1486 - accuracy: 0.9409 - val_loss: 0.9180 - val_accuracy: 0.7565\n",
      "Epoch 403/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1461 - accuracy: 0.9416 - val_loss: 0.9198 - val_accuracy: 0.7631\n",
      "Epoch 404/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1468 - accuracy: 0.9427 - val_loss: 0.9423 - val_accuracy: 0.7590\n",
      "Epoch 405/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1459 - accuracy: 0.9424 - val_loss: 0.9241 - val_accuracy: 0.7570\n",
      "Epoch 406/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1464 - accuracy: 0.9411 - val_loss: 0.9308 - val_accuracy: 0.7545\n",
      "Epoch 407/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1476 - accuracy: 0.9419 - val_loss: 0.9237 - val_accuracy: 0.7574\n",
      "Epoch 408/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1458 - accuracy: 0.9414 - val_loss: 0.9468 - val_accuracy: 0.7525\n",
      "Epoch 409/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1453 - accuracy: 0.9413 - val_loss: 0.9215 - val_accuracy: 0.7597\n",
      "Epoch 410/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1463 - accuracy: 0.9421 - val_loss: 0.9232 - val_accuracy: 0.7613\n",
      "Epoch 411/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1457 - accuracy: 0.9431 - val_loss: 0.9276 - val_accuracy: 0.7597\n",
      "Epoch 412/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1452 - accuracy: 0.9436 - val_loss: 0.9251 - val_accuracy: 0.7586\n",
      "Epoch 413/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1462 - accuracy: 0.9415 - val_loss: 0.9372 - val_accuracy: 0.7617\n",
      "Epoch 414/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1462 - accuracy: 0.9418 - val_loss: 0.9325 - val_accuracy: 0.7543\n",
      "Epoch 415/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1468 - accuracy: 0.9429 - val_loss: 0.9462 - val_accuracy: 0.7626\n",
      "Epoch 416/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1445 - accuracy: 0.9437 - val_loss: 0.9367 - val_accuracy: 0.7604\n",
      "Epoch 417/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1447 - accuracy: 0.9425 - val_loss: 0.9267 - val_accuracy: 0.7599\n",
      "Epoch 418/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1464 - accuracy: 0.9413 - val_loss: 0.9489 - val_accuracy: 0.7609\n",
      "Epoch 419/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1461 - accuracy: 0.9419 - val_loss: 0.9452 - val_accuracy: 0.7575\n",
      "Epoch 420/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1459 - accuracy: 0.9420 - val_loss: 0.9400 - val_accuracy: 0.7620\n",
      "Epoch 421/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1416 - accuracy: 0.9440 - val_loss: 0.9487 - val_accuracy: 0.7625\n",
      "Epoch 422/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1445 - accuracy: 0.9416 - val_loss: 0.9441 - val_accuracy: 0.7571\n",
      "Epoch 423/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1438 - accuracy: 0.9424 - val_loss: 0.9519 - val_accuracy: 0.7588\n",
      "Epoch 424/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1473 - accuracy: 0.9410 - val_loss: 0.9421 - val_accuracy: 0.7549\n",
      "Epoch 425/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1459 - accuracy: 0.9416 - val_loss: 0.9431 - val_accuracy: 0.7581\n",
      "Epoch 426/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1462 - accuracy: 0.9417 - val_loss: 0.9406 - val_accuracy: 0.7579\n",
      "Epoch 427/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1459 - accuracy: 0.9422 - val_loss: 0.9339 - val_accuracy: 0.7610\n",
      "Epoch 428/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1441 - accuracy: 0.9411 - val_loss: 0.9571 - val_accuracy: 0.7601\n",
      "Epoch 429/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1451 - accuracy: 0.9425 - val_loss: 0.9444 - val_accuracy: 0.7571\n",
      "Epoch 430/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1416 - accuracy: 0.9427 - val_loss: 0.9620 - val_accuracy: 0.7590\n",
      "Epoch 431/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1419 - accuracy: 0.9430 - val_loss: 0.9581 - val_accuracy: 0.7525\n",
      "Epoch 432/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1436 - accuracy: 0.9424 - val_loss: 0.9604 - val_accuracy: 0.7590\n",
      "Epoch 433/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1430 - accuracy: 0.9436 - val_loss: 0.9437 - val_accuracy: 0.7614\n",
      "Epoch 434/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1439 - accuracy: 0.9427 - val_loss: 0.9555 - val_accuracy: 0.7575\n",
      "Epoch 435/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1425 - accuracy: 0.9431 - val_loss: 0.9523 - val_accuracy: 0.7592\n",
      "Epoch 436/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1417 - accuracy: 0.9441 - val_loss: 0.9480 - val_accuracy: 0.7555\n",
      "Epoch 437/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1444 - accuracy: 0.9428 - val_loss: 0.9536 - val_accuracy: 0.7612\n",
      "Epoch 438/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1409 - accuracy: 0.9452 - val_loss: 0.9559 - val_accuracy: 0.7605\n",
      "Epoch 439/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1421 - accuracy: 0.9427 - val_loss: 0.9630 - val_accuracy: 0.7538\n",
      "Epoch 440/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1399 - accuracy: 0.9438 - val_loss: 0.9602 - val_accuracy: 0.7530\n",
      "Epoch 441/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1413 - accuracy: 0.9440 - val_loss: 0.9639 - val_accuracy: 0.7587\n",
      "Epoch 442/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1389 - accuracy: 0.9445 - val_loss: 0.9634 - val_accuracy: 0.7570\n",
      "Epoch 443/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1382 - accuracy: 0.9449 - val_loss: 0.9796 - val_accuracy: 0.7540\n",
      "Epoch 444/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1397 - accuracy: 0.9443 - val_loss: 0.9712 - val_accuracy: 0.7542\n",
      "Epoch 445/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1396 - accuracy: 0.9448 - val_loss: 0.9569 - val_accuracy: 0.7544\n",
      "Epoch 446/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1398 - accuracy: 0.9455 - val_loss: 0.9659 - val_accuracy: 0.7595\n",
      "Epoch 447/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1403 - accuracy: 0.9444 - val_loss: 0.9649 - val_accuracy: 0.7500\n",
      "Epoch 448/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1403 - accuracy: 0.9438 - val_loss: 0.9621 - val_accuracy: 0.7548\n",
      "Epoch 449/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1406 - accuracy: 0.9449 - val_loss: 0.9695 - val_accuracy: 0.7568\n",
      "Epoch 450/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1411 - accuracy: 0.9444 - val_loss: 0.9838 - val_accuracy: 0.7548\n",
      "Epoch 451/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1386 - accuracy: 0.9446 - val_loss: 0.9592 - val_accuracy: 0.7613\n",
      "Epoch 452/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1421 - accuracy: 0.9442 - val_loss: 0.9777 - val_accuracy: 0.7608\n",
      "Epoch 453/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1404 - accuracy: 0.9450 - val_loss: 0.9677 - val_accuracy: 0.7586\n",
      "Epoch 454/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1390 - accuracy: 0.9447 - val_loss: 0.9763 - val_accuracy: 0.7538\n",
      "Epoch 455/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1388 - accuracy: 0.9450 - val_loss: 0.9748 - val_accuracy: 0.7547\n",
      "Epoch 456/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1385 - accuracy: 0.9449 - val_loss: 0.9828 - val_accuracy: 0.7599\n",
      "Epoch 457/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1389 - accuracy: 0.9448 - val_loss: 0.9984 - val_accuracy: 0.7562\n",
      "Epoch 458/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1401 - accuracy: 0.9445 - val_loss: 0.9640 - val_accuracy: 0.7561\n",
      "Epoch 459/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1379 - accuracy: 0.9459 - val_loss: 0.9839 - val_accuracy: 0.7591\n",
      "Epoch 460/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1396 - accuracy: 0.9445 - val_loss: 0.9795 - val_accuracy: 0.7542\n",
      "Epoch 461/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1405 - accuracy: 0.9445 - val_loss: 0.9847 - val_accuracy: 0.7539\n",
      "Epoch 462/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1396 - accuracy: 0.9427 - val_loss: 0.9715 - val_accuracy: 0.7552\n",
      "Epoch 463/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1376 - accuracy: 0.9454 - val_loss: 0.9829 - val_accuracy: 0.7579\n",
      "Epoch 464/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1361 - accuracy: 0.9448 - val_loss: 0.9879 - val_accuracy: 0.7621\n",
      "Epoch 465/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1397 - accuracy: 0.9447 - val_loss: 0.9818 - val_accuracy: 0.7568\n",
      "Epoch 466/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1394 - accuracy: 0.9445 - val_loss: 0.9787 - val_accuracy: 0.7597\n",
      "Epoch 467/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1400 - accuracy: 0.9449 - val_loss: 0.9766 - val_accuracy: 0.7612\n",
      "Epoch 468/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1413 - accuracy: 0.9438 - val_loss: 0.9634 - val_accuracy: 0.7610\n",
      "Epoch 469/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1383 - accuracy: 0.9455 - val_loss: 0.9966 - val_accuracy: 0.7618\n",
      "Epoch 470/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1381 - accuracy: 0.9458 - val_loss: 0.9828 - val_accuracy: 0.7579\n",
      "Epoch 471/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1396 - accuracy: 0.9451 - val_loss: 0.9794 - val_accuracy: 0.7625\n",
      "Epoch 472/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1362 - accuracy: 0.9456 - val_loss: 0.9862 - val_accuracy: 0.7549\n",
      "Epoch 473/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1372 - accuracy: 0.9457 - val_loss: 0.9873 - val_accuracy: 0.7569\n",
      "Epoch 474/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1366 - accuracy: 0.9456 - val_loss: 0.9718 - val_accuracy: 0.7588\n",
      "Epoch 475/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1389 - accuracy: 0.9443 - val_loss: 0.9881 - val_accuracy: 0.7551\n",
      "Epoch 476/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1363 - accuracy: 0.9473 - val_loss: 0.9763 - val_accuracy: 0.7594\n",
      "Epoch 477/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1368 - accuracy: 0.9464 - val_loss: 0.9920 - val_accuracy: 0.7595\n",
      "Epoch 478/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1346 - accuracy: 0.9479 - val_loss: 0.9898 - val_accuracy: 0.7558\n",
      "Epoch 479/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1352 - accuracy: 0.9461 - val_loss: 1.0001 - val_accuracy: 0.7594\n",
      "Epoch 480/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1373 - accuracy: 0.9460 - val_loss: 0.9997 - val_accuracy: 0.7529\n",
      "Epoch 481/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1348 - accuracy: 0.9464 - val_loss: 0.9948 - val_accuracy: 0.7599\n",
      "Epoch 482/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1353 - accuracy: 0.9476 - val_loss: 0.9932 - val_accuracy: 0.7542\n",
      "Epoch 483/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1361 - accuracy: 0.9464 - val_loss: 1.0153 - val_accuracy: 0.7622\n",
      "Epoch 484/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1379 - accuracy: 0.9454 - val_loss: 0.9979 - val_accuracy: 0.7549\n",
      "Epoch 485/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1379 - accuracy: 0.9450 - val_loss: 0.9795 - val_accuracy: 0.7600\n",
      "Epoch 486/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.1337 - accuracy: 0.9463 - val_loss: 0.9949 - val_accuracy: 0.7608\n",
      "Epoch 487/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.1333 - accuracy: 0.9469 - val_loss: 0.9940 - val_accuracy: 0.7564\n",
      "Epoch 488/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.1351 - accuracy: 0.9460 - val_loss: 1.0197 - val_accuracy: 0.7588\n",
      "Epoch 489/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.1360 - accuracy: 0.9462 - val_loss: 0.9873 - val_accuracy: 0.7581\n",
      "Epoch 490/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1356 - accuracy: 0.9471 - val_loss: 1.0077 - val_accuracy: 0.7600\n",
      "Epoch 491/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.1336 - accuracy: 0.9478 - val_loss: 0.9972 - val_accuracy: 0.7594\n",
      "Epoch 492/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1339 - accuracy: 0.9462 - val_loss: 0.9944 - val_accuracy: 0.7620\n",
      "Epoch 493/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.1359 - accuracy: 0.9471 - val_loss: 1.0217 - val_accuracy: 0.7539\n",
      "Epoch 494/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.1358 - accuracy: 0.9470 - val_loss: 0.9994 - val_accuracy: 0.7568\n",
      "Epoch 495/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1329 - accuracy: 0.9463 - val_loss: 0.9919 - val_accuracy: 0.7562\n",
      "Epoch 496/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1330 - accuracy: 0.9480 - val_loss: 0.9924 - val_accuracy: 0.7539\n",
      "Epoch 497/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1323 - accuracy: 0.9480 - val_loss: 1.0113 - val_accuracy: 0.7583\n",
      "Epoch 498/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1340 - accuracy: 0.9474 - val_loss: 0.9949 - val_accuracy: 0.7620\n",
      "Epoch 499/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1367 - accuracy: 0.9457 - val_loss: 1.0127 - val_accuracy: 0.7574\n",
      "Epoch 500/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.1361 - accuracy: 0.9454 - val_loss: 0.9951 - val_accuracy: 0.7604\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.9951 - accuracy: 0.7604\n",
      "\n",
      "--------------\n",
      " [0.9950832724571228, 0.7603949904441833]\n",
      "241/241 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.7603950103950103\n",
      "Recall: 0.7858439201451906\n",
      "Precision: 0.7485798962706841\n",
      "F1: 0.7667594232228686\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      " relu \n",
      "\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 200)               2400      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 150)               30150     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 50)                7550      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,151\n",
      "Trainable params: 40,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n",
      "962/962 [==============================] - 3s 2ms/step - loss: 0.5218 - accuracy: 0.7440 - val_loss: 0.5116 - val_accuracy: 0.7487\n",
      "Epoch 2/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4974 - accuracy: 0.7587 - val_loss: 0.4935 - val_accuracy: 0.7574\n",
      "Epoch 3/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4881 - accuracy: 0.7645 - val_loss: 0.5051 - val_accuracy: 0.7555\n",
      "Epoch 4/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4818 - accuracy: 0.7663 - val_loss: 0.4848 - val_accuracy: 0.7655\n",
      "Epoch 5/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4764 - accuracy: 0.7710 - val_loss: 0.4832 - val_accuracy: 0.7648\n",
      "Epoch 6/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4723 - accuracy: 0.7740 - val_loss: 0.4759 - val_accuracy: 0.7701\n",
      "Epoch 7/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4695 - accuracy: 0.7759 - val_loss: 0.4818 - val_accuracy: 0.7695\n",
      "Epoch 8/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4662 - accuracy: 0.7784 - val_loss: 0.4836 - val_accuracy: 0.7636\n",
      "Epoch 9/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4637 - accuracy: 0.7799 - val_loss: 0.4903 - val_accuracy: 0.7607\n",
      "Epoch 10/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4614 - accuracy: 0.7818 - val_loss: 0.4698 - val_accuracy: 0.7733\n",
      "Epoch 11/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4588 - accuracy: 0.7825 - val_loss: 0.4751 - val_accuracy: 0.7762\n",
      "Epoch 12/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4569 - accuracy: 0.7852 - val_loss: 0.4742 - val_accuracy: 0.7769\n",
      "Epoch 13/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4553 - accuracy: 0.7845 - val_loss: 0.4687 - val_accuracy: 0.7813\n",
      "Epoch 14/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4527 - accuracy: 0.7858 - val_loss: 0.4684 - val_accuracy: 0.7708\n",
      "Epoch 15/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4521 - accuracy: 0.7861 - val_loss: 0.4654 - val_accuracy: 0.7760\n",
      "Epoch 16/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4510 - accuracy: 0.7892 - val_loss: 0.4803 - val_accuracy: 0.7704\n",
      "Epoch 17/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4493 - accuracy: 0.7904 - val_loss: 0.4614 - val_accuracy: 0.7821\n",
      "Epoch 18/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4464 - accuracy: 0.7910 - val_loss: 0.4677 - val_accuracy: 0.7878\n",
      "Epoch 19/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4455 - accuracy: 0.7907 - val_loss: 0.4750 - val_accuracy: 0.7762\n",
      "Epoch 20/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4444 - accuracy: 0.7922 - val_loss: 0.4712 - val_accuracy: 0.7852\n",
      "Epoch 21/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4441 - accuracy: 0.7933 - val_loss: 0.4667 - val_accuracy: 0.7825\n",
      "Epoch 22/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4434 - accuracy: 0.7926 - val_loss: 0.4644 - val_accuracy: 0.7817\n",
      "Epoch 23/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4433 - accuracy: 0.7947 - val_loss: 0.4596 - val_accuracy: 0.7827\n",
      "Epoch 24/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4413 - accuracy: 0.7958 - val_loss: 0.4787 - val_accuracy: 0.7826\n",
      "Epoch 25/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4391 - accuracy: 0.7955 - val_loss: 0.4685 - val_accuracy: 0.7816\n",
      "Epoch 26/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4374 - accuracy: 0.7976 - val_loss: 0.4728 - val_accuracy: 0.7840\n",
      "Epoch 27/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4384 - accuracy: 0.7974 - val_loss: 0.4620 - val_accuracy: 0.7870\n",
      "Epoch 28/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4374 - accuracy: 0.7986 - val_loss: 0.4701 - val_accuracy: 0.7857\n",
      "Epoch 29/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4353 - accuracy: 0.7999 - val_loss: 0.4620 - val_accuracy: 0.7887\n",
      "Epoch 30/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4357 - accuracy: 0.7999 - val_loss: 0.4850 - val_accuracy: 0.7760\n",
      "Epoch 31/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4363 - accuracy: 0.7988 - val_loss: 0.4687 - val_accuracy: 0.7857\n",
      "Epoch 32/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4349 - accuracy: 0.7999 - val_loss: 0.4619 - val_accuracy: 0.7916\n",
      "Epoch 33/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4344 - accuracy: 0.8000 - val_loss: 0.4617 - val_accuracy: 0.7924\n",
      "Epoch 34/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4331 - accuracy: 0.8030 - val_loss: 0.4753 - val_accuracy: 0.7890\n",
      "Epoch 35/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4335 - accuracy: 0.7984 - val_loss: 0.5060 - val_accuracy: 0.7830\n",
      "Epoch 36/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4325 - accuracy: 0.8010 - val_loss: 0.4760 - val_accuracy: 0.7899\n",
      "Epoch 37/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4311 - accuracy: 0.8023 - val_loss: 0.4712 - val_accuracy: 0.7822\n",
      "Epoch 38/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4296 - accuracy: 0.8024 - val_loss: 0.4757 - val_accuracy: 0.7866\n",
      "Epoch 39/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4308 - accuracy: 0.8028 - val_loss: 0.4782 - val_accuracy: 0.7891\n",
      "Epoch 40/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4293 - accuracy: 0.8036 - val_loss: 0.4716 - val_accuracy: 0.7917\n",
      "Epoch 41/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4280 - accuracy: 0.8047 - val_loss: 0.4651 - val_accuracy: 0.7931\n",
      "Epoch 42/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4284 - accuracy: 0.8048 - val_loss: 0.4813 - val_accuracy: 0.7899\n",
      "Epoch 43/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4267 - accuracy: 0.8044 - val_loss: 0.4883 - val_accuracy: 0.7882\n",
      "Epoch 44/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4284 - accuracy: 0.8043 - val_loss: 0.4855 - val_accuracy: 0.7912\n",
      "Epoch 45/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4250 - accuracy: 0.8075 - val_loss: 0.4856 - val_accuracy: 0.7900\n",
      "Epoch 46/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4275 - accuracy: 0.8066 - val_loss: 0.4814 - val_accuracy: 0.7848\n",
      "Epoch 47/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4250 - accuracy: 0.8066 - val_loss: 0.4874 - val_accuracy: 0.7908\n",
      "Epoch 48/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4249 - accuracy: 0.8064 - val_loss: 0.4774 - val_accuracy: 0.7916\n",
      "Epoch 49/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4256 - accuracy: 0.8081 - val_loss: 0.4829 - val_accuracy: 0.7887\n",
      "Epoch 50/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4277 - accuracy: 0.8067 - val_loss: 0.4989 - val_accuracy: 0.7908\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4267 - accuracy: 0.8072 - val_loss: 0.4957 - val_accuracy: 0.7877\n",
      "Epoch 52/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4244 - accuracy: 0.8088 - val_loss: 0.4981 - val_accuracy: 0.7840\n",
      "Epoch 53/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4254 - accuracy: 0.8078 - val_loss: 0.4878 - val_accuracy: 0.7891\n",
      "Epoch 54/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4252 - accuracy: 0.8095 - val_loss: 0.4989 - val_accuracy: 0.7883\n",
      "Epoch 55/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4251 - accuracy: 0.8087 - val_loss: 0.4926 - val_accuracy: 0.7929\n",
      "Epoch 56/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4261 - accuracy: 0.8084 - val_loss: 0.5072 - val_accuracy: 0.7915\n",
      "Epoch 57/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4243 - accuracy: 0.8087 - val_loss: 0.5086 - val_accuracy: 0.7920\n",
      "Epoch 58/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4253 - accuracy: 0.8105 - val_loss: 0.5385 - val_accuracy: 0.7946\n",
      "Epoch 59/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4253 - accuracy: 0.8079 - val_loss: 0.5145 - val_accuracy: 0.7900\n",
      "Epoch 60/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.4253 - accuracy: 0.8100 - val_loss: 0.5109 - val_accuracy: 0.7939\n",
      "Epoch 61/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4296 - accuracy: 0.8102 - val_loss: 0.4897 - val_accuracy: 0.7904\n",
      "Epoch 62/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4279 - accuracy: 0.8107 - val_loss: 0.5136 - val_accuracy: 0.7879\n",
      "Epoch 63/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4243 - accuracy: 0.8108 - val_loss: 0.5069 - val_accuracy: 0.7899\n",
      "Epoch 64/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4238 - accuracy: 0.8103 - val_loss: 0.5082 - val_accuracy: 0.7916\n",
      "Epoch 65/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4218 - accuracy: 0.8097 - val_loss: 0.5001 - val_accuracy: 0.7920\n",
      "Epoch 66/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4219 - accuracy: 0.8107 - val_loss: 0.4941 - val_accuracy: 0.7903\n",
      "Epoch 67/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4273 - accuracy: 0.8105 - val_loss: 0.4893 - val_accuracy: 0.7905\n",
      "Epoch 68/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4227 - accuracy: 0.8106 - val_loss: 0.5078 - val_accuracy: 0.7934\n",
      "Epoch 69/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4240 - accuracy: 0.8079 - val_loss: 0.4922 - val_accuracy: 0.7960\n",
      "Epoch 70/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4241 - accuracy: 0.8108 - val_loss: 0.4983 - val_accuracy: 0.7957\n",
      "Epoch 71/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4223 - accuracy: 0.8077 - val_loss: 0.4949 - val_accuracy: 0.7937\n",
      "Epoch 72/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4231 - accuracy: 0.8105 - val_loss: 0.4949 - val_accuracy: 0.7974\n",
      "Epoch 73/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4214 - accuracy: 0.8132 - val_loss: 0.4994 - val_accuracy: 0.7918\n",
      "Epoch 74/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4209 - accuracy: 0.8112 - val_loss: 0.5396 - val_accuracy: 0.7938\n",
      "Epoch 75/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4229 - accuracy: 0.8124 - val_loss: 0.4933 - val_accuracy: 0.7982\n",
      "Epoch 76/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4207 - accuracy: 0.8112 - val_loss: 0.5041 - val_accuracy: 0.7895\n",
      "Epoch 77/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4255 - accuracy: 0.8118 - val_loss: 0.5156 - val_accuracy: 0.7878\n",
      "Epoch 78/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4210 - accuracy: 0.8116 - val_loss: 0.5031 - val_accuracy: 0.7909\n",
      "Epoch 79/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4180 - accuracy: 0.8134 - val_loss: 0.5155 - val_accuracy: 0.7911\n",
      "Epoch 80/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4217 - accuracy: 0.8098 - val_loss: 0.5197 - val_accuracy: 0.7966\n",
      "Epoch 81/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4230 - accuracy: 0.8131 - val_loss: 0.5145 - val_accuracy: 0.7909\n",
      "Epoch 82/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4228 - accuracy: 0.8120 - val_loss: 0.5233 - val_accuracy: 0.7935\n",
      "Epoch 83/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4200 - accuracy: 0.8120 - val_loss: 0.5199 - val_accuracy: 0.7885\n",
      "Epoch 84/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4207 - accuracy: 0.8145 - val_loss: 0.5495 - val_accuracy: 0.7911\n",
      "Epoch 85/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4247 - accuracy: 0.8136 - val_loss: 0.5486 - val_accuracy: 0.7917\n",
      "Epoch 86/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4232 - accuracy: 0.8117 - val_loss: 0.5108 - val_accuracy: 0.7953\n",
      "Epoch 87/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4184 - accuracy: 0.8136 - val_loss: 0.5207 - val_accuracy: 0.7909\n",
      "Epoch 88/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4202 - accuracy: 0.8119 - val_loss: 0.5435 - val_accuracy: 0.7905\n",
      "Epoch 89/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4184 - accuracy: 0.8152 - val_loss: 0.5020 - val_accuracy: 0.7947\n",
      "Epoch 90/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4245 - accuracy: 0.8123 - val_loss: 0.5119 - val_accuracy: 0.7837\n",
      "Epoch 91/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4214 - accuracy: 0.8132 - val_loss: 0.4960 - val_accuracy: 0.7917\n",
      "Epoch 92/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4212 - accuracy: 0.8141 - val_loss: 0.4932 - val_accuracy: 0.7944\n",
      "Epoch 93/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4237 - accuracy: 0.8133 - val_loss: 0.5090 - val_accuracy: 0.7922\n",
      "Epoch 94/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4158 - accuracy: 0.8130 - val_loss: 0.5762 - val_accuracy: 0.7947\n",
      "Epoch 95/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4199 - accuracy: 0.8159 - val_loss: 0.5245 - val_accuracy: 0.7899\n",
      "Epoch 96/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4188 - accuracy: 0.8136 - val_loss: 0.5352 - val_accuracy: 0.7922\n",
      "Epoch 97/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4153 - accuracy: 0.8151 - val_loss: 0.5250 - val_accuracy: 0.7944\n",
      "Epoch 98/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4211 - accuracy: 0.8154 - val_loss: 0.5471 - val_accuracy: 0.7938\n",
      "Epoch 99/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4146 - accuracy: 0.8153 - val_loss: 0.5368 - val_accuracy: 0.7926\n",
      "Epoch 100/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4176 - accuracy: 0.8161 - val_loss: 0.5225 - val_accuracy: 0.7896\n",
      "Epoch 101/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4155 - accuracy: 0.8145 - val_loss: 0.5569 - val_accuracy: 0.7909\n",
      "Epoch 102/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4176 - accuracy: 0.8155 - val_loss: 0.5429 - val_accuracy: 0.7904\n",
      "Epoch 103/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4162 - accuracy: 0.8160 - val_loss: 0.5589 - val_accuracy: 0.7889\n",
      "Epoch 104/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4193 - accuracy: 0.8154 - val_loss: 0.5332 - val_accuracy: 0.7968\n",
      "Epoch 105/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4194 - accuracy: 0.8156 - val_loss: 0.5235 - val_accuracy: 0.7924\n",
      "Epoch 106/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4254 - accuracy: 0.8149 - val_loss: 0.5330 - val_accuracy: 0.7900\n",
      "Epoch 107/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4266 - accuracy: 0.8165 - val_loss: 0.5289 - val_accuracy: 0.7939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4257 - accuracy: 0.8178 - val_loss: 0.5717 - val_accuracy: 0.7899\n",
      "Epoch 109/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4179 - accuracy: 0.8162 - val_loss: 0.5525 - val_accuracy: 0.7927\n",
      "Epoch 110/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4268 - accuracy: 0.8176 - val_loss: 0.5282 - val_accuracy: 0.7942\n",
      "Epoch 111/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4226 - accuracy: 0.8180 - val_loss: 0.5636 - val_accuracy: 0.7960\n",
      "Epoch 112/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4188 - accuracy: 0.8148 - val_loss: 0.5970 - val_accuracy: 0.7917\n",
      "Epoch 113/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4230 - accuracy: 0.8170 - val_loss: 0.5579 - val_accuracy: 0.7925\n",
      "Epoch 114/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4176 - accuracy: 0.8173 - val_loss: 0.5158 - val_accuracy: 0.7935\n",
      "Epoch 115/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4261 - accuracy: 0.8170 - val_loss: 0.5505 - val_accuracy: 0.7915\n",
      "Epoch 116/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4161 - accuracy: 0.8157 - val_loss: 0.5955 - val_accuracy: 0.7940\n",
      "Epoch 117/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4225 - accuracy: 0.8153 - val_loss: 0.6096 - val_accuracy: 0.7950\n",
      "Epoch 118/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4212 - accuracy: 0.8166 - val_loss: 0.5622 - val_accuracy: 0.7865\n",
      "Epoch 119/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4222 - accuracy: 0.8179 - val_loss: 0.6050 - val_accuracy: 0.7905\n",
      "Epoch 120/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4158 - accuracy: 0.8172 - val_loss: 0.5941 - val_accuracy: 0.7959\n",
      "Epoch 121/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4222 - accuracy: 0.8164 - val_loss: 0.5652 - val_accuracy: 0.7963\n",
      "Epoch 122/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4206 - accuracy: 0.8160 - val_loss: 0.5915 - val_accuracy: 0.7951\n",
      "Epoch 123/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4199 - accuracy: 0.8156 - val_loss: 0.5930 - val_accuracy: 0.7925\n",
      "Epoch 124/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4268 - accuracy: 0.8173 - val_loss: 0.5852 - val_accuracy: 0.7915\n",
      "Epoch 125/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4209 - accuracy: 0.8181 - val_loss: 0.5833 - val_accuracy: 0.7920\n",
      "Epoch 126/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4207 - accuracy: 0.8173 - val_loss: 0.5538 - val_accuracy: 0.7965\n",
      "Epoch 127/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4217 - accuracy: 0.8185 - val_loss: 0.6409 - val_accuracy: 0.7934\n",
      "Epoch 128/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4184 - accuracy: 0.8175 - val_loss: 0.6660 - val_accuracy: 0.7929\n",
      "Epoch 129/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4292 - accuracy: 0.8173 - val_loss: 0.6197 - val_accuracy: 0.7911\n",
      "Epoch 130/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4164 - accuracy: 0.8174 - val_loss: 0.5994 - val_accuracy: 0.7940\n",
      "Epoch 131/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4127 - accuracy: 0.8169 - val_loss: 0.6509 - val_accuracy: 0.7947\n",
      "Epoch 132/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4218 - accuracy: 0.8174 - val_loss: 0.6004 - val_accuracy: 0.7943\n",
      "Epoch 133/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4185 - accuracy: 0.8195 - val_loss: 0.5870 - val_accuracy: 0.7950\n",
      "Epoch 134/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4283 - accuracy: 0.8177 - val_loss: 0.6646 - val_accuracy: 0.7946\n",
      "Epoch 135/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4144 - accuracy: 0.8176 - val_loss: 0.6803 - val_accuracy: 0.7982\n",
      "Epoch 136/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4204 - accuracy: 0.8203 - val_loss: 0.6682 - val_accuracy: 0.7929\n",
      "Epoch 137/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4199 - accuracy: 0.8203 - val_loss: 0.6429 - val_accuracy: 0.7961\n",
      "Epoch 138/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4228 - accuracy: 0.8180 - val_loss: 0.7479 - val_accuracy: 0.7935\n",
      "Epoch 139/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4263 - accuracy: 0.8186 - val_loss: 0.5770 - val_accuracy: 0.7977\n",
      "Epoch 140/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4197 - accuracy: 0.8196 - val_loss: 0.5878 - val_accuracy: 0.7916\n",
      "Epoch 141/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4208 - accuracy: 0.8189 - val_loss: 0.5817 - val_accuracy: 0.7903\n",
      "Epoch 142/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4269 - accuracy: 0.8192 - val_loss: 0.5684 - val_accuracy: 0.7912\n",
      "Epoch 143/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4253 - accuracy: 0.8200 - val_loss: 0.6605 - val_accuracy: 0.7934\n",
      "Epoch 144/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4206 - accuracy: 0.8172 - val_loss: 0.6527 - val_accuracy: 0.7905\n",
      "Epoch 145/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4104 - accuracy: 0.8186 - val_loss: 0.7134 - val_accuracy: 0.7851\n",
      "Epoch 146/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4291 - accuracy: 0.8195 - val_loss: 0.7046 - val_accuracy: 0.7929\n",
      "Epoch 147/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4167 - accuracy: 0.8200 - val_loss: 0.6283 - val_accuracy: 0.7935\n",
      "Epoch 148/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4236 - accuracy: 0.8194 - val_loss: 0.7244 - val_accuracy: 0.7904\n",
      "Epoch 149/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4156 - accuracy: 0.8187 - val_loss: 0.6225 - val_accuracy: 0.7926\n",
      "Epoch 150/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4198 - accuracy: 0.8221 - val_loss: 0.6644 - val_accuracy: 0.7911\n",
      "Epoch 151/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4207 - accuracy: 0.8205 - val_loss: 0.6228 - val_accuracy: 0.7951\n",
      "Epoch 152/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4189 - accuracy: 0.8213 - val_loss: 0.8891 - val_accuracy: 0.7909\n",
      "Epoch 153/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4336 - accuracy: 0.8215 - val_loss: 0.6214 - val_accuracy: 0.7931\n",
      "Epoch 154/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4109 - accuracy: 0.8194 - val_loss: 0.6467 - val_accuracy: 0.7898\n",
      "Epoch 155/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4153 - accuracy: 0.8217 - val_loss: 0.7270 - val_accuracy: 0.7925\n",
      "Epoch 156/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4274 - accuracy: 0.8219 - val_loss: 0.7293 - val_accuracy: 0.7929\n",
      "Epoch 157/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4118 - accuracy: 0.8211 - val_loss: 0.7228 - val_accuracy: 0.7939\n",
      "Epoch 158/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4080 - accuracy: 0.8192 - val_loss: 0.6005 - val_accuracy: 0.7886\n",
      "Epoch 159/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4256 - accuracy: 0.8203 - val_loss: 0.6282 - val_accuracy: 0.7960\n",
      "Epoch 160/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4121 - accuracy: 0.8214 - val_loss: 0.7150 - val_accuracy: 0.7942\n",
      "Epoch 161/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4276 - accuracy: 0.8203 - val_loss: 0.7523 - val_accuracy: 0.7903\n",
      "Epoch 162/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4147 - accuracy: 0.8206 - val_loss: 0.6352 - val_accuracy: 0.7908\n",
      "Epoch 163/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4126 - accuracy: 0.8213 - val_loss: 0.6312 - val_accuracy: 0.7920\n",
      "Epoch 164/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4167 - accuracy: 0.8228 - val_loss: 0.7177 - val_accuracy: 0.7887\n",
      "Epoch 165/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4382 - accuracy: 0.8197 - val_loss: 0.7003 - val_accuracy: 0.7953\n",
      "Epoch 166/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4199 - accuracy: 0.8226 - val_loss: 0.6669 - val_accuracy: 0.7948\n",
      "Epoch 167/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4307 - accuracy: 0.8226 - val_loss: 0.6834 - val_accuracy: 0.7904\n",
      "Epoch 168/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4277 - accuracy: 0.8217 - val_loss: 0.7317 - val_accuracy: 0.7939\n",
      "Epoch 169/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4274 - accuracy: 0.8220 - val_loss: 0.7247 - val_accuracy: 0.7904\n",
      "Epoch 170/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4304 - accuracy: 0.8217 - val_loss: 0.7881 - val_accuracy: 0.7860\n",
      "Epoch 171/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4219 - accuracy: 0.8225 - val_loss: 0.6845 - val_accuracy: 0.7956\n",
      "Epoch 172/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4194 - accuracy: 0.8216 - val_loss: 0.7105 - val_accuracy: 0.7979\n",
      "Epoch 173/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4166 - accuracy: 0.8211 - val_loss: 0.7157 - val_accuracy: 0.7899\n",
      "Epoch 174/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4127 - accuracy: 0.8213 - val_loss: 0.7951 - val_accuracy: 0.7981\n",
      "Epoch 175/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4459 - accuracy: 0.8230 - val_loss: 0.7097 - val_accuracy: 0.7921\n",
      "Epoch 176/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4230 - accuracy: 0.8229 - val_loss: 0.5918 - val_accuracy: 0.7896\n",
      "Epoch 177/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4196 - accuracy: 0.8228 - val_loss: 0.6198 - val_accuracy: 0.7917\n",
      "Epoch 178/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4235 - accuracy: 0.8241 - val_loss: 0.8303 - val_accuracy: 0.7848\n",
      "Epoch 179/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4336 - accuracy: 0.8221 - val_loss: 0.7594 - val_accuracy: 0.7911\n",
      "Epoch 180/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4097 - accuracy: 0.8216 - val_loss: 0.8768 - val_accuracy: 0.7939\n",
      "Epoch 181/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4049 - accuracy: 0.8223 - val_loss: 0.7676 - val_accuracy: 0.7961\n",
      "Epoch 182/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4360 - accuracy: 0.8231 - val_loss: 0.7486 - val_accuracy: 0.7898\n",
      "Epoch 183/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4103 - accuracy: 0.8221 - val_loss: 0.8828 - val_accuracy: 0.7920\n",
      "Epoch 184/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4386 - accuracy: 0.8229 - val_loss: 0.7265 - val_accuracy: 0.7890\n",
      "Epoch 185/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4251 - accuracy: 0.8262 - val_loss: 0.7665 - val_accuracy: 0.7912\n",
      "Epoch 186/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4343 - accuracy: 0.8234 - val_loss: 0.7702 - val_accuracy: 0.7907\n",
      "Epoch 187/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4275 - accuracy: 0.8225 - val_loss: 0.7416 - val_accuracy: 0.7896\n",
      "Epoch 188/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4355 - accuracy: 0.8230 - val_loss: 0.8077 - val_accuracy: 0.7925\n",
      "Epoch 189/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4298 - accuracy: 0.8232 - val_loss: 0.7675 - val_accuracy: 0.7929\n",
      "Epoch 190/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4181 - accuracy: 0.8210 - val_loss: 0.8125 - val_accuracy: 0.7861\n",
      "Epoch 191/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4474 - accuracy: 0.8224 - val_loss: 0.8270 - val_accuracy: 0.7935\n",
      "Epoch 192/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4156 - accuracy: 0.8230 - val_loss: 0.9500 - val_accuracy: 0.7986\n",
      "Epoch 193/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4398 - accuracy: 0.8231 - val_loss: 0.7537 - val_accuracy: 0.7926\n",
      "Epoch 194/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4125 - accuracy: 0.8235 - val_loss: 0.9125 - val_accuracy: 0.7966\n",
      "Epoch 195/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4308 - accuracy: 0.8249 - val_loss: 0.8447 - val_accuracy: 0.7881\n",
      "Epoch 196/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4277 - accuracy: 0.8217 - val_loss: 0.9233 - val_accuracy: 0.7873\n",
      "Epoch 197/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4307 - accuracy: 0.8238 - val_loss: 0.8379 - val_accuracy: 0.7964\n",
      "Epoch 198/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4170 - accuracy: 0.8239 - val_loss: 0.7785 - val_accuracy: 0.7976\n",
      "Epoch 199/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4182 - accuracy: 0.8218 - val_loss: 0.9739 - val_accuracy: 0.7986\n",
      "Epoch 200/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4099 - accuracy: 0.8229 - val_loss: 0.8066 - val_accuracy: 0.7909\n",
      "Epoch 201/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4300 - accuracy: 0.8261 - val_loss: 0.7441 - val_accuracy: 0.7977\n",
      "Epoch 202/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4194 - accuracy: 0.8254 - val_loss: 0.7369 - val_accuracy: 0.7942\n",
      "Epoch 203/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4195 - accuracy: 0.8239 - val_loss: 0.8074 - val_accuracy: 0.7942\n",
      "Epoch 204/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4410 - accuracy: 0.8243 - val_loss: 0.6903 - val_accuracy: 0.7926\n",
      "Epoch 205/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4095 - accuracy: 0.8241 - val_loss: 0.9792 - val_accuracy: 0.7933\n",
      "Epoch 206/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4293 - accuracy: 0.8249 - val_loss: 0.7073 - val_accuracy: 0.7965\n",
      "Epoch 207/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4353 - accuracy: 0.8238 - val_loss: 0.7030 - val_accuracy: 0.7931\n",
      "Epoch 208/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4070 - accuracy: 0.8235 - val_loss: 0.7520 - val_accuracy: 0.7916\n",
      "Epoch 209/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4464 - accuracy: 0.8245 - val_loss: 0.8717 - val_accuracy: 0.7927\n",
      "Epoch 210/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4286 - accuracy: 0.8242 - val_loss: 0.8632 - val_accuracy: 0.7844\n",
      "Epoch 211/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4292 - accuracy: 0.8247 - val_loss: 0.9829 - val_accuracy: 0.7985\n",
      "Epoch 212/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3998 - accuracy: 0.8254 - val_loss: 0.8189 - val_accuracy: 0.7935\n",
      "Epoch 213/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4237 - accuracy: 0.8255 - val_loss: 0.9871 - val_accuracy: 0.7908\n",
      "Epoch 214/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4494 - accuracy: 0.8239 - val_loss: 0.8929 - val_accuracy: 0.7917\n",
      "Epoch 215/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4221 - accuracy: 0.8242 - val_loss: 0.8797 - val_accuracy: 0.7989\n",
      "Epoch 216/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4238 - accuracy: 0.8269 - val_loss: 0.9137 - val_accuracy: 0.7970\n",
      "Epoch 217/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4145 - accuracy: 0.8240 - val_loss: 0.8911 - val_accuracy: 0.7921\n",
      "Epoch 218/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4328 - accuracy: 0.8256 - val_loss: 0.8799 - val_accuracy: 0.7969\n",
      "Epoch 219/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4211 - accuracy: 0.8250 - val_loss: 0.7823 - val_accuracy: 0.7902\n",
      "Epoch 220/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4359 - accuracy: 0.8237 - val_loss: 0.8544 - val_accuracy: 0.7933\n",
      "Epoch 221/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4019 - accuracy: 0.8275 - val_loss: 0.8135 - val_accuracy: 0.7900\n",
      "Epoch 222/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4338 - accuracy: 0.8257 - val_loss: 0.9094 - val_accuracy: 0.7866\n",
      "Epoch 223/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4316 - accuracy: 0.8254 - val_loss: 0.8787 - val_accuracy: 0.7915\n",
      "Epoch 224/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4262 - accuracy: 0.8271 - val_loss: 0.9170 - val_accuracy: 0.7957\n",
      "Epoch 225/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4479 - accuracy: 0.8265 - val_loss: 0.8550 - val_accuracy: 0.7924\n",
      "Epoch 226/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4470 - accuracy: 0.8260 - val_loss: 0.8072 - val_accuracy: 0.7911\n",
      "Epoch 227/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4159 - accuracy: 0.8255 - val_loss: 1.0457 - val_accuracy: 0.7924\n",
      "Epoch 228/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4349 - accuracy: 0.8262 - val_loss: 0.7762 - val_accuracy: 0.7900\n",
      "Epoch 229/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4477 - accuracy: 0.8254 - val_loss: 0.9459 - val_accuracy: 0.7911\n",
      "Epoch 230/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4287 - accuracy: 0.8273 - val_loss: 0.8516 - val_accuracy: 0.7909\n",
      "Epoch 231/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4088 - accuracy: 0.8277 - val_loss: 0.9110 - val_accuracy: 0.7930\n",
      "Epoch 232/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4243 - accuracy: 0.8269 - val_loss: 1.0863 - val_accuracy: 0.7956\n",
      "Epoch 233/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4328 - accuracy: 0.8253 - val_loss: 0.9186 - val_accuracy: 0.7869\n",
      "Epoch 234/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4349 - accuracy: 0.8266 - val_loss: 0.9573 - val_accuracy: 0.7916\n",
      "Epoch 235/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4561 - accuracy: 0.8244 - val_loss: 0.9230 - val_accuracy: 0.7857\n",
      "Epoch 236/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4508 - accuracy: 0.8250 - val_loss: 0.9092 - val_accuracy: 0.7882\n",
      "Epoch 237/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4184 - accuracy: 0.8260 - val_loss: 0.8234 - val_accuracy: 0.7913\n",
      "Epoch 238/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4377 - accuracy: 0.8271 - val_loss: 0.8777 - val_accuracy: 0.7890\n",
      "Epoch 239/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4566 - accuracy: 0.8256 - val_loss: 0.7992 - val_accuracy: 0.7952\n",
      "Epoch 240/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4567 - accuracy: 0.8265 - val_loss: 1.2664 - val_accuracy: 0.7887\n",
      "Epoch 241/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4431 - accuracy: 0.8269 - val_loss: 0.9237 - val_accuracy: 0.7939\n",
      "Epoch 242/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4387 - accuracy: 0.8260 - val_loss: 0.8549 - val_accuracy: 0.7887\n",
      "Epoch 243/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4342 - accuracy: 0.8261 - val_loss: 1.2601 - val_accuracy: 0.7927\n",
      "Epoch 244/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4339 - accuracy: 0.8270 - val_loss: 0.9254 - val_accuracy: 0.7918\n",
      "Epoch 245/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4588 - accuracy: 0.8272 - val_loss: 0.8575 - val_accuracy: 0.7944\n",
      "Epoch 246/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4128 - accuracy: 0.8255 - val_loss: 1.2118 - val_accuracy: 0.7957\n",
      "Epoch 247/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4279 - accuracy: 0.8268 - val_loss: 0.8566 - val_accuracy: 0.7907\n",
      "Epoch 248/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4322 - accuracy: 0.8288 - val_loss: 0.8385 - val_accuracy: 0.7883\n",
      "Epoch 249/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4333 - accuracy: 0.8288 - val_loss: 0.8849 - val_accuracy: 0.7902\n",
      "Epoch 250/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4266 - accuracy: 0.8264 - val_loss: 0.8999 - val_accuracy: 0.7900\n",
      "Epoch 251/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4644 - accuracy: 0.8265 - val_loss: 0.8918 - val_accuracy: 0.7948\n",
      "Epoch 252/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4360 - accuracy: 0.8290 - val_loss: 1.0615 - val_accuracy: 0.7924\n",
      "Epoch 253/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4477 - accuracy: 0.8293 - val_loss: 1.1022 - val_accuracy: 0.7925\n",
      "Epoch 254/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4191 - accuracy: 0.8277 - val_loss: 0.9684 - val_accuracy: 0.7942\n",
      "Epoch 255/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4278 - accuracy: 0.8278 - val_loss: 1.0729 - val_accuracy: 0.7912\n",
      "Epoch 256/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4375 - accuracy: 0.8277 - val_loss: 1.2873 - val_accuracy: 0.7911\n",
      "Epoch 257/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4390 - accuracy: 0.8270 - val_loss: 1.1321 - val_accuracy: 0.7909\n",
      "Epoch 258/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4341 - accuracy: 0.8290 - val_loss: 1.7203 - val_accuracy: 0.7930\n",
      "Epoch 259/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4636 - accuracy: 0.8277 - val_loss: 1.1601 - val_accuracy: 0.7959\n",
      "Epoch 260/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4409 - accuracy: 0.8293 - val_loss: 0.9103 - val_accuracy: 0.7905\n",
      "Epoch 261/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4151 - accuracy: 0.8285 - val_loss: 0.8180 - val_accuracy: 0.7934\n",
      "Epoch 262/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4372 - accuracy: 0.8258 - val_loss: 1.1544 - val_accuracy: 0.7959\n",
      "Epoch 263/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4236 - accuracy: 0.8286 - val_loss: 0.8540 - val_accuracy: 0.7933\n",
      "Epoch 264/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4670 - accuracy: 0.8289 - val_loss: 1.0774 - val_accuracy: 0.7933\n",
      "Epoch 265/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4151 - accuracy: 0.8286 - val_loss: 1.0143 - val_accuracy: 0.7843\n",
      "Epoch 266/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4609 - accuracy: 0.8274 - val_loss: 1.0635 - val_accuracy: 0.7915\n",
      "Epoch 267/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4215 - accuracy: 0.8296 - val_loss: 0.9539 - val_accuracy: 0.7885\n",
      "Epoch 268/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4426 - accuracy: 0.8282 - val_loss: 1.3765 - val_accuracy: 0.7935\n",
      "Epoch 269/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4388 - accuracy: 0.8279 - val_loss: 0.9376 - val_accuracy: 0.7963\n",
      "Epoch 270/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4372 - accuracy: 0.8286 - val_loss: 1.3733 - val_accuracy: 0.7890\n",
      "Epoch 271/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4446 - accuracy: 0.8292 - val_loss: 1.0648 - val_accuracy: 0.7878\n",
      "Epoch 272/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4648 - accuracy: 0.8290 - val_loss: 0.8868 - val_accuracy: 0.7904\n",
      "Epoch 273/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4138 - accuracy: 0.8310 - val_loss: 1.1605 - val_accuracy: 0.7915\n",
      "Epoch 274/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4214 - accuracy: 0.8287 - val_loss: 1.0602 - val_accuracy: 0.7879\n",
      "Epoch 275/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4563 - accuracy: 0.8280 - val_loss: 0.9592 - val_accuracy: 0.7890\n",
      "Epoch 276/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4006 - accuracy: 0.8294 - val_loss: 1.2291 - val_accuracy: 0.7924\n",
      "Epoch 277/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4456 - accuracy: 0.8283 - val_loss: 1.1712 - val_accuracy: 0.7937\n",
      "Epoch 278/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4418 - accuracy: 0.8267 - val_loss: 1.3156 - val_accuracy: 0.7900\n",
      "Epoch 279/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4295 - accuracy: 0.8289 - val_loss: 1.3073 - val_accuracy: 0.7899\n",
      "Epoch 280/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4623 - accuracy: 0.8297 - val_loss: 0.9891 - val_accuracy: 0.7943\n",
      "Epoch 281/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4679 - accuracy: 0.8299 - val_loss: 1.4908 - val_accuracy: 0.7876\n",
      "Epoch 282/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4323 - accuracy: 0.8282 - val_loss: 0.9505 - val_accuracy: 0.7917\n",
      "Epoch 283/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4492 - accuracy: 0.8282 - val_loss: 0.9958 - val_accuracy: 0.7913\n",
      "Epoch 284/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4153 - accuracy: 0.8293 - val_loss: 1.1834 - val_accuracy: 0.7955\n",
      "Epoch 285/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4117 - accuracy: 0.8282 - val_loss: 1.0598 - val_accuracy: 0.7920\n",
      "Epoch 286/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4751 - accuracy: 0.8307 - val_loss: 1.2566 - val_accuracy: 0.7956\n",
      "Epoch 287/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4647 - accuracy: 0.8302 - val_loss: 0.9375 - val_accuracy: 0.7905\n",
      "Epoch 288/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4335 - accuracy: 0.8278 - val_loss: 1.4836 - val_accuracy: 0.7868\n",
      "Epoch 289/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4375 - accuracy: 0.8283 - val_loss: 1.0045 - val_accuracy: 0.7935\n",
      "Epoch 290/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4132 - accuracy: 0.8305 - val_loss: 1.4570 - val_accuracy: 0.7892\n",
      "Epoch 291/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4236 - accuracy: 0.8291 - val_loss: 1.1221 - val_accuracy: 0.7890\n",
      "Epoch 292/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4441 - accuracy: 0.8271 - val_loss: 1.0016 - val_accuracy: 0.7938\n",
      "Epoch 293/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4461 - accuracy: 0.8286 - val_loss: 0.9689 - val_accuracy: 0.7900\n",
      "Epoch 294/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4471 - accuracy: 0.8305 - val_loss: 1.4446 - val_accuracy: 0.7857\n",
      "Epoch 295/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4585 - accuracy: 0.8306 - val_loss: 1.2045 - val_accuracy: 0.7905\n",
      "Epoch 296/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4548 - accuracy: 0.8289 - val_loss: 1.2269 - val_accuracy: 0.7912\n",
      "Epoch 297/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4598 - accuracy: 0.8300 - val_loss: 1.1105 - val_accuracy: 0.7904\n",
      "Epoch 298/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3998 - accuracy: 0.8308 - val_loss: 1.1827 - val_accuracy: 0.7881\n",
      "Epoch 299/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4181 - accuracy: 0.8291 - val_loss: 1.4769 - val_accuracy: 0.7852\n",
      "Epoch 300/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4511 - accuracy: 0.8310 - val_loss: 0.9428 - val_accuracy: 0.7818\n",
      "Epoch 301/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4627 - accuracy: 0.8293 - val_loss: 1.1914 - val_accuracy: 0.7869\n",
      "Epoch 302/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4372 - accuracy: 0.8301 - val_loss: 1.0028 - val_accuracy: 0.7903\n",
      "Epoch 303/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4602 - accuracy: 0.8308 - val_loss: 1.0959 - val_accuracy: 0.7902\n",
      "Epoch 304/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4747 - accuracy: 0.8290 - val_loss: 1.4376 - val_accuracy: 0.7872\n",
      "Epoch 305/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4290 - accuracy: 0.8310 - val_loss: 1.1321 - val_accuracy: 0.7881\n",
      "Epoch 306/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4430 - accuracy: 0.8297 - val_loss: 1.0071 - val_accuracy: 0.7850\n",
      "Epoch 307/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4346 - accuracy: 0.8302 - val_loss: 1.6153 - val_accuracy: 0.7889\n",
      "Epoch 308/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4703 - accuracy: 0.8290 - val_loss: 1.2576 - val_accuracy: 0.7887\n",
      "Epoch 309/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4649 - accuracy: 0.8307 - val_loss: 1.5244 - val_accuracy: 0.7896\n",
      "Epoch 310/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4781 - accuracy: 0.8323 - val_loss: 1.1661 - val_accuracy: 0.7892\n",
      "Epoch 311/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4380 - accuracy: 0.8305 - val_loss: 1.2737 - val_accuracy: 0.7864\n",
      "Epoch 312/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4372 - accuracy: 0.8307 - val_loss: 1.0965 - val_accuracy: 0.7895\n",
      "Epoch 313/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4884 - accuracy: 0.8311 - val_loss: 1.2407 - val_accuracy: 0.7903\n",
      "Epoch 314/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4799 - accuracy: 0.8293 - val_loss: 1.5209 - val_accuracy: 0.7900\n",
      "Epoch 315/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4051 - accuracy: 0.8315 - val_loss: 1.5438 - val_accuracy: 0.7874\n",
      "Epoch 316/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4354 - accuracy: 0.8324 - val_loss: 1.1853 - val_accuracy: 0.7917\n",
      "Epoch 317/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4430 - accuracy: 0.8305 - val_loss: 1.0232 - val_accuracy: 0.7927\n",
      "Epoch 318/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4128 - accuracy: 0.8313 - val_loss: 0.9571 - val_accuracy: 0.7886\n",
      "Epoch 319/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4696 - accuracy: 0.8315 - val_loss: 1.0185 - val_accuracy: 0.7885\n",
      "Epoch 320/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4765 - accuracy: 0.8315 - val_loss: 1.6611 - val_accuracy: 0.7886\n",
      "Epoch 321/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4833 - accuracy: 0.8303 - val_loss: 1.3662 - val_accuracy: 0.7944\n",
      "Epoch 322/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4481 - accuracy: 0.8310 - val_loss: 1.1753 - val_accuracy: 0.7889\n",
      "Epoch 323/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5171 - accuracy: 0.8301 - val_loss: 0.8757 - val_accuracy: 0.7882\n",
      "Epoch 324/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4732 - accuracy: 0.8314 - val_loss: 1.3192 - val_accuracy: 0.7874\n",
      "Epoch 325/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4417 - accuracy: 0.8312 - val_loss: 1.2246 - val_accuracy: 0.7885\n",
      "Epoch 326/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4142 - accuracy: 0.8312 - val_loss: 0.9871 - val_accuracy: 0.7934\n",
      "Epoch 327/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4805 - accuracy: 0.8322 - val_loss: 1.2423 - val_accuracy: 0.7856\n",
      "Epoch 328/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4336 - accuracy: 0.8325 - val_loss: 1.2728 - val_accuracy: 0.7927\n",
      "Epoch 329/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4841 - accuracy: 0.8328 - val_loss: 1.4341 - val_accuracy: 0.7935\n",
      "Epoch 330/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4473 - accuracy: 0.8319 - val_loss: 1.0920 - val_accuracy: 0.7911\n",
      "Epoch 331/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4979 - accuracy: 0.8318 - val_loss: 1.0388 - val_accuracy: 0.7887\n",
      "Epoch 332/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4058 - accuracy: 0.8312 - val_loss: 1.0150 - val_accuracy: 0.7902\n",
      "Epoch 333/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4709 - accuracy: 0.8308 - val_loss: 1.3609 - val_accuracy: 0.7840\n",
      "Epoch 334/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4765 - accuracy: 0.8332 - val_loss: 1.1752 - val_accuracy: 0.7879\n",
      "Epoch 335/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3997 - accuracy: 0.8312 - val_loss: 1.0109 - val_accuracy: 0.7856\n",
      "Epoch 336/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4122 - accuracy: 0.8324 - val_loss: 1.8005 - val_accuracy: 0.7933\n",
      "Epoch 337/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4153 - accuracy: 0.8339 - val_loss: 2.0649 - val_accuracy: 0.7931\n",
      "Epoch 338/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4538 - accuracy: 0.8330 - val_loss: 1.1563 - val_accuracy: 0.7876\n",
      "Epoch 339/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4327 - accuracy: 0.8323 - val_loss: 1.4234 - val_accuracy: 0.7864\n",
      "Epoch 340/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4777 - accuracy: 0.8306 - val_loss: 1.1414 - val_accuracy: 0.7879\n",
      "Epoch 341/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3961 - accuracy: 0.8328 - val_loss: 0.9943 - val_accuracy: 0.7876\n",
      "Epoch 342/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5161 - accuracy: 0.8319 - val_loss: 1.1074 - val_accuracy: 0.7864\n",
      "Epoch 343/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4387 - accuracy: 0.8325 - val_loss: 1.8024 - val_accuracy: 0.7898\n",
      "Epoch 344/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5379 - accuracy: 0.8317 - val_loss: 1.1470 - val_accuracy: 0.7902\n",
      "Epoch 345/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4665 - accuracy: 0.8335 - val_loss: 1.0954 - val_accuracy: 0.7857\n",
      "Epoch 346/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4176 - accuracy: 0.8308 - val_loss: 1.1096 - val_accuracy: 0.7900\n",
      "Epoch 347/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4787 - accuracy: 0.8319 - val_loss: 1.8651 - val_accuracy: 0.7840\n",
      "Epoch 348/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4639 - accuracy: 0.8314 - val_loss: 1.0629 - val_accuracy: 0.7809\n",
      "Epoch 349/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4616 - accuracy: 0.8321 - val_loss: 2.0503 - val_accuracy: 0.7917\n",
      "Epoch 350/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4393 - accuracy: 0.8337 - val_loss: 0.9780 - val_accuracy: 0.7895\n",
      "Epoch 351/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4210 - accuracy: 0.8346 - val_loss: 1.0863 - val_accuracy: 0.7868\n",
      "Epoch 352/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5156 - accuracy: 0.8310 - val_loss: 1.8129 - val_accuracy: 0.7864\n",
      "Epoch 353/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4972 - accuracy: 0.8328 - val_loss: 1.3062 - val_accuracy: 0.7878\n",
      "Epoch 354/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5403 - accuracy: 0.8321 - val_loss: 1.2410 - val_accuracy: 0.7778\n",
      "Epoch 355/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4423 - accuracy: 0.8332 - val_loss: 1.9047 - val_accuracy: 0.7859\n",
      "Epoch 356/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4583 - accuracy: 0.8329 - val_loss: 1.1134 - val_accuracy: 0.7907\n",
      "Epoch 357/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4235 - accuracy: 0.8327 - val_loss: 1.0993 - val_accuracy: 0.7887\n",
      "Epoch 358/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4035 - accuracy: 0.8344 - val_loss: 1.2903 - val_accuracy: 0.7890\n",
      "Epoch 359/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4564 - accuracy: 0.8331 - val_loss: 1.0632 - val_accuracy: 0.7924\n",
      "Epoch 360/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5335 - accuracy: 0.8325 - val_loss: 1.1295 - val_accuracy: 0.7911\n",
      "Epoch 361/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5240 - accuracy: 0.8309 - val_loss: 1.0162 - val_accuracy: 0.7861\n",
      "Epoch 362/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5617 - accuracy: 0.8320 - val_loss: 1.4624 - val_accuracy: 0.7930\n",
      "Epoch 363/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5040 - accuracy: 0.8333 - val_loss: 0.9490 - val_accuracy: 0.7904\n",
      "Epoch 364/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4296 - accuracy: 0.8332 - val_loss: 1.4136 - val_accuracy: 0.7920\n",
      "Epoch 365/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4310 - accuracy: 0.8312 - val_loss: 0.9867 - val_accuracy: 0.7881\n",
      "Epoch 366/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3971 - accuracy: 0.8332 - val_loss: 1.1134 - val_accuracy: 0.7878\n",
      "Epoch 367/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4702 - accuracy: 0.8347 - val_loss: 1.0897 - val_accuracy: 0.7872\n",
      "Epoch 368/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4165 - accuracy: 0.8311 - val_loss: 1.5948 - val_accuracy: 0.7882\n",
      "Epoch 369/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4473 - accuracy: 0.8348 - val_loss: 1.4281 - val_accuracy: 0.7890\n",
      "Epoch 370/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4026 - accuracy: 0.8331 - val_loss: 1.7204 - val_accuracy: 0.7869\n",
      "Epoch 371/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4136 - accuracy: 0.8328 - val_loss: 1.1906 - val_accuracy: 0.7891\n",
      "Epoch 372/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5233 - accuracy: 0.8344 - val_loss: 0.9941 - val_accuracy: 0.7902\n",
      "Epoch 373/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4339 - accuracy: 0.8336 - val_loss: 2.1091 - val_accuracy: 0.7855\n",
      "Epoch 374/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5316 - accuracy: 0.8322 - val_loss: 1.4275 - val_accuracy: 0.7882\n",
      "Epoch 375/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4502 - accuracy: 0.8332 - val_loss: 1.8234 - val_accuracy: 0.7885\n",
      "Epoch 376/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4241 - accuracy: 0.8329 - val_loss: 2.0791 - val_accuracy: 0.7913\n",
      "Epoch 377/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5411 - accuracy: 0.8351 - val_loss: 1.7304 - val_accuracy: 0.7894\n",
      "Epoch 378/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4520 - accuracy: 0.8341 - val_loss: 1.6373 - val_accuracy: 0.7843\n",
      "Epoch 379/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5266 - accuracy: 0.8351 - val_loss: 1.0670 - val_accuracy: 0.7861\n",
      "Epoch 380/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4675 - accuracy: 0.8322 - val_loss: 1.9566 - val_accuracy: 0.7896\n",
      "Epoch 381/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5284 - accuracy: 0.8318 - val_loss: 1.8259 - val_accuracy: 0.7859\n",
      "Epoch 382/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4855 - accuracy: 0.8335 - val_loss: 1.1070 - val_accuracy: 0.7918\n",
      "Epoch 383/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3927 - accuracy: 0.8332 - val_loss: 2.3771 - val_accuracy: 0.7920\n",
      "Epoch 384/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4748 - accuracy: 0.8350 - val_loss: 1.1242 - val_accuracy: 0.7834\n",
      "Epoch 385/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5241 - accuracy: 0.8334 - val_loss: 1.2840 - val_accuracy: 0.7874\n",
      "Epoch 386/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4643 - accuracy: 0.8339 - val_loss: 1.6510 - val_accuracy: 0.7909\n",
      "Epoch 387/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5111 - accuracy: 0.8349 - val_loss: 2.2663 - val_accuracy: 0.7873\n",
      "Epoch 388/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4872 - accuracy: 0.8318 - val_loss: 1.1300 - val_accuracy: 0.7904\n",
      "Epoch 389/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4828 - accuracy: 0.8353 - val_loss: 1.3509 - val_accuracy: 0.7864\n",
      "Epoch 390/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5888 - accuracy: 0.8348 - val_loss: 1.9429 - val_accuracy: 0.7873\n",
      "Epoch 391/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4824 - accuracy: 0.8352 - val_loss: 1.1426 - val_accuracy: 0.7911\n",
      "Epoch 392/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5218 - accuracy: 0.8346 - val_loss: 1.1795 - val_accuracy: 0.7903\n",
      "Epoch 393/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5316 - accuracy: 0.8326 - val_loss: 1.8065 - val_accuracy: 0.7877\n",
      "Epoch 394/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4912 - accuracy: 0.8326 - val_loss: 1.1236 - val_accuracy: 0.7943\n",
      "Epoch 395/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5078 - accuracy: 0.8325 - val_loss: 1.1980 - val_accuracy: 0.7907\n",
      "Epoch 396/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4155 - accuracy: 0.8327 - val_loss: 1.6128 - val_accuracy: 0.7887\n",
      "Epoch 397/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4824 - accuracy: 0.8344 - val_loss: 1.5447 - val_accuracy: 0.7873\n",
      "Epoch 398/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4451 - accuracy: 0.8344 - val_loss: 1.6824 - val_accuracy: 0.7925\n",
      "Epoch 399/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5146 - accuracy: 0.8329 - val_loss: 1.5168 - val_accuracy: 0.7846\n",
      "Epoch 400/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5411 - accuracy: 0.8327 - val_loss: 1.5652 - val_accuracy: 0.7834\n",
      "Epoch 401/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4951 - accuracy: 0.8345 - val_loss: 1.2664 - val_accuracy: 0.7924\n",
      "Epoch 402/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4533 - accuracy: 0.8346 - val_loss: 1.5963 - val_accuracy: 0.7892\n",
      "Epoch 403/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5135 - accuracy: 0.8351 - val_loss: 1.5069 - val_accuracy: 0.7851\n",
      "Epoch 404/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5274 - accuracy: 0.8344 - val_loss: 1.1537 - val_accuracy: 0.7940\n",
      "Epoch 405/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4589 - accuracy: 0.8344 - val_loss: 1.0008 - val_accuracy: 0.7908\n",
      "Epoch 406/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5464 - accuracy: 0.8332 - val_loss: 1.3583 - val_accuracy: 0.7894\n",
      "Epoch 407/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4090 - accuracy: 0.8350 - val_loss: 1.5022 - val_accuracy: 0.7937\n",
      "Epoch 408/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5025 - accuracy: 0.8358 - val_loss: 1.2926 - val_accuracy: 0.7903\n",
      "Epoch 409/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4540 - accuracy: 0.8335 - val_loss: 1.7213 - val_accuracy: 0.7835\n",
      "Epoch 410/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5413 - accuracy: 0.8338 - val_loss: 1.4238 - val_accuracy: 0.7899\n",
      "Epoch 411/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4218 - accuracy: 0.8341 - val_loss: 1.6775 - val_accuracy: 0.7931\n",
      "Epoch 412/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4348 - accuracy: 0.8350 - val_loss: 1.7003 - val_accuracy: 0.7891\n",
      "Epoch 413/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4345 - accuracy: 0.8341 - val_loss: 1.5873 - val_accuracy: 0.7926\n",
      "Epoch 414/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.6093 - accuracy: 0.8349 - val_loss: 1.1999 - val_accuracy: 0.7935\n",
      "Epoch 415/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4793 - accuracy: 0.8332 - val_loss: 1.0701 - val_accuracy: 0.7940\n",
      "Epoch 416/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4959 - accuracy: 0.8349 - val_loss: 1.3823 - val_accuracy: 0.7856\n",
      "Epoch 417/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5068 - accuracy: 0.8366 - val_loss: 1.4424 - val_accuracy: 0.7918\n",
      "Epoch 418/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3917 - accuracy: 0.8344 - val_loss: 2.0198 - val_accuracy: 0.7873\n",
      "Epoch 419/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4066 - accuracy: 0.8354 - val_loss: 1.3948 - val_accuracy: 0.7904\n",
      "Epoch 420/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5190 - accuracy: 0.8343 - val_loss: 1.6183 - val_accuracy: 0.7878\n",
      "Epoch 421/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4089 - accuracy: 0.8352 - val_loss: 1.9112 - val_accuracy: 0.7835\n",
      "Epoch 422/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5548 - accuracy: 0.8363 - val_loss: 1.2158 - val_accuracy: 0.7859\n",
      "Epoch 423/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4399 - accuracy: 0.8335 - val_loss: 1.5830 - val_accuracy: 0.7937\n",
      "Epoch 424/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4928 - accuracy: 0.8338 - val_loss: 1.2650 - val_accuracy: 0.7947\n",
      "Epoch 425/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5264 - accuracy: 0.8359 - val_loss: 1.3702 - val_accuracy: 0.7843\n",
      "Epoch 426/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4374 - accuracy: 0.8342 - val_loss: 1.0502 - val_accuracy: 0.7944\n",
      "Epoch 427/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5512 - accuracy: 0.8355 - val_loss: 1.3765 - val_accuracy: 0.7876\n",
      "Epoch 428/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5410 - accuracy: 0.8345 - val_loss: 1.8390 - val_accuracy: 0.7890\n",
      "Epoch 429/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4701 - accuracy: 0.8357 - val_loss: 1.6079 - val_accuracy: 0.7837\n",
      "Epoch 430/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5096 - accuracy: 0.8358 - val_loss: 1.4112 - val_accuracy: 0.7960\n",
      "Epoch 431/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4242 - accuracy: 0.8338 - val_loss: 1.3069 - val_accuracy: 0.7874\n",
      "Epoch 432/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4471 - accuracy: 0.8339 - val_loss: 2.5585 - val_accuracy: 0.7769\n",
      "Epoch 433/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5959 - accuracy: 0.8344 - val_loss: 1.4475 - val_accuracy: 0.7852\n",
      "Epoch 434/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5689 - accuracy: 0.8367 - val_loss: 1.3934 - val_accuracy: 0.7899\n",
      "Epoch 435/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4246 - accuracy: 0.8354 - val_loss: 1.6518 - val_accuracy: 0.7892\n",
      "Epoch 436/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5632 - accuracy: 0.8344 - val_loss: 1.2340 - val_accuracy: 0.7882\n",
      "Epoch 437/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4800 - accuracy: 0.8345 - val_loss: 1.5961 - val_accuracy: 0.7864\n",
      "Epoch 438/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5597 - accuracy: 0.8336 - val_loss: 2.0832 - val_accuracy: 0.7840\n",
      "Epoch 439/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5468 - accuracy: 0.8338 - val_loss: 2.4155 - val_accuracy: 0.7800\n",
      "Epoch 440/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4143 - accuracy: 0.8343 - val_loss: 2.2884 - val_accuracy: 0.7820\n",
      "Epoch 441/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4524 - accuracy: 0.8356 - val_loss: 2.9064 - val_accuracy: 0.7931\n",
      "Epoch 442/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5900 - accuracy: 0.8362 - val_loss: 1.9300 - val_accuracy: 0.7933\n",
      "Epoch 443/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5032 - accuracy: 0.8352 - val_loss: 1.3578 - val_accuracy: 0.7856\n",
      "Epoch 444/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.6067 - accuracy: 0.8351 - val_loss: 1.1750 - val_accuracy: 0.7840\n",
      "Epoch 445/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4338 - accuracy: 0.8354 - val_loss: 2.3960 - val_accuracy: 0.7872\n",
      "Epoch 446/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5086 - accuracy: 0.8343 - val_loss: 1.0779 - val_accuracy: 0.7886\n",
      "Epoch 447/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5196 - accuracy: 0.8353 - val_loss: 1.1702 - val_accuracy: 0.7922\n",
      "Epoch 448/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4981 - accuracy: 0.8341 - val_loss: 2.1868 - val_accuracy: 0.7869\n",
      "Epoch 449/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5681 - accuracy: 0.8370 - val_loss: 2.3862 - val_accuracy: 0.7872\n",
      "Epoch 450/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4283 - accuracy: 0.8345 - val_loss: 1.4835 - val_accuracy: 0.7917\n",
      "Epoch 451/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4354 - accuracy: 0.8344 - val_loss: 1.3641 - val_accuracy: 0.7918\n",
      "Epoch 452/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4238 - accuracy: 0.8340 - val_loss: 2.6551 - val_accuracy: 0.7931\n",
      "Epoch 453/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4869 - accuracy: 0.8344 - val_loss: 1.5833 - val_accuracy: 0.7908\n",
      "Epoch 454/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5306 - accuracy: 0.8353 - val_loss: 1.2659 - val_accuracy: 0.7951\n",
      "Epoch 455/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4162 - accuracy: 0.8332 - val_loss: 2.8204 - val_accuracy: 0.7764\n",
      "Epoch 456/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5408 - accuracy: 0.8361 - val_loss: 1.2875 - val_accuracy: 0.7878\n",
      "Epoch 457/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4313 - accuracy: 0.8360 - val_loss: 1.4205 - val_accuracy: 0.7811\n",
      "Epoch 458/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5206 - accuracy: 0.8342 - val_loss: 1.3547 - val_accuracy: 0.7853\n",
      "Epoch 459/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4059 - accuracy: 0.8340 - val_loss: 1.6794 - val_accuracy: 0.7907\n",
      "Epoch 460/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5065 - accuracy: 0.8361 - val_loss: 1.6645 - val_accuracy: 0.7895\n",
      "Epoch 461/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4641 - accuracy: 0.8356 - val_loss: 1.5340 - val_accuracy: 0.7895\n",
      "Epoch 462/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5180 - accuracy: 0.8336 - val_loss: 1.4645 - val_accuracy: 0.7930\n",
      "Epoch 463/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5653 - accuracy: 0.8369 - val_loss: 1.4878 - val_accuracy: 0.7878\n",
      "Epoch 464/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5390 - accuracy: 0.8357 - val_loss: 1.4098 - val_accuracy: 0.7872\n",
      "Epoch 465/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4364 - accuracy: 0.8372 - val_loss: 1.2305 - val_accuracy: 0.7890\n",
      "Epoch 466/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5156 - accuracy: 0.8347 - val_loss: 1.3604 - val_accuracy: 0.7853\n",
      "Epoch 467/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4616 - accuracy: 0.8328 - val_loss: 1.4526 - val_accuracy: 0.7865\n",
      "Epoch 468/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4519 - accuracy: 0.8328 - val_loss: 1.3122 - val_accuracy: 0.7851\n",
      "Epoch 469/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4124 - accuracy: 0.8343 - val_loss: 1.2415 - val_accuracy: 0.7942\n",
      "Epoch 470/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4348 - accuracy: 0.8354 - val_loss: 1.3963 - val_accuracy: 0.7883\n",
      "Epoch 471/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5403 - accuracy: 0.8360 - val_loss: 1.4689 - val_accuracy: 0.7876\n",
      "Epoch 472/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5724 - accuracy: 0.8334 - val_loss: 2.3271 - val_accuracy: 0.7866\n",
      "Epoch 473/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5427 - accuracy: 0.8359 - val_loss: 1.3895 - val_accuracy: 0.7835\n",
      "Epoch 474/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5915 - accuracy: 0.8362 - val_loss: 2.8789 - val_accuracy: 0.7866\n",
      "Epoch 475/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4637 - accuracy: 0.8353 - val_loss: 1.6224 - val_accuracy: 0.7874\n",
      "Epoch 476/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5387 - accuracy: 0.8355 - val_loss: 2.6447 - val_accuracy: 0.7786\n",
      "Epoch 477/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5301 - accuracy: 0.8368 - val_loss: 1.1859 - val_accuracy: 0.7894\n",
      "Epoch 478/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5953 - accuracy: 0.8353 - val_loss: 1.4992 - val_accuracy: 0.7831\n",
      "Epoch 479/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4753 - accuracy: 0.8354 - val_loss: 1.3575 - val_accuracy: 0.7952\n",
      "Epoch 480/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5157 - accuracy: 0.8341 - val_loss: 1.2721 - val_accuracy: 0.7874\n",
      "Epoch 481/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5054 - accuracy: 0.8369 - val_loss: 1.8012 - val_accuracy: 0.7856\n",
      "Epoch 482/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4572 - accuracy: 0.8342 - val_loss: 1.2992 - val_accuracy: 0.7894\n",
      "Epoch 483/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4845 - accuracy: 0.8360 - val_loss: 1.1764 - val_accuracy: 0.7890\n",
      "Epoch 484/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.6392 - accuracy: 0.8341 - val_loss: 1.1549 - val_accuracy: 0.7827\n",
      "Epoch 485/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.6450 - accuracy: 0.8356 - val_loss: 1.2642 - val_accuracy: 0.7876\n",
      "Epoch 486/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4266 - accuracy: 0.8367 - val_loss: 1.8434 - val_accuracy: 0.7835\n",
      "Epoch 487/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5361 - accuracy: 0.8360 - val_loss: 1.4647 - val_accuracy: 0.7886\n",
      "Epoch 488/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5032 - accuracy: 0.8352 - val_loss: 1.9279 - val_accuracy: 0.7903\n",
      "Epoch 489/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4724 - accuracy: 0.8350 - val_loss: 1.5442 - val_accuracy: 0.7905\n",
      "Epoch 490/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5703 - accuracy: 0.8342 - val_loss: 1.7179 - val_accuracy: 0.7855\n",
      "Epoch 491/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5878 - accuracy: 0.8355 - val_loss: 1.8282 - val_accuracy: 0.7881\n",
      "Epoch 492/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.8357 - val_loss: 2.6341 - val_accuracy: 0.7930\n",
      "Epoch 493/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5697 - accuracy: 0.8350 - val_loss: 1.5299 - val_accuracy: 0.7855\n",
      "Epoch 494/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4247 - accuracy: 0.8351 - val_loss: 1.6782 - val_accuracy: 0.7881\n",
      "Epoch 495/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.6062 - accuracy: 0.8341 - val_loss: 1.9805 - val_accuracy: 0.7903\n",
      "Epoch 496/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5348 - accuracy: 0.8351 - val_loss: 1.6903 - val_accuracy: 0.7838\n",
      "Epoch 497/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.6423 - accuracy: 0.8343 - val_loss: 1.3426 - val_accuracy: 0.7887\n",
      "Epoch 498/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5152 - accuracy: 0.8362 - val_loss: 2.2751 - val_accuracy: 0.7935\n",
      "Epoch 499/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4396 - accuracy: 0.8380 - val_loss: 1.8502 - val_accuracy: 0.7883\n",
      "Epoch 500/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4121 - accuracy: 0.8363 - val_loss: 1.2183 - val_accuracy: 0.7760\n",
      "241/241 [==============================] - 0s 1ms/step - loss: 1.2183 - accuracy: 0.7760\n",
      "\n",
      "--------------\n",
      " [1.218286156654358, 0.7759875059127808]\n",
      "241/241 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.775987525987526\n",
      "Recall: 0.705989110707804\n",
      "Precision: 0.8219136734077875\n",
      "F1: 0.7595536959553696\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      " elu \n",
      "\n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 200)               2400      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 150)               30150     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 50)                7550      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,151\n",
      "Trainable params: 40,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n",
      "962/962 [==============================] - 3s 2ms/step - loss: 0.5265 - accuracy: 0.7359 - val_loss: 0.5241 - val_accuracy: 0.7418\n",
      "Epoch 2/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5090 - accuracy: 0.7489 - val_loss: 0.5265 - val_accuracy: 0.7241\n",
      "Epoch 3/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.5016 - accuracy: 0.7535 - val_loss: 0.5004 - val_accuracy: 0.7429\n",
      "Epoch 4/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4967 - accuracy: 0.7560 - val_loss: 0.4971 - val_accuracy: 0.7492\n",
      "Epoch 5/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4909 - accuracy: 0.7601 - val_loss: 0.4960 - val_accuracy: 0.7588\n",
      "Epoch 6/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4879 - accuracy: 0.7629 - val_loss: 0.4899 - val_accuracy: 0.7555\n",
      "Epoch 7/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4841 - accuracy: 0.7616 - val_loss: 0.4847 - val_accuracy: 0.7582\n",
      "Epoch 8/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4811 - accuracy: 0.7653 - val_loss: 0.4926 - val_accuracy: 0.7599\n",
      "Epoch 9/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4798 - accuracy: 0.7666 - val_loss: 0.4847 - val_accuracy: 0.7646\n",
      "Epoch 10/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4770 - accuracy: 0.7673 - val_loss: 0.4866 - val_accuracy: 0.7583\n",
      "Epoch 11/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4745 - accuracy: 0.7693 - val_loss: 0.4908 - val_accuracy: 0.7553\n",
      "Epoch 12/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4725 - accuracy: 0.7710 - val_loss: 0.4925 - val_accuracy: 0.7596\n",
      "Epoch 13/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4711 - accuracy: 0.7723 - val_loss: 0.4836 - val_accuracy: 0.7679\n",
      "Epoch 14/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4702 - accuracy: 0.7712 - val_loss: 0.4906 - val_accuracy: 0.7587\n",
      "Epoch 15/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4675 - accuracy: 0.7731 - val_loss: 0.4858 - val_accuracy: 0.7556\n",
      "Epoch 16/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4671 - accuracy: 0.7735 - val_loss: 0.4743 - val_accuracy: 0.7655\n",
      "Epoch 17/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4650 - accuracy: 0.7744 - val_loss: 0.4821 - val_accuracy: 0.7665\n",
      "Epoch 18/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4645 - accuracy: 0.7761 - val_loss: 0.4722 - val_accuracy: 0.7733\n",
      "Epoch 19/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4629 - accuracy: 0.7754 - val_loss: 0.4741 - val_accuracy: 0.7699\n",
      "Epoch 20/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4621 - accuracy: 0.7775 - val_loss: 0.4761 - val_accuracy: 0.7709\n",
      "Epoch 21/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4599 - accuracy: 0.7771 - val_loss: 0.4731 - val_accuracy: 0.7683\n",
      "Epoch 22/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4585 - accuracy: 0.7804 - val_loss: 0.4861 - val_accuracy: 0.7642\n",
      "Epoch 23/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4571 - accuracy: 0.7799 - val_loss: 0.4739 - val_accuracy: 0.7710\n",
      "Epoch 24/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4567 - accuracy: 0.7825 - val_loss: 0.4747 - val_accuracy: 0.7710\n",
      "Epoch 25/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4562 - accuracy: 0.7820 - val_loss: 0.4725 - val_accuracy: 0.7746\n",
      "Epoch 26/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4548 - accuracy: 0.7809 - val_loss: 0.4757 - val_accuracy: 0.7664\n",
      "Epoch 27/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4534 - accuracy: 0.7836 - val_loss: 0.4800 - val_accuracy: 0.7622\n",
      "Epoch 28/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4524 - accuracy: 0.7835 - val_loss: 0.4664 - val_accuracy: 0.7783\n",
      "Epoch 29/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4514 - accuracy: 0.7852 - val_loss: 0.4756 - val_accuracy: 0.7692\n",
      "Epoch 30/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4506 - accuracy: 0.7834 - val_loss: 0.4708 - val_accuracy: 0.7736\n",
      "Epoch 31/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4500 - accuracy: 0.7837 - val_loss: 0.4641 - val_accuracy: 0.7762\n",
      "Epoch 32/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4486 - accuracy: 0.7868 - val_loss: 0.4719 - val_accuracy: 0.7753\n",
      "Epoch 33/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4487 - accuracy: 0.7868 - val_loss: 0.4641 - val_accuracy: 0.7816\n",
      "Epoch 34/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4457 - accuracy: 0.7869 - val_loss: 0.4633 - val_accuracy: 0.7817\n",
      "Epoch 35/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4463 - accuracy: 0.7887 - val_loss: 0.4679 - val_accuracy: 0.7748\n",
      "Epoch 36/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4463 - accuracy: 0.7884 - val_loss: 0.4664 - val_accuracy: 0.7756\n",
      "Epoch 37/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4442 - accuracy: 0.7889 - val_loss: 0.4759 - val_accuracy: 0.7672\n",
      "Epoch 38/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4453 - accuracy: 0.7866 - val_loss: 0.4613 - val_accuracy: 0.7817\n",
      "Epoch 39/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4436 - accuracy: 0.7883 - val_loss: 0.4624 - val_accuracy: 0.7782\n",
      "Epoch 40/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4426 - accuracy: 0.7899 - val_loss: 0.4631 - val_accuracy: 0.7778\n",
      "Epoch 41/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4409 - accuracy: 0.7919 - val_loss: 0.4595 - val_accuracy: 0.7837\n",
      "Epoch 42/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4421 - accuracy: 0.7917 - val_loss: 0.4742 - val_accuracy: 0.7756\n",
      "Epoch 43/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4405 - accuracy: 0.7915 - val_loss: 0.4625 - val_accuracy: 0.7779\n",
      "Epoch 44/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4394 - accuracy: 0.7919 - val_loss: 0.4581 - val_accuracy: 0.7807\n",
      "Epoch 45/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4377 - accuracy: 0.7943 - val_loss: 0.4594 - val_accuracy: 0.7812\n",
      "Epoch 46/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4389 - accuracy: 0.7937 - val_loss: 0.4769 - val_accuracy: 0.7749\n",
      "Epoch 47/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4373 - accuracy: 0.7930 - val_loss: 0.4605 - val_accuracy: 0.7800\n",
      "Epoch 48/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4368 - accuracy: 0.7932 - val_loss: 0.4617 - val_accuracy: 0.7787\n",
      "Epoch 49/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4355 - accuracy: 0.7951 - val_loss: 0.4666 - val_accuracy: 0.7829\n",
      "Epoch 50/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4346 - accuracy: 0.7964 - val_loss: 0.4702 - val_accuracy: 0.7736\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4344 - accuracy: 0.7962 - val_loss: 0.4677 - val_accuracy: 0.7756\n",
      "Epoch 52/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4332 - accuracy: 0.7948 - val_loss: 0.4639 - val_accuracy: 0.7822\n",
      "Epoch 53/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4326 - accuracy: 0.7956 - val_loss: 0.4585 - val_accuracy: 0.7864\n",
      "Epoch 54/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4324 - accuracy: 0.7959 - val_loss: 0.4580 - val_accuracy: 0.7850\n",
      "Epoch 55/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4314 - accuracy: 0.7943 - val_loss: 0.4597 - val_accuracy: 0.7869\n",
      "Epoch 56/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4309 - accuracy: 0.7954 - val_loss: 0.4518 - val_accuracy: 0.7869\n",
      "Epoch 57/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4302 - accuracy: 0.7965 - val_loss: 0.4606 - val_accuracy: 0.7839\n",
      "Epoch 58/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4298 - accuracy: 0.7976 - val_loss: 0.4545 - val_accuracy: 0.7865\n",
      "Epoch 59/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4281 - accuracy: 0.7988 - val_loss: 0.4558 - val_accuracy: 0.7863\n",
      "Epoch 60/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4270 - accuracy: 0.7991 - val_loss: 0.4519 - val_accuracy: 0.7890\n",
      "Epoch 61/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4281 - accuracy: 0.7994 - val_loss: 0.4587 - val_accuracy: 0.7813\n",
      "Epoch 62/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4269 - accuracy: 0.7986 - val_loss: 0.4615 - val_accuracy: 0.7843\n",
      "Epoch 63/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4272 - accuracy: 0.8002 - val_loss: 0.4515 - val_accuracy: 0.7859\n",
      "Epoch 64/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4264 - accuracy: 0.7986 - val_loss: 0.4566 - val_accuracy: 0.7850\n",
      "Epoch 65/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4240 - accuracy: 0.7998 - val_loss: 0.4584 - val_accuracy: 0.7829\n",
      "Epoch 66/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4249 - accuracy: 0.7998 - val_loss: 0.4652 - val_accuracy: 0.7814\n",
      "Epoch 67/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4245 - accuracy: 0.8012 - val_loss: 0.4538 - val_accuracy: 0.7852\n",
      "Epoch 68/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4225 - accuracy: 0.8012 - val_loss: 0.4636 - val_accuracy: 0.7825\n",
      "Epoch 69/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4228 - accuracy: 0.8020 - val_loss: 0.4568 - val_accuracy: 0.7902\n",
      "Epoch 70/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4221 - accuracy: 0.8055 - val_loss: 0.4577 - val_accuracy: 0.7898\n",
      "Epoch 71/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4220 - accuracy: 0.8021 - val_loss: 0.4610 - val_accuracy: 0.7842\n",
      "Epoch 72/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4205 - accuracy: 0.8037 - val_loss: 0.4636 - val_accuracy: 0.7808\n",
      "Epoch 73/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4205 - accuracy: 0.8043 - val_loss: 0.4608 - val_accuracy: 0.7852\n",
      "Epoch 74/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4205 - accuracy: 0.8040 - val_loss: 0.4632 - val_accuracy: 0.7825\n",
      "Epoch 75/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4195 - accuracy: 0.8044 - val_loss: 0.4514 - val_accuracy: 0.7870\n",
      "Epoch 76/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4194 - accuracy: 0.8056 - val_loss: 0.4692 - val_accuracy: 0.7803\n",
      "Epoch 77/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4188 - accuracy: 0.8041 - val_loss: 0.4563 - val_accuracy: 0.7885\n",
      "Epoch 78/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4180 - accuracy: 0.8035 - val_loss: 0.4601 - val_accuracy: 0.7801\n",
      "Epoch 79/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4194 - accuracy: 0.8049 - val_loss: 0.4552 - val_accuracy: 0.7881\n",
      "Epoch 80/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4179 - accuracy: 0.8054 - val_loss: 0.4536 - val_accuracy: 0.7866\n",
      "Epoch 81/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4180 - accuracy: 0.8045 - val_loss: 0.4570 - val_accuracy: 0.7903\n",
      "Epoch 82/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4173 - accuracy: 0.8064 - val_loss: 0.4599 - val_accuracy: 0.7821\n",
      "Epoch 83/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4168 - accuracy: 0.8057 - val_loss: 0.4543 - val_accuracy: 0.7865\n",
      "Epoch 84/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4146 - accuracy: 0.8071 - val_loss: 0.4567 - val_accuracy: 0.7922\n",
      "Epoch 85/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4162 - accuracy: 0.8068 - val_loss: 0.4512 - val_accuracy: 0.7927\n",
      "Epoch 86/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4154 - accuracy: 0.8071 - val_loss: 0.4538 - val_accuracy: 0.7898\n",
      "Epoch 87/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4139 - accuracy: 0.8054 - val_loss: 0.4596 - val_accuracy: 0.7788\n",
      "Epoch 88/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4142 - accuracy: 0.8084 - val_loss: 0.4555 - val_accuracy: 0.7895\n",
      "Epoch 89/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4137 - accuracy: 0.8072 - val_loss: 0.4563 - val_accuracy: 0.7881\n",
      "Epoch 90/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4126 - accuracy: 0.8083 - val_loss: 0.4544 - val_accuracy: 0.7899\n",
      "Epoch 91/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4139 - accuracy: 0.8090 - val_loss: 0.4539 - val_accuracy: 0.7889\n",
      "Epoch 92/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4139 - accuracy: 0.8079 - val_loss: 0.4519 - val_accuracy: 0.7899\n",
      "Epoch 93/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4129 - accuracy: 0.8087 - val_loss: 0.4618 - val_accuracy: 0.7889\n",
      "Epoch 94/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4126 - accuracy: 0.8090 - val_loss: 0.4481 - val_accuracy: 0.7902\n",
      "Epoch 95/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4107 - accuracy: 0.8093 - val_loss: 0.4778 - val_accuracy: 0.7792\n",
      "Epoch 96/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4119 - accuracy: 0.8105 - val_loss: 0.4500 - val_accuracy: 0.7926\n",
      "Epoch 97/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4116 - accuracy: 0.8081 - val_loss: 0.4540 - val_accuracy: 0.7913\n",
      "Epoch 98/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4107 - accuracy: 0.8118 - val_loss: 0.4634 - val_accuracy: 0.7839\n",
      "Epoch 99/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4097 - accuracy: 0.8095 - val_loss: 0.4551 - val_accuracy: 0.7881\n",
      "Epoch 100/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4108 - accuracy: 0.8108 - val_loss: 0.4683 - val_accuracy: 0.7851\n",
      "Epoch 101/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4085 - accuracy: 0.8115 - val_loss: 0.4531 - val_accuracy: 0.7918\n",
      "Epoch 102/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4091 - accuracy: 0.8104 - val_loss: 0.4715 - val_accuracy: 0.7817\n",
      "Epoch 103/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4083 - accuracy: 0.8118 - val_loss: 0.4556 - val_accuracy: 0.7890\n",
      "Epoch 104/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4086 - accuracy: 0.8093 - val_loss: 0.4621 - val_accuracy: 0.7764\n",
      "Epoch 105/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4087 - accuracy: 0.8132 - val_loss: 0.4697 - val_accuracy: 0.7710\n",
      "Epoch 106/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4078 - accuracy: 0.8138 - val_loss: 0.4518 - val_accuracy: 0.7905\n",
      "Epoch 107/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4067 - accuracy: 0.8131 - val_loss: 0.4701 - val_accuracy: 0.7847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4080 - accuracy: 0.8111 - val_loss: 0.4554 - val_accuracy: 0.7927\n",
      "Epoch 109/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4059 - accuracy: 0.8129 - val_loss: 0.4630 - val_accuracy: 0.7908\n",
      "Epoch 110/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4064 - accuracy: 0.8145 - val_loss: 0.4682 - val_accuracy: 0.7847\n",
      "Epoch 111/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4068 - accuracy: 0.8118 - val_loss: 0.4551 - val_accuracy: 0.7905\n",
      "Epoch 112/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4058 - accuracy: 0.8127 - val_loss: 0.4549 - val_accuracy: 0.7886\n",
      "Epoch 113/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4044 - accuracy: 0.8116 - val_loss: 0.4614 - val_accuracy: 0.7889\n",
      "Epoch 114/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4040 - accuracy: 0.8127 - val_loss: 0.4590 - val_accuracy: 0.7848\n",
      "Epoch 115/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4050 - accuracy: 0.8143 - val_loss: 0.4473 - val_accuracy: 0.7913\n",
      "Epoch 116/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4036 - accuracy: 0.8142 - val_loss: 0.4730 - val_accuracy: 0.7798\n",
      "Epoch 117/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4038 - accuracy: 0.8138 - val_loss: 0.4702 - val_accuracy: 0.7739\n",
      "Epoch 118/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4052 - accuracy: 0.8145 - val_loss: 0.4586 - val_accuracy: 0.7872\n",
      "Epoch 119/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4043 - accuracy: 0.8141 - val_loss: 0.4515 - val_accuracy: 0.7902\n",
      "Epoch 120/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4029 - accuracy: 0.8135 - val_loss: 0.4588 - val_accuracy: 0.7904\n",
      "Epoch 121/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4036 - accuracy: 0.8159 - val_loss: 0.4591 - val_accuracy: 0.7869\n",
      "Epoch 122/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4024 - accuracy: 0.8126 - val_loss: 0.4566 - val_accuracy: 0.7843\n",
      "Epoch 123/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4021 - accuracy: 0.8165 - val_loss: 0.4585 - val_accuracy: 0.7857\n",
      "Epoch 124/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4023 - accuracy: 0.8156 - val_loss: 0.4627 - val_accuracy: 0.7886\n",
      "Epoch 125/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4004 - accuracy: 0.8155 - val_loss: 0.4581 - val_accuracy: 0.7853\n",
      "Epoch 126/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4013 - accuracy: 0.8166 - val_loss: 0.4631 - val_accuracy: 0.7947\n",
      "Epoch 127/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3994 - accuracy: 0.8164 - val_loss: 0.4660 - val_accuracy: 0.7909\n",
      "Epoch 128/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4001 - accuracy: 0.8163 - val_loss: 0.4628 - val_accuracy: 0.7934\n",
      "Epoch 129/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.4001 - accuracy: 0.8167 - val_loss: 0.4682 - val_accuracy: 0.7878\n",
      "Epoch 130/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3999 - accuracy: 0.8143 - val_loss: 0.4738 - val_accuracy: 0.7831\n",
      "Epoch 131/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3990 - accuracy: 0.8170 - val_loss: 0.4732 - val_accuracy: 0.7951\n",
      "Epoch 132/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3985 - accuracy: 0.8166 - val_loss: 0.4701 - val_accuracy: 0.7902\n",
      "Epoch 133/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3989 - accuracy: 0.8186 - val_loss: 0.4590 - val_accuracy: 0.7870\n",
      "Epoch 134/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3991 - accuracy: 0.8165 - val_loss: 0.4534 - val_accuracy: 0.7935\n",
      "Epoch 135/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3981 - accuracy: 0.8166 - val_loss: 0.4604 - val_accuracy: 0.7950\n",
      "Epoch 136/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3981 - accuracy: 0.8172 - val_loss: 0.4749 - val_accuracy: 0.7903\n",
      "Epoch 137/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3977 - accuracy: 0.8183 - val_loss: 0.4630 - val_accuracy: 0.7877\n",
      "Epoch 138/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3980 - accuracy: 0.8173 - val_loss: 0.4566 - val_accuracy: 0.7869\n",
      "Epoch 139/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3974 - accuracy: 0.8186 - val_loss: 0.4950 - val_accuracy: 0.7833\n",
      "Epoch 140/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3976 - accuracy: 0.8192 - val_loss: 0.4626 - val_accuracy: 0.7943\n",
      "Epoch 141/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3956 - accuracy: 0.8188 - val_loss: 0.4720 - val_accuracy: 0.7935\n",
      "Epoch 142/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3975 - accuracy: 0.8177 - val_loss: 0.4577 - val_accuracy: 0.7922\n",
      "Epoch 143/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3971 - accuracy: 0.8193 - val_loss: 0.4557 - val_accuracy: 0.7861\n",
      "Epoch 144/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3970 - accuracy: 0.8187 - val_loss: 0.4697 - val_accuracy: 0.7879\n",
      "Epoch 145/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3958 - accuracy: 0.8195 - val_loss: 0.4624 - val_accuracy: 0.7874\n",
      "Epoch 146/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3954 - accuracy: 0.8188 - val_loss: 0.4595 - val_accuracy: 0.7904\n",
      "Epoch 147/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3960 - accuracy: 0.8185 - val_loss: 0.4922 - val_accuracy: 0.7751\n",
      "Epoch 148/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3958 - accuracy: 0.8187 - val_loss: 0.4752 - val_accuracy: 0.7918\n",
      "Epoch 149/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3969 - accuracy: 0.8206 - val_loss: 0.4572 - val_accuracy: 0.7937\n",
      "Epoch 150/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3939 - accuracy: 0.8202 - val_loss: 0.4785 - val_accuracy: 0.7794\n",
      "Epoch 151/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3947 - accuracy: 0.8202 - val_loss: 0.4689 - val_accuracy: 0.7920\n",
      "Epoch 152/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3951 - accuracy: 0.8179 - val_loss: 0.4666 - val_accuracy: 0.7891\n",
      "Epoch 153/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3941 - accuracy: 0.8219 - val_loss: 0.4638 - val_accuracy: 0.7900\n",
      "Epoch 154/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3939 - accuracy: 0.8202 - val_loss: 0.4703 - val_accuracy: 0.7955\n",
      "Epoch 155/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3937 - accuracy: 0.8222 - val_loss: 0.4644 - val_accuracy: 0.7873\n",
      "Epoch 156/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3949 - accuracy: 0.8209 - val_loss: 0.4647 - val_accuracy: 0.7881\n",
      "Epoch 157/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3940 - accuracy: 0.8209 - val_loss: 0.4809 - val_accuracy: 0.7920\n",
      "Epoch 158/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3933 - accuracy: 0.8217 - val_loss: 0.4885 - val_accuracy: 0.7972\n",
      "Epoch 159/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3956 - accuracy: 0.8196 - val_loss: 0.4591 - val_accuracy: 0.7940\n",
      "Epoch 160/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3946 - accuracy: 0.8202 - val_loss: 0.4641 - val_accuracy: 0.7848\n",
      "Epoch 161/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3923 - accuracy: 0.8197 - val_loss: 0.4738 - val_accuracy: 0.7886\n",
      "Epoch 162/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3926 - accuracy: 0.8224 - val_loss: 0.4858 - val_accuracy: 0.7833\n",
      "Epoch 163/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3933 - accuracy: 0.8216 - val_loss: 0.4662 - val_accuracy: 0.7908\n",
      "Epoch 164/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3922 - accuracy: 0.8202 - val_loss: 0.4669 - val_accuracy: 0.7946\n",
      "Epoch 165/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3914 - accuracy: 0.8224 - val_loss: 0.4708 - val_accuracy: 0.7868\n",
      "Epoch 166/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3949 - accuracy: 0.8204 - val_loss: 0.4866 - val_accuracy: 0.7831\n",
      "Epoch 167/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3924 - accuracy: 0.8230 - val_loss: 0.4916 - val_accuracy: 0.7886\n",
      "Epoch 168/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3907 - accuracy: 0.8198 - val_loss: 0.4826 - val_accuracy: 0.7909\n",
      "Epoch 169/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3944 - accuracy: 0.8216 - val_loss: 0.4819 - val_accuracy: 0.7852\n",
      "Epoch 170/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3926 - accuracy: 0.8228 - val_loss: 0.4837 - val_accuracy: 0.7816\n",
      "Epoch 171/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3896 - accuracy: 0.8243 - val_loss: 0.4757 - val_accuracy: 0.7909\n",
      "Epoch 172/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3904 - accuracy: 0.8224 - val_loss: 0.4656 - val_accuracy: 0.7920\n",
      "Epoch 173/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3918 - accuracy: 0.8217 - val_loss: 0.4759 - val_accuracy: 0.7874\n",
      "Epoch 174/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3923 - accuracy: 0.8222 - val_loss: 0.4675 - val_accuracy: 0.7912\n",
      "Epoch 175/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3917 - accuracy: 0.8240 - val_loss: 0.4765 - val_accuracy: 0.7872\n",
      "Epoch 176/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3905 - accuracy: 0.8230 - val_loss: 0.4872 - val_accuracy: 0.7851\n",
      "Epoch 177/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3910 - accuracy: 0.8211 - val_loss: 0.4848 - val_accuracy: 0.7900\n",
      "Epoch 178/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3909 - accuracy: 0.8208 - val_loss: 0.4756 - val_accuracy: 0.7843\n",
      "Epoch 179/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3903 - accuracy: 0.8243 - val_loss: 0.4757 - val_accuracy: 0.7848\n",
      "Epoch 180/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3905 - accuracy: 0.8227 - val_loss: 0.4691 - val_accuracy: 0.7900\n",
      "Epoch 181/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3888 - accuracy: 0.8246 - val_loss: 0.4827 - val_accuracy: 0.7864\n",
      "Epoch 182/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3871 - accuracy: 0.8249 - val_loss: 0.4845 - val_accuracy: 0.7866\n",
      "Epoch 183/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3881 - accuracy: 0.8233 - val_loss: 0.4887 - val_accuracy: 0.7886\n",
      "Epoch 184/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3884 - accuracy: 0.8226 - val_loss: 0.4705 - val_accuracy: 0.7969\n",
      "Epoch 185/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3913 - accuracy: 0.8240 - val_loss: 0.4668 - val_accuracy: 0.7938\n",
      "Epoch 186/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3879 - accuracy: 0.8250 - val_loss: 0.4730 - val_accuracy: 0.7979\n",
      "Epoch 187/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3894 - accuracy: 0.8228 - val_loss: 0.4768 - val_accuracy: 0.7874\n",
      "Epoch 188/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3873 - accuracy: 0.8242 - val_loss: 0.5227 - val_accuracy: 0.7785\n",
      "Epoch 189/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3888 - accuracy: 0.8258 - val_loss: 0.4895 - val_accuracy: 0.7987\n",
      "Epoch 190/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3892 - accuracy: 0.8235 - val_loss: 0.4737 - val_accuracy: 0.7898\n",
      "Epoch 191/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3894 - accuracy: 0.8259 - val_loss: 0.4695 - val_accuracy: 0.7921\n",
      "Epoch 192/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3877 - accuracy: 0.8245 - val_loss: 0.4970 - val_accuracy: 0.7829\n",
      "Epoch 193/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3875 - accuracy: 0.8240 - val_loss: 0.4808 - val_accuracy: 0.7938\n",
      "Epoch 194/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3860 - accuracy: 0.8245 - val_loss: 0.4776 - val_accuracy: 0.7881\n",
      "Epoch 195/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3886 - accuracy: 0.8249 - val_loss: 0.4828 - val_accuracy: 0.7855\n",
      "Epoch 196/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3891 - accuracy: 0.8262 - val_loss: 0.4689 - val_accuracy: 0.7959\n",
      "Epoch 197/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3877 - accuracy: 0.8254 - val_loss: 0.4632 - val_accuracy: 0.7944\n",
      "Epoch 198/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3876 - accuracy: 0.8241 - val_loss: 0.4940 - val_accuracy: 0.7915\n",
      "Epoch 199/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3886 - accuracy: 0.8267 - val_loss: 0.4761 - val_accuracy: 0.7908\n",
      "Epoch 200/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3867 - accuracy: 0.8239 - val_loss: 0.4896 - val_accuracy: 0.7850\n",
      "Epoch 201/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3870 - accuracy: 0.8276 - val_loss: 0.4762 - val_accuracy: 0.7970\n",
      "Epoch 202/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3872 - accuracy: 0.8238 - val_loss: 0.4770 - val_accuracy: 0.7915\n",
      "Epoch 203/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3880 - accuracy: 0.8259 - val_loss: 0.4928 - val_accuracy: 0.7736\n",
      "Epoch 204/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3851 - accuracy: 0.8259 - val_loss: 0.4977 - val_accuracy: 0.7903\n",
      "Epoch 205/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3880 - accuracy: 0.8258 - val_loss: 0.4758 - val_accuracy: 0.7917\n",
      "Epoch 206/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3844 - accuracy: 0.8259 - val_loss: 0.4863 - val_accuracy: 0.7830\n",
      "Epoch 207/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3867 - accuracy: 0.8262 - val_loss: 0.5004 - val_accuracy: 0.7952\n",
      "Epoch 208/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3867 - accuracy: 0.8256 - val_loss: 0.4966 - val_accuracy: 0.7864\n",
      "Epoch 209/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3878 - accuracy: 0.8278 - val_loss: 0.4928 - val_accuracy: 0.7966\n",
      "Epoch 210/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3855 - accuracy: 0.8254 - val_loss: 0.4962 - val_accuracy: 0.7951\n",
      "Epoch 211/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3873 - accuracy: 0.8281 - val_loss: 0.4898 - val_accuracy: 0.7970\n",
      "Epoch 212/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3850 - accuracy: 0.8268 - val_loss: 0.4852 - val_accuracy: 0.7911\n",
      "Epoch 213/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3827 - accuracy: 0.8277 - val_loss: 0.4957 - val_accuracy: 0.7859\n",
      "Epoch 214/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3868 - accuracy: 0.8272 - val_loss: 0.4909 - val_accuracy: 0.7911\n",
      "Epoch 215/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3855 - accuracy: 0.8263 - val_loss: 0.5016 - val_accuracy: 0.7759\n",
      "Epoch 216/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3856 - accuracy: 0.8268 - val_loss: 0.4811 - val_accuracy: 0.7895\n",
      "Epoch 217/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3866 - accuracy: 0.8280 - val_loss: 0.4868 - val_accuracy: 0.7907\n",
      "Epoch 218/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3835 - accuracy: 0.8259 - val_loss: 0.4867 - val_accuracy: 0.7891\n",
      "Epoch 219/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3869 - accuracy: 0.8268 - val_loss: 0.4744 - val_accuracy: 0.7956\n",
      "Epoch 220/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3852 - accuracy: 0.8270 - val_loss: 0.5035 - val_accuracy: 0.7905\n",
      "Epoch 221/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3884 - accuracy: 0.8277 - val_loss: 0.4894 - val_accuracy: 0.7905\n",
      "Epoch 222/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3838 - accuracy: 0.8268 - val_loss: 0.4811 - val_accuracy: 0.7950\n",
      "Epoch 223/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3856 - accuracy: 0.8274 - val_loss: 0.4927 - val_accuracy: 0.7827\n",
      "Epoch 224/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3846 - accuracy: 0.8280 - val_loss: 0.4938 - val_accuracy: 0.7959\n",
      "Epoch 225/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3822 - accuracy: 0.8297 - val_loss: 0.4962 - val_accuracy: 0.7842\n",
      "Epoch 226/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3847 - accuracy: 0.8276 - val_loss: 0.5027 - val_accuracy: 0.7795\n",
      "Epoch 227/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3851 - accuracy: 0.8274 - val_loss: 0.5012 - val_accuracy: 0.7952\n",
      "Epoch 228/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3816 - accuracy: 0.8268 - val_loss: 0.5217 - val_accuracy: 0.7898\n",
      "Epoch 229/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3815 - accuracy: 0.8257 - val_loss: 0.5126 - val_accuracy: 0.7938\n",
      "Epoch 230/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3840 - accuracy: 0.8278 - val_loss: 0.4946 - val_accuracy: 0.7904\n",
      "Epoch 231/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3809 - accuracy: 0.8272 - val_loss: 0.5136 - val_accuracy: 0.7895\n",
      "Epoch 232/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3893 - accuracy: 0.8267 - val_loss: 0.5029 - val_accuracy: 0.7840\n",
      "Epoch 233/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3871 - accuracy: 0.8296 - val_loss: 0.5658 - val_accuracy: 0.7824\n",
      "Epoch 234/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3812 - accuracy: 0.8306 - val_loss: 0.5014 - val_accuracy: 0.7929\n",
      "Epoch 235/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3828 - accuracy: 0.8311 - val_loss: 0.5099 - val_accuracy: 0.7859\n",
      "Epoch 236/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3865 - accuracy: 0.8293 - val_loss: 0.5038 - val_accuracy: 0.7892\n",
      "Epoch 237/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3834 - accuracy: 0.8265 - val_loss: 0.5095 - val_accuracy: 0.7885\n",
      "Epoch 238/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3830 - accuracy: 0.8286 - val_loss: 0.4978 - val_accuracy: 0.7929\n",
      "Epoch 239/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3829 - accuracy: 0.8301 - val_loss: 0.5035 - val_accuracy: 0.7921\n",
      "Epoch 240/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3848 - accuracy: 0.8286 - val_loss: 0.5319 - val_accuracy: 0.7798\n",
      "Epoch 241/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3812 - accuracy: 0.8290 - val_loss: 0.5052 - val_accuracy: 0.7876\n",
      "Epoch 242/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3845 - accuracy: 0.8277 - val_loss: 0.5053 - val_accuracy: 0.7887\n",
      "Epoch 243/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3842 - accuracy: 0.8284 - val_loss: 0.5087 - val_accuracy: 0.7904\n",
      "Epoch 244/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3850 - accuracy: 0.8291 - val_loss: 0.5066 - val_accuracy: 0.7895\n",
      "Epoch 245/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3830 - accuracy: 0.8292 - val_loss: 0.5013 - val_accuracy: 0.7944\n",
      "Epoch 246/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3860 - accuracy: 0.8289 - val_loss: 0.5173 - val_accuracy: 0.7846\n",
      "Epoch 247/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3813 - accuracy: 0.8297 - val_loss: 0.5032 - val_accuracy: 0.7847\n",
      "Epoch 248/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3842 - accuracy: 0.8324 - val_loss: 0.5195 - val_accuracy: 0.7812\n",
      "Epoch 249/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3835 - accuracy: 0.8310 - val_loss: 0.5303 - val_accuracy: 0.7705\n",
      "Epoch 250/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3848 - accuracy: 0.8299 - val_loss: 0.5050 - val_accuracy: 0.7835\n",
      "Epoch 251/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3855 - accuracy: 0.8293 - val_loss: 0.5383 - val_accuracy: 0.7859\n",
      "Epoch 252/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3828 - accuracy: 0.8305 - val_loss: 0.5133 - val_accuracy: 0.7856\n",
      "Epoch 253/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3831 - accuracy: 0.8280 - val_loss: 0.5205 - val_accuracy: 0.7916\n",
      "Epoch 254/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3854 - accuracy: 0.8312 - val_loss: 0.5126 - val_accuracy: 0.7911\n",
      "Epoch 255/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3805 - accuracy: 0.8306 - val_loss: 0.5309 - val_accuracy: 0.7838\n",
      "Epoch 256/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3819 - accuracy: 0.8316 - val_loss: 0.5075 - val_accuracy: 0.7842\n",
      "Epoch 257/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3847 - accuracy: 0.8299 - val_loss: 0.5006 - val_accuracy: 0.7839\n",
      "Epoch 258/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3795 - accuracy: 0.8309 - val_loss: 0.5283 - val_accuracy: 0.7857\n",
      "Epoch 259/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3859 - accuracy: 0.8294 - val_loss: 0.5083 - val_accuracy: 0.7987\n",
      "Epoch 260/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3806 - accuracy: 0.8306 - val_loss: 0.5044 - val_accuracy: 0.7912\n",
      "Epoch 261/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3853 - accuracy: 0.8290 - val_loss: 0.5229 - val_accuracy: 0.7921\n",
      "Epoch 262/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3803 - accuracy: 0.8309 - val_loss: 0.5126 - val_accuracy: 0.7872\n",
      "Epoch 263/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3840 - accuracy: 0.8297 - val_loss: 0.5357 - val_accuracy: 0.7857\n",
      "Epoch 264/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3858 - accuracy: 0.8312 - val_loss: 0.5090 - val_accuracy: 0.7777\n",
      "Epoch 265/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3801 - accuracy: 0.8294 - val_loss: 0.5611 - val_accuracy: 0.7863\n",
      "Epoch 266/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3811 - accuracy: 0.8309 - val_loss: 0.5099 - val_accuracy: 0.7791\n",
      "Epoch 267/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3809 - accuracy: 0.8299 - val_loss: 0.5412 - val_accuracy: 0.7777\n",
      "Epoch 268/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3817 - accuracy: 0.8295 - val_loss: 0.5150 - val_accuracy: 0.7889\n",
      "Epoch 269/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3830 - accuracy: 0.8315 - val_loss: 0.5371 - val_accuracy: 0.7878\n",
      "Epoch 270/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3789 - accuracy: 0.8321 - val_loss: 0.5178 - val_accuracy: 0.7839\n",
      "Epoch 271/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3802 - accuracy: 0.8308 - val_loss: 0.5030 - val_accuracy: 0.7892\n",
      "Epoch 272/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3815 - accuracy: 0.8296 - val_loss: 0.5196 - val_accuracy: 0.7883\n",
      "Epoch 273/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3809 - accuracy: 0.8308 - val_loss: 0.5279 - val_accuracy: 0.7829\n",
      "Epoch 274/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3790 - accuracy: 0.8316 - val_loss: 0.5258 - val_accuracy: 0.7812\n",
      "Epoch 275/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3811 - accuracy: 0.8321 - val_loss: 0.5167 - val_accuracy: 0.7799\n",
      "Epoch 276/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3796 - accuracy: 0.8309 - val_loss: 0.5443 - val_accuracy: 0.7834\n",
      "Epoch 277/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3805 - accuracy: 0.8316 - val_loss: 0.5268 - val_accuracy: 0.7856\n",
      "Epoch 278/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3799 - accuracy: 0.8318 - val_loss: 0.5655 - val_accuracy: 0.7855\n",
      "Epoch 279/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3853 - accuracy: 0.8321 - val_loss: 0.5446 - val_accuracy: 0.7787\n",
      "Epoch 280/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3787 - accuracy: 0.8318 - val_loss: 0.5392 - val_accuracy: 0.7889\n",
      "Epoch 281/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3866 - accuracy: 0.8311 - val_loss: 0.5143 - val_accuracy: 0.7925\n",
      "Epoch 282/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3812 - accuracy: 0.8320 - val_loss: 0.5164 - val_accuracy: 0.7855\n",
      "Epoch 283/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3848 - accuracy: 0.8312 - val_loss: 0.5291 - val_accuracy: 0.7887\n",
      "Epoch 284/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3830 - accuracy: 0.8300 - val_loss: 0.5443 - val_accuracy: 0.7829\n",
      "Epoch 285/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3820 - accuracy: 0.8310 - val_loss: 0.5294 - val_accuracy: 0.7827\n",
      "Epoch 286/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3798 - accuracy: 0.8327 - val_loss: 0.5490 - val_accuracy: 0.7915\n",
      "Epoch 287/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3816 - accuracy: 0.8339 - val_loss: 0.5470 - val_accuracy: 0.7865\n",
      "Epoch 288/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3794 - accuracy: 0.8302 - val_loss: 0.5426 - val_accuracy: 0.7902\n",
      "Epoch 289/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3812 - accuracy: 0.8330 - val_loss: 0.5313 - val_accuracy: 0.7868\n",
      "Epoch 290/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3845 - accuracy: 0.8345 - val_loss: 0.5406 - val_accuracy: 0.7873\n",
      "Epoch 291/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3776 - accuracy: 0.8328 - val_loss: 0.5240 - val_accuracy: 0.7822\n",
      "Epoch 292/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3827 - accuracy: 0.8318 - val_loss: 0.5380 - val_accuracy: 0.7773\n",
      "Epoch 293/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3748 - accuracy: 0.8331 - val_loss: 0.5369 - val_accuracy: 0.7891\n",
      "Epoch 294/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3786 - accuracy: 0.8318 - val_loss: 0.5462 - val_accuracy: 0.7733\n",
      "Epoch 295/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3793 - accuracy: 0.8321 - val_loss: 0.5553 - val_accuracy: 0.7839\n",
      "Epoch 296/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3773 - accuracy: 0.8342 - val_loss: 0.5361 - val_accuracy: 0.7794\n",
      "Epoch 297/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3799 - accuracy: 0.8348 - val_loss: 0.5533 - val_accuracy: 0.7809\n",
      "Epoch 298/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3810 - accuracy: 0.8322 - val_loss: 0.5387 - val_accuracy: 0.7890\n",
      "Epoch 299/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3804 - accuracy: 0.8318 - val_loss: 0.5414 - val_accuracy: 0.7794\n",
      "Epoch 300/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3842 - accuracy: 0.8345 - val_loss: 0.5470 - val_accuracy: 0.7809\n",
      "Epoch 301/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3773 - accuracy: 0.8334 - val_loss: 0.5495 - val_accuracy: 0.7902\n",
      "Epoch 302/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3818 - accuracy: 0.8336 - val_loss: 0.5651 - val_accuracy: 0.7870\n",
      "Epoch 303/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3757 - accuracy: 0.8320 - val_loss: 0.5431 - val_accuracy: 0.7848\n",
      "Epoch 304/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3772 - accuracy: 0.8349 - val_loss: 0.5545 - val_accuracy: 0.7774\n",
      "Epoch 305/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3807 - accuracy: 0.8312 - val_loss: 0.5425 - val_accuracy: 0.7818\n",
      "Epoch 306/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3764 - accuracy: 0.8327 - val_loss: 0.5507 - val_accuracy: 0.7870\n",
      "Epoch 307/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3820 - accuracy: 0.8329 - val_loss: 0.5733 - val_accuracy: 0.7775\n",
      "Epoch 308/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3828 - accuracy: 0.8314 - val_loss: 0.5536 - val_accuracy: 0.7798\n",
      "Epoch 309/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3807 - accuracy: 0.8300 - val_loss: 0.5323 - val_accuracy: 0.7890\n",
      "Epoch 310/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3792 - accuracy: 0.8340 - val_loss: 0.5509 - val_accuracy: 0.7857\n",
      "Epoch 311/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3766 - accuracy: 0.8350 - val_loss: 0.5360 - val_accuracy: 0.7848\n",
      "Epoch 312/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3808 - accuracy: 0.8341 - val_loss: 0.5581 - val_accuracy: 0.7709\n",
      "Epoch 313/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3776 - accuracy: 0.8353 - val_loss: 0.5451 - val_accuracy: 0.7818\n",
      "Epoch 314/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3762 - accuracy: 0.8328 - val_loss: 0.5904 - val_accuracy: 0.7756\n",
      "Epoch 315/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3812 - accuracy: 0.8341 - val_loss: 0.5387 - val_accuracy: 0.7852\n",
      "Epoch 316/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3801 - accuracy: 0.8333 - val_loss: 0.5357 - val_accuracy: 0.7830\n",
      "Epoch 317/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3771 - accuracy: 0.8347 - val_loss: 0.5488 - val_accuracy: 0.7876\n",
      "Epoch 318/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3812 - accuracy: 0.8332 - val_loss: 0.5698 - val_accuracy: 0.7765\n",
      "Epoch 319/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3767 - accuracy: 0.8337 - val_loss: 0.5656 - val_accuracy: 0.7865\n",
      "Epoch 320/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3730 - accuracy: 0.8332 - val_loss: 0.5559 - val_accuracy: 0.7816\n",
      "Epoch 321/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3792 - accuracy: 0.8318 - val_loss: 0.5869 - val_accuracy: 0.7738\n",
      "Epoch 322/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3786 - accuracy: 0.8339 - val_loss: 0.5651 - val_accuracy: 0.7852\n",
      "Epoch 323/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3756 - accuracy: 0.8344 - val_loss: 0.5481 - val_accuracy: 0.7860\n",
      "Epoch 324/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3803 - accuracy: 0.8352 - val_loss: 0.5444 - val_accuracy: 0.7865\n",
      "Epoch 325/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3824 - accuracy: 0.8354 - val_loss: 0.5658 - val_accuracy: 0.7848\n",
      "Epoch 326/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3794 - accuracy: 0.8340 - val_loss: 0.5690 - val_accuracy: 0.7870\n",
      "Epoch 327/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3770 - accuracy: 0.8345 - val_loss: 0.5704 - val_accuracy: 0.7756\n",
      "Epoch 328/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3748 - accuracy: 0.8344 - val_loss: 0.5462 - val_accuracy: 0.7847\n",
      "Epoch 329/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3779 - accuracy: 0.8337 - val_loss: 0.5413 - val_accuracy: 0.7896\n",
      "Epoch 330/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3813 - accuracy: 0.8352 - val_loss: 0.5469 - val_accuracy: 0.7859\n",
      "Epoch 331/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3818 - accuracy: 0.8357 - val_loss: 0.5934 - val_accuracy: 0.7904\n",
      "Epoch 332/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3766 - accuracy: 0.8346 - val_loss: 0.5874 - val_accuracy: 0.7808\n",
      "Epoch 333/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3801 - accuracy: 0.8343 - val_loss: 0.5511 - val_accuracy: 0.7782\n",
      "Epoch 334/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3854 - accuracy: 0.8351 - val_loss: 0.5509 - val_accuracy: 0.7811\n",
      "Epoch 335/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3775 - accuracy: 0.8334 - val_loss: 0.5650 - val_accuracy: 0.7807\n",
      "Epoch 336/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3836 - accuracy: 0.8341 - val_loss: 0.5763 - val_accuracy: 0.7834\n",
      "Epoch 337/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3788 - accuracy: 0.8346 - val_loss: 0.5656 - val_accuracy: 0.7855\n",
      "Epoch 338/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3825 - accuracy: 0.8337 - val_loss: 0.5963 - val_accuracy: 0.7833\n",
      "Epoch 339/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3861 - accuracy: 0.8353 - val_loss: 0.5503 - val_accuracy: 0.7877\n",
      "Epoch 340/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3815 - accuracy: 0.8324 - val_loss: 0.5451 - val_accuracy: 0.7800\n",
      "Epoch 341/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3781 - accuracy: 0.8345 - val_loss: 0.5943 - val_accuracy: 0.7822\n",
      "Epoch 342/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3785 - accuracy: 0.8359 - val_loss: 0.5994 - val_accuracy: 0.7790\n",
      "Epoch 343/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3870 - accuracy: 0.8347 - val_loss: 0.5603 - val_accuracy: 0.7859\n",
      "Epoch 344/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3774 - accuracy: 0.8354 - val_loss: 0.5725 - val_accuracy: 0.7851\n",
      "Epoch 345/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3776 - accuracy: 0.8346 - val_loss: 0.5421 - val_accuracy: 0.7872\n",
      "Epoch 346/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3751 - accuracy: 0.8353 - val_loss: 0.5680 - val_accuracy: 0.7803\n",
      "Epoch 347/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3832 - accuracy: 0.8347 - val_loss: 0.5501 - val_accuracy: 0.7930\n",
      "Epoch 348/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3784 - accuracy: 0.8355 - val_loss: 0.5357 - val_accuracy: 0.7813\n",
      "Epoch 349/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3836 - accuracy: 0.8342 - val_loss: 0.6046 - val_accuracy: 0.7798\n",
      "Epoch 350/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3768 - accuracy: 0.8353 - val_loss: 0.5775 - val_accuracy: 0.7850\n",
      "Epoch 351/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3829 - accuracy: 0.8344 - val_loss: 0.5961 - val_accuracy: 0.7850\n",
      "Epoch 352/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3811 - accuracy: 0.8343 - val_loss: 0.5655 - val_accuracy: 0.7746\n",
      "Epoch 353/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3839 - accuracy: 0.8329 - val_loss: 0.5680 - val_accuracy: 0.7856\n",
      "Epoch 354/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3810 - accuracy: 0.8341 - val_loss: 0.5887 - val_accuracy: 0.7808\n",
      "Epoch 355/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3813 - accuracy: 0.8339 - val_loss: 0.5601 - val_accuracy: 0.7820\n",
      "Epoch 356/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3833 - accuracy: 0.8347 - val_loss: 0.5940 - val_accuracy: 0.7760\n",
      "Epoch 357/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3818 - accuracy: 0.8340 - val_loss: 0.5670 - val_accuracy: 0.7831\n",
      "Epoch 358/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3808 - accuracy: 0.8343 - val_loss: 0.5750 - val_accuracy: 0.7821\n",
      "Epoch 359/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3799 - accuracy: 0.8339 - val_loss: 0.5698 - val_accuracy: 0.7866\n",
      "Epoch 360/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3785 - accuracy: 0.8341 - val_loss: 0.5450 - val_accuracy: 0.7874\n",
      "Epoch 361/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3819 - accuracy: 0.8345 - val_loss: 0.5653 - val_accuracy: 0.7899\n",
      "Epoch 362/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3819 - accuracy: 0.8350 - val_loss: 0.5775 - val_accuracy: 0.7825\n",
      "Epoch 363/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3798 - accuracy: 0.8357 - val_loss: 0.6182 - val_accuracy: 0.7761\n",
      "Epoch 364/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3762 - accuracy: 0.8356 - val_loss: 0.5781 - val_accuracy: 0.7809\n",
      "Epoch 365/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3827 - accuracy: 0.8368 - val_loss: 0.5649 - val_accuracy: 0.7825\n",
      "Epoch 366/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3771 - accuracy: 0.8347 - val_loss: 0.5595 - val_accuracy: 0.7807\n",
      "Epoch 367/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3867 - accuracy: 0.8330 - val_loss: 0.5397 - val_accuracy: 0.7848\n",
      "Epoch 368/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3769 - accuracy: 0.8366 - val_loss: 0.5857 - val_accuracy: 0.7892\n",
      "Epoch 369/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3824 - accuracy: 0.8355 - val_loss: 0.5598 - val_accuracy: 0.7736\n",
      "Epoch 370/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3807 - accuracy: 0.8346 - val_loss: 0.5706 - val_accuracy: 0.7863\n",
      "Epoch 371/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3826 - accuracy: 0.8342 - val_loss: 0.5738 - val_accuracy: 0.7812\n",
      "Epoch 372/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3751 - accuracy: 0.8338 - val_loss: 0.5545 - val_accuracy: 0.7870\n",
      "Epoch 373/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3777 - accuracy: 0.8339 - val_loss: 0.5669 - val_accuracy: 0.7839\n",
      "Epoch 374/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3814 - accuracy: 0.8363 - val_loss: 0.5397 - val_accuracy: 0.7829\n",
      "Epoch 375/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3794 - accuracy: 0.8353 - val_loss: 0.5670 - val_accuracy: 0.7890\n",
      "Epoch 376/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3796 - accuracy: 0.8357 - val_loss: 0.5996 - val_accuracy: 0.7781\n",
      "Epoch 377/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3742 - accuracy: 0.8361 - val_loss: 0.5750 - val_accuracy: 0.7877\n",
      "Epoch 378/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3792 - accuracy: 0.8341 - val_loss: 0.5535 - val_accuracy: 0.7840\n",
      "Epoch 379/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3751 - accuracy: 0.8335 - val_loss: 0.5963 - val_accuracy: 0.7918\n",
      "Epoch 380/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3823 - accuracy: 0.8360 - val_loss: 0.5606 - val_accuracy: 0.7857\n",
      "Epoch 381/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3835 - accuracy: 0.8345 - val_loss: 0.5863 - val_accuracy: 0.7856\n",
      "Epoch 382/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3767 - accuracy: 0.8350 - val_loss: 0.5603 - val_accuracy: 0.7859\n",
      "Epoch 383/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3824 - accuracy: 0.8310 - val_loss: 0.5578 - val_accuracy: 0.7895\n",
      "Epoch 384/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3865 - accuracy: 0.8362 - val_loss: 0.5634 - val_accuracy: 0.7922\n",
      "Epoch 385/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3827 - accuracy: 0.8364 - val_loss: 0.5551 - val_accuracy: 0.7826\n",
      "Epoch 386/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3865 - accuracy: 0.8347 - val_loss: 0.5799 - val_accuracy: 0.7857\n",
      "Epoch 387/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3773 - accuracy: 0.8345 - val_loss: 0.5802 - val_accuracy: 0.7769\n",
      "Epoch 388/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3799 - accuracy: 0.8359 - val_loss: 0.5727 - val_accuracy: 0.7804\n",
      "Epoch 389/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3831 - accuracy: 0.8355 - val_loss: 0.5693 - val_accuracy: 0.7792\n",
      "Epoch 390/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3813 - accuracy: 0.8345 - val_loss: 0.5640 - val_accuracy: 0.7743\n",
      "Epoch 391/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3828 - accuracy: 0.8367 - val_loss: 0.5638 - val_accuracy: 0.7856\n",
      "Epoch 392/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3801 - accuracy: 0.8369 - val_loss: 0.5686 - val_accuracy: 0.7907\n",
      "Epoch 393/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3801 - accuracy: 0.8356 - val_loss: 0.5893 - val_accuracy: 0.7864\n",
      "Epoch 394/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3771 - accuracy: 0.8346 - val_loss: 0.5729 - val_accuracy: 0.7896\n",
      "Epoch 395/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3720 - accuracy: 0.8369 - val_loss: 0.5915 - val_accuracy: 0.7840\n",
      "Epoch 396/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3795 - accuracy: 0.8333 - val_loss: 0.6016 - val_accuracy: 0.7870\n",
      "Epoch 397/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3812 - accuracy: 0.8361 - val_loss: 0.5875 - val_accuracy: 0.7855\n",
      "Epoch 398/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3797 - accuracy: 0.8367 - val_loss: 0.5838 - val_accuracy: 0.7857\n",
      "Epoch 399/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3797 - accuracy: 0.8361 - val_loss: 0.5783 - val_accuracy: 0.7826\n",
      "Epoch 400/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3801 - accuracy: 0.8364 - val_loss: 0.6069 - val_accuracy: 0.7847\n",
      "Epoch 401/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3834 - accuracy: 0.8367 - val_loss: 0.6006 - val_accuracy: 0.7864\n",
      "Epoch 402/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3823 - accuracy: 0.8356 - val_loss: 0.5775 - val_accuracy: 0.7873\n",
      "Epoch 403/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3840 - accuracy: 0.8346 - val_loss: 0.5760 - val_accuracy: 0.7840\n",
      "Epoch 404/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3877 - accuracy: 0.8344 - val_loss: 0.5839 - val_accuracy: 0.7764\n",
      "Epoch 405/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3822 - accuracy: 0.8361 - val_loss: 0.6655 - val_accuracy: 0.7840\n",
      "Epoch 406/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3813 - accuracy: 0.8353 - val_loss: 0.6098 - val_accuracy: 0.7843\n",
      "Epoch 407/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3801 - accuracy: 0.8341 - val_loss: 0.6130 - val_accuracy: 0.7762\n",
      "Epoch 408/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3793 - accuracy: 0.8352 - val_loss: 0.5582 - val_accuracy: 0.7892\n",
      "Epoch 409/500\n",
      "962/962 [==============================] - 2s 2ms/step - loss: 0.3767 - accuracy: 0.8377 - val_loss: 0.6257 - val_accuracy: 0.7735\n",
      "Epoch 410/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3803 - accuracy: 0.8381 - val_loss: 0.6246 - val_accuracy: 0.7824\n",
      "Epoch 411/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3761 - accuracy: 0.8357 - val_loss: 0.6046 - val_accuracy: 0.7837\n",
      "Epoch 412/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3846 - accuracy: 0.8356 - val_loss: 0.5816 - val_accuracy: 0.7803\n",
      "Epoch 413/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3853 - accuracy: 0.8356 - val_loss: 0.6151 - val_accuracy: 0.7851\n",
      "Epoch 414/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3835 - accuracy: 0.8348 - val_loss: 0.5840 - val_accuracy: 0.7839\n",
      "Epoch 415/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3757 - accuracy: 0.8347 - val_loss: 0.5972 - val_accuracy: 0.7852\n",
      "Epoch 416/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3821 - accuracy: 0.8378 - val_loss: 0.5811 - val_accuracy: 0.7870\n",
      "Epoch 417/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3806 - accuracy: 0.8348 - val_loss: 0.6191 - val_accuracy: 0.7753\n",
      "Epoch 418/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3797 - accuracy: 0.8364 - val_loss: 0.5903 - val_accuracy: 0.7938\n",
      "Epoch 419/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3816 - accuracy: 0.8354 - val_loss: 0.6205 - val_accuracy: 0.7834\n",
      "Epoch 420/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3800 - accuracy: 0.8357 - val_loss: 0.6559 - val_accuracy: 0.7801\n",
      "Epoch 421/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3816 - accuracy: 0.8364 - val_loss: 0.5916 - val_accuracy: 0.7831\n",
      "Epoch 422/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3796 - accuracy: 0.8368 - val_loss: 0.5897 - val_accuracy: 0.7865\n",
      "Epoch 423/500\n",
      "962/962 [==============================] - 2s 3ms/step - loss: 0.3793 - accuracy: 0.8371 - val_loss: 0.5950 - val_accuracy: 0.7859\n",
      "Epoch 424/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3807 - accuracy: 0.8371 - val_loss: 0.6102 - val_accuracy: 0.7744\n",
      "Epoch 425/500\n",
      "962/962 [==============================] - 3s 3ms/step - loss: 0.3821 - accuracy: 0.8353 - val_loss: 0.5883 - val_accuracy: 0.7844\n",
      "Epoch 426/500\n",
      "611/962 [==================>...........] - ETA: 1s - loss: 0.3797 - accuracy: 0.8364"
     ]
    }
   ],
   "source": [
    "act_fun = [\"sigmoid\",\"tanh\",\"relu\", \"elu\",\"LeakyReLU\"]\n",
    "dfs = []\n",
    "\n",
    "accuracy_score = []\n",
    "recall_score = []\n",
    "precision_score = []\n",
    "f1_score = []\n",
    "\n",
    "best_activation_function = \"sigmoid\"\n",
    "best_activation_function_f1 = 0\n",
    "\n",
    "for activ in act_fun:\n",
    "    history = History()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200,activation=activ,input_shape=(X_train_scaled.shape[1],)))\n",
    "    model.add(Dense(150,activation=activ))\n",
    "    model.add(Dense(50,activation=activ))\n",
    "    # sigmoid zawsze na ostatnim w przypadku binarnej klasyfikacji, zgodnie z rekomendacja\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    print(\"--------------\\n\", activ,'\\n\\n')\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    history = model.fit(X_train_scaled, y_train, validation_data= (X_test_scaled, y_test), batch_size=32, epochs=500)\n",
    "\n",
    "    print(\"\\n--------------\\n\",model.evaluate(X_test_scaled,y_test))\n",
    "    \n",
    "    ypred = model.predict(X_test_scaled)\n",
    "    pred_classes = np.where(ypred > 0.5, 1,0)\n",
    "    \n",
    "    print(\"Accuracy:\", metrics.accuracy_score(y_true= y_test, y_pred= pred_classes))\n",
    "    print(\"Recall:\", metrics.recall_score(y_true= y_test, y_pred= pred_classes))\n",
    "    print(\"Precision:\", metrics.precision_score(y_true= y_test, y_pred= pred_classes))\n",
    "    print(\"F1:\", metrics.f1_score(y_true= y_test, y_pred= pred_classes))\n",
    "\n",
    "    dfs.append(pd.DataFrame(history.history))\n",
    "    \n",
    "    accuracy_score.append(metrics.accuracy_score(y_true= y_test, y_pred= pred_classes ))\n",
    "    recall_score.append(metrics.recall_score(y_true= y_test, y_pred= pred_classes))\n",
    "    precision_score.append(metrics.precision_score(y_true= y_test, y_pred= pred_classes))\n",
    "    f1_score.append(metrics.f1_score(y_true= y_test, y_pred= pred_classes))\n",
    "    \n",
    "    if metrics.f1_score(y_true= y_test, y_pred= pred_classes) > best_activation_function_f1:\n",
    "        best_activation_function_f1 = metrics.f1_score(y_true= y_test, y_pred= pred_classes)\n",
    "        best_activation_function = activ\n",
    "    \n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9427915",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "#     print(act_fun[dfs.index(df)])\n",
    "    df.plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497631a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {\n",
    "    'accuracy_score' : accuracy_score, \n",
    "    'recall_score': recall_score, \n",
    "    'precision_score': precision_score, \n",
    "    'f1_score': f1_score\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "df.insert(loc=0, column='Activation', value=[\"sigmoid\",\"tanh\",\"relu\", \"elu\",\"LeakyReLU\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e08dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_activation_function, best_activation_function_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deydujemy optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bfc049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "act_fun = [\"SGD\",\"RMSprop\",\"Adagrad\",\"Adadelta\",\"Adam\"]\n",
    "dfs = []\n",
    "\n",
    "accuracy_score = []\n",
    "recall_score = []\n",
    "precision_score = []\n",
    "f1_score = []\n",
    "\n",
    "best_optimizer = \"SGD\"\n",
    "best_optimizer_f1 = 0\n",
    "\n",
    "for activ in act_fun:\n",
    "    history = History()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200,activation=best_activation_function,input_shape=(X_train_scaled.shape[1],)))\n",
    "    model.add(Dense(150,activation=best_activation_function))\n",
    "    model.add(Dense(50,activation=best_activation_function))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\",optimizer=activ, metrics=[\"accuracy\"])\n",
    "\n",
    "    print(\"--------------\\n\", activ,'\\n\\n')\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    history = model.fit(X_train_scaled, y_train, validation_data= (X_test_scaled, y_test), batch_size=32,epochs=500);\n",
    "\n",
    "    print(\"\\n--------------\\n\",model.evaluate(X_test_scaled,y_test))\n",
    "    \n",
    "    ypred = model.predict(X_test_scaled)\n",
    "    pred_classes = np.where(ypred > 0.5, 1,0)\n",
    "    \n",
    "    print(\"Accuracy:\", metrics.accuracy_score(y_true= y_test, y_pred= pred_classes))\n",
    "    print(\"Recall:\", metrics.recall_score(y_true= y_test, y_pred= pred_classes))\n",
    "    print(\"Precision:\", metrics.precision_score(y_true= y_test, y_pred= pred_classes))\n",
    "    print(\"F1:\", metrics.f1_score(y_true= y_test, y_pred= pred_classes))\n",
    "\n",
    "    dfs.append(pd.DataFrame(history.history))\n",
    "    \n",
    "    accuracy_score.append(metrics.accuracy_score(y_true= y_test, y_pred= pred_classes ))\n",
    "    recall_score.append(metrics.recall_score(y_true= y_test, y_pred= pred_classes))\n",
    "    precision_score.append(metrics.precision_score(y_true= y_test, y_pred= pred_classes))\n",
    "    f1_score.append(metrics.f1_score(y_true= y_test, y_pred= pred_classes))\n",
    "    \n",
    "    if metrics.f1_score(y_true= y_test, y_pred= pred_classes) > best_optimizer_f1:\n",
    "        best_optimizer_f1 = metrics.f1_score(y_true= y_test, y_pred= pred_classes)\n",
    "        best_optimizer = activ\n",
    "    \n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "#     print(act_fun[dfs.index(df)])\n",
    "    df.plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea43493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {\n",
    "    'accuracy_score' : accuracy_score, \n",
    "    'recall_score': recall_score, \n",
    "    'precision_score': precision_score, \n",
    "    'f1_score': f1_score\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "df.insert(loc=0, column='Optimizer', value=[\"SGD\",\"RMSprop\",\"Adagrad\",\"Adadelta\",\"Adam\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888b04c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_optimizer, best_optimizer_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f8922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# laczymy w calosc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podstawowy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69602481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history1 = History()\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(200, activation=best_activation_function, input_shape=(X_train_scaled.shape[1],)))\n",
    "model1.add(Dense(150, activation=best_activation_function))\n",
    "model1.add(Dense(50, activation=best_activation_function))\n",
    "model1.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model1.compile(loss=\"binary_crossentropy\", optimizer=best_optimizer, metrics=[\"accuracy\"])\n",
    "history1 = model1.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test), batch_size=32, epochs=1000, callbacks=[])\n",
    "\n",
    "ypred1 = model1.predict(X_test_scaled)\n",
    "model1_pred_classes = np.where(ypred1 > 0.5, 1,0)\n",
    "\n",
    "print(model1.evaluate(X_test_scaled, y_test))\n",
    "print(\"R2:\",metrics.r2_score(y_true= y_test, y_pred= ypred1 ))\n",
    "\n",
    "print(classification_report(y_test, model1_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d80f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation:\", model1.evaluate(X_val, y_val))\n",
    "print(\"Validation R2:\", metrics.r2_score(y_true= y_val, y_pred= model1.predict(X_val) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model + learningratescheduler + early stopping + history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a12b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "\n",
    "# scheduler\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * np.power(drop, np.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "history2 = History()\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(200, use_bias=False,input_shape=(X_train_scaled.shape[1],)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation(best_activation_function))\n",
    "model2.add(Dense(150, use_bias=False))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation(best_activation_function))\n",
    "model2.add(Dense(50, use_bias=False))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation(best_activation_function))\n",
    "model2.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model2.compile(loss=\"binary_crossentropy\",optimizer=best_optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, mode='min', verbose=1)\n",
    "\n",
    "# LearningRateScheduler\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "history2 = model2.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test), batch_size=32, epochs=1000, callbacks=[lrate, early_stopping, history])\n",
    "\n",
    "ypred2 = model2.predict(X_test_scaled)\n",
    "model2_pred_classes = np.where(ypred2 > 0.5, 1,0)\n",
    "\n",
    "print(model2.evaluate(X_test_scaled, y_test))\n",
    "print(\"R2:\",metrics.r2_score(y_true= y_test, y_pred= ypred2 ))\n",
    "\n",
    "print(classification_report(y_test, model2_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b82229",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred1 = model1.predict(scaler.fit_transform(X_test))\n",
    "pred_val_classes1 = np.where(y_val_pred1 > 0.5, 1,0)\n",
    "\n",
    "y_val_pred2 = model2.predict(scaler.fit_transform(X_test))\n",
    "pred_val_classes2 = np.where(y_val_pred2 > 0.5, 1,0)\n",
    "\n",
    "y_val_pred3 = grid_1.predict(X_test)\n",
    "# pred_val_classes3 = np.where(y_val_pred3 > 0.5, 1,0)\n",
    "\n",
    "y_val_pred4 = grid_2.predict(X_test)\n",
    "# pred_val_classes4 = np.where(y_val_pred4 > 0.5, 1,0)\n",
    "\n",
    "# print(pred_val_classes2, y_val_pred3)\n",
    "\n",
    "import pandas as pd\n",
    "flatten_list1 = []\n",
    "flatten_list2 = []\n",
    "flatten_list3 = []\n",
    "flatten_list4 = []\n",
    "\n",
    "for s in pred_val_classes1:\n",
    "    for i in s: \n",
    "        flatten_list1.append(i)\n",
    "        \n",
    "for s in pred_val_classes2:\n",
    "    for i in s: \n",
    "        flatten_list2.append(i)\n",
    "        \n",
    "# for s in y_val_pred3:\n",
    "#     flatten_list3.append(i)\n",
    "# #     for i in s: \n",
    "# #         flatten_list3.append(i)\n",
    "        \n",
    "# for s in y_val_pred4:\n",
    "#     flatten_list4.append(i)\n",
    "# #     for i in s: \n",
    "# #         flatten_list4.append(i)\n",
    "\n",
    "x = pd.DataFrame({'orig': y_test, 'model1': flatten_list1, 'model2': flatten_list1, 'xgb': y_val_pred3, 'svc': y_val_pred4})\n",
    "pd.options.display.max_rows = 8000\n",
    "# print(\"Validation F1:\", metrics.f1_score(y_true= y_val, y_pred= flatten_list ))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1\n",
    "print(classification_report(y_test, pred_val_classes1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dcde52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2\n",
    "print(classification_report(y_test, pred_val_classes2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e19fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model xgb\n",
    "print(classification_report(y_test, pred_val_classes3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac07ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model svc\n",
    "print(classification_report(y_test, pred_val_classes4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd19eb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import  metrics\n",
    "\n",
    "models = []\n",
    "models.append(('XGBClassifier', grid_1.best_estimator_))\n",
    "models.append(('SVC rbf', grid_2.best_estimator_))\n",
    "# models.append(('Keras', model.best_estimator_))\n",
    "# models.append(('Keras scaled', model2.best_estimator_))\n",
    "\n",
    "precision_score = []\n",
    "recall_score = []\n",
    "f1_score = []\n",
    "accuracy_score = []\n",
    "for name, model in models:\n",
    "    print(name)\n",
    "    print(\"precision_score: {}\".format(metrics.precision_score(y_test, model.predict(X_test)) ))\n",
    "    print(\"recall_score: {}\".format( metrics.recall_score(y_test, model.predict(X_test)) ))\n",
    "    print(\"f1_score: {}\".format( metrics.f1_score(y_test, model.predict(X_test)) ))\n",
    "    print(\"accuracy_score: {}\".format( metrics.accuracy_score(y_test, model.predict(X_test)) ))\n",
    "    precision_score.append(metrics.precision_score(y_test, model.predict(X_test)))\n",
    "    recall_score.append(metrics.recall_score(y_test, model.predict(X_test)))\n",
    "    f1_score.append( metrics.f1_score(y_test, model.predict(X_test)))\n",
    "    accuracy_score.append(metrics.accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27820c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1\n",
    "precision_score.append(metrics.precision_score(y_test, model1_pred_classes))\n",
    "recall_score.append(metrics.recall_score(y_test, model1_pred_classes))\n",
    "f1_score.append( metrics.f1_score(y_test, model1_pred_classes))\n",
    "accuracy_score.append(metrics.accuracy_score(y_test, model1_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db49340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2\n",
    "precision_score.append(metrics.precision_score(y_test, model2_pred_classes))\n",
    "recall_score.append(metrics.recall_score(y_test, model2_pred_classes))\n",
    "f1_score.append( metrics.f1_score(y_test, model2_pred_classes))\n",
    "accuracy_score.append(metrics.accuracy_score(y_test, model2_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val model 1\n",
    "scaler = StandardScaler()\n",
    "X_val_scaled = scaler.fit_transform(X_val)\n",
    "\n",
    "ypred1_v = model1.predict(X_val_scaled)\n",
    "model1_pred_classes_v = np.where(ypred1_v > 0.5, 1,0)\n",
    "\n",
    "precision_score.append(metrics.precision_score(y_val, model1_pred_classes_v))\n",
    "recall_score.append(metrics.recall_score(y_val, model1_pred_classes_v))\n",
    "f1_score.append( metrics.f1_score(y_val, model1_pred_classes_v))\n",
    "accuracy_score.append(metrics.accuracy_score(y_val, model1_pred_classes_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val model 2\n",
    "ypred2_v = model2.predict(X_val_scaled)\n",
    "model2_pred_classes_v = np.where(ypred2_v > 0.5, 1,0)\n",
    "\n",
    "precision_score.append(metrics.precision_score(y_val, model2_pred_classes_v))\n",
    "recall_score.append(metrics.recall_score(y_val, model2_pred_classes_v))\n",
    "f1_score.append( metrics.f1_score(y_val, model2_pred_classes_v))\n",
    "accuracy_score.append(metrics.accuracy_score(y_val, model2_pred_classes_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b4e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val model 1 unscaled\n",
    "ypred1_v2 = model1.predict(X_val)\n",
    "model1_pred_classes_v2 = np.where(ypred1_v2 > 0.5, 1,0)\n",
    "\n",
    "precision_score.append(metrics.precision_score(y_val, model1_pred_classes_v2))\n",
    "recall_score.append(metrics.recall_score(y_val, model1_pred_classes_v2))\n",
    "f1_score.append( metrics.f1_score(y_val, model1_pred_classes_v2))\n",
    "accuracy_score.append(metrics.accuracy_score(y_val, model1_pred_classes_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd41d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val model 2 unscaled\n",
    "ypred2_v2 = model2.predict(X_val)\n",
    "model2_pred_classes_v2 = np.where(ypred2_v2 > 0.5, 1,0)\n",
    "\n",
    "precision_score.append(metrics.precision_score(y_val, model2_pred_classes_v2))\n",
    "recall_score.append(metrics.recall_score(y_val, model2_pred_classes_v2))\n",
    "f1_score.append( metrics.f1_score(y_val, model2_pred_classes_v2))\n",
    "accuracy_score.append(metrics.accuracy_score(y_val, model2_pred_classes_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val XGBClassifier\n",
    "ypred_xgb_v = grid_1.predict(X_val_scaled)\n",
    "model_xgb_pred_classes = np.where(ypred_xgb_v > 0.5, 1,0)\n",
    "\n",
    "precision_score.append(metrics.precision_score(y_val, model_xgb_pred_classes))\n",
    "recall_score.append(metrics.recall_score(y_val, model_xgb_pred_classes))\n",
    "f1_score.append( metrics.f1_score(y_val, model_xgb_pred_classes))\n",
    "accuracy_score.append(metrics.accuracy_score(y_val, model_xgb_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d364198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val SVC rbf\n",
    "ypred_svc_v = grid_2.predict(X_val_scaled)\n",
    "model_svc_pred_classes = np.where(ypred_svc_v > 0.5, 1,0)\n",
    "\n",
    "precision_score.append(metrics.precision_score(y_val, model_svc_pred_classes))\n",
    "recall_score.append(metrics.recall_score(y_val, model_svc_pred_classes))\n",
    "f1_score.append( metrics.f1_score(y_val, model_svc_pred_classes))\n",
    "accuracy_score.append(metrics.accuracy_score(y_val, model_svc_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {'precision_score': precision_score, \n",
    "     'recall_score': recall_score, \n",
    "     'f1_score': f1_score,\n",
    "     'accuracy_score' : accuracy_score\n",
    "    }\n",
    "df = pd.DataFrame(data=d)\n",
    "df.insert(loc=0, column='Method', value=['XGBClassifier', 'SVC rbf', 'Model 1 scaled', 'Model 2 scaled + extras', \n",
    "                                         'Validation Model 1', 'Valiadtion Model 2',\n",
    "                                         'Validation Model 1 unscaled', 'Valiadtion Model 2 unscaled',\n",
    "                                         'Validation XGBClassifier', 'Validation SVC rbf'\n",
    "                                        ])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
